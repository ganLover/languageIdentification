{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Language Identification from Speech\n",
    "\n",
    "\n",
    "OBJECTIVE:    Develop a NN to find identify the language (English/Hindi) used in the SPEECH.\n",
    "\n",
    "SPEECH:\n",
    "1. To get SPEECH files (audio files) use any TEXT to SPEECH converter.\n",
    "2. For this project-> https://www.indiadict.com/web/text-to-speech.html \n",
    "3. This website allows to generate English and Hindi speech from English Text.\n",
    "4. The speech files are in mp3 format.\n",
    "\n",
    "PROCESSING OF SPEECH:\n",
    "1. Speech is an audio signal.\n",
    "2. To process audio signals use LIBROSA library. https://librosa.github.io/librosa/\n",
    "3. If the downloaded files are in mp3 fomat->  pip install audioread\n",
    "\n",
    "FEATURE COLLECTION:\n",
    "1. Audio Signal-> Features-> Neural Network-> TRAIN\n",
    "2. To get feature LIBROSA library willbe very usefull.\n",
    "3. For analysis we find-> \"mfccs,chroma,mel,spectral contrast,tonnetz\" FEATURES.\n",
    "\n",
    "SAMPLING:\n",
    "1. https://en.wikipedia.org/wiki/Sampling_(signal_processing)\n",
    "\n",
    "MFCCS-> Mel Frequency Cepstral Coefficient\n",
    "1. https://en.wikipedia.org/wiki/Mel-frequency_cepstrum\n",
    "2. https://nlp.stanford.edu/courses/lsa352/lsa352.lec6.ppt\n",
    "\n",
    "    Pre-emphasis\n",
    "    1. https://www.quora.com/Why-is-pre-emphasis-i-e-passing-the-speech-signal-through-a-first-order-high-pass-filter-required-in-speech-processing-and-how-does-it-work\n",
    "    2. https://www.daenotes.com/electronics/communication-system/pre-emphasis-and-de-emphasis\n",
    "\n",
    "    Windowing\n",
    "    1. https://www.youtube.com/watch?v=RJ4ZU_SOH7I\n",
    "    2. https://www.youtube.com/watch?v=aVA_mUiRyHc\n",
    "    3. http://download.ni.com/evaluation/pxi/Understanding%20FFTs%20and%20Windowing.pdf\n",
    "\n",
    "    Fourier Tranform\n",
    "    1. https://www.youtube.com/watch?v=b-JxoHKv27Y&t=595s\n",
    "    2. https://www.youtube.com/watch?v=BXghmsH-mKY&t=484s\n",
    "    3. https://www.youtube.com/watch?v=dCeHOf4cJE0\n",
    "\n",
    "    Mel Filter\n",
    "    1. https://haythamfayek.com/2016/04/21/speech-processing-for-machine-learning.html\n",
    "    2. https://dsp.stackexchange.com/questions/19574/mel-filter-in-mfcc-is-it-necessary\n",
    "\n",
    "CHROMA Feature\n",
    "1. https://en.wikipedia.org/wiki/Chromatic_scale\n",
    "2. https://labrosa.ee.columbia.edu/matlab/chroma-ansyn/\n",
    "\n",
    "SPECTRAL CONTRAST Feature\n",
    "1. http://docs.twoears.eu/en/latest/afe/available-processors/spectral-features/\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import librosa\n",
    "import glob \n",
    "import matplotlib.pyplot as plt\n",
    "import librosa.display\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PolyCollection at 0x1fbdbcf1c18>"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data, sampling_rate = librosa.load('F:/4 year/NLP/project/file/test/eng/bro can I help you.mp3')\n",
    "plt.figure(figsize=(12, 4))\n",
    "librosa.display.waveplot(data, sr=sampling_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PolyCollection at 0x1fbdbd4b9e8>"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data, sampling_rate = librosa.load('F:/4 year/NLP/project/file/test/eng/bro can I help you.mp3')\n",
    "plt.figure(figsize=(12, 4))\n",
    "librosa.display.waveplot(data, sr=sampling_rate)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_feature(file_name):\n",
    "    \n",
    "    X, sample_rate = librosa.load(file_name)\n",
    "    \n",
    "    stft = np.abs(librosa.stft(X))\n",
    "    mfccs = np.mean(librosa.feature.mfcc(y=X, sr=sample_rate, n_mfcc=40).T,axis=0)\n",
    "    chroma = np.mean(librosa.feature.chroma_stft(S=stft, sr=sample_rate).T,axis=0)\n",
    "    mel = np.mean(librosa.feature.melspectrogram(X, sr=sample_rate).T,axis=0)\n",
    "    contrast = np.mean(librosa.feature.spectral_contrast(S=stft, sr=sample_rate).T,axis=0)\n",
    "    tonnetz = np.mean(librosa.feature.tonnetz(y=librosa.effects.harmonic(X),\n",
    "    sr=sample_rate).T,axis=0)\n",
    "    \n",
    "    #print(file_name + str(mfccs.shape) + str(chroma.shape) + str(mel.shape) + str(contrast.shape) + str(tonnetz.shape))\n",
    "    #in total we get 193 features-> Therefore the size of Numpy array is 193\n",
    "    \n",
    "    return mfccs,chroma,mel,contrast,tonnetz\n",
    "\n",
    "# finding all the files in a folder with extension .mp3\n",
    "# sub_dir will has all the audio files\n",
    "# parent_dir will have folders for different language audio files \n",
    "def parse_audio_files(parent_dir,sub_dirs,file_ext=\"*.mp3\"):\n",
    "    \n",
    "    \n",
    "    features, labels = np.empty((0,193)), np.empty(0)\n",
    "    \n",
    "    for label, sub_dir in enumerate(sub_dirs):\n",
    "        for fn in glob.glob(os.path.join(parent_dir, sub_dir, file_ext)):\n",
    "            \n",
    "            \n",
    "            try:\n",
    "              mfccs, chroma, mel, contrast,tonnetz = extract_feature(fn)\n",
    "            except Exception as e:\n",
    "              print (\"Error encountered while parsing file: \", fn)\n",
    "              continue\n",
    "            ext_features = np.hstack([mfccs,chroma,mel,contrast,tonnetz])\n",
    "            features = np.vstack([features,ext_features])\n",
    "            \n",
    "            if sub_dir=='hindi':\n",
    "                labels = np.append(labels, 0)\n",
    "            elif sub_dir=='eng':\n",
    "                labels = np.append(labels, 1)            \n",
    "                            \n",
    "    return np.array(features) , np.array(labels, dtype = np.int)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 0 0 0 1 1 1 1 1 1]\n",
      "0\n",
      "-------------\n",
      "[-2.62273015e+02  1.35077874e+02 -7.08939617e+01  4.54595726e+01\n",
      " -1.01618946e+01 -8.95454247e+00  9.27526676e+00 -1.26280563e+01\n",
      " -1.17841322e+01  1.54300466e+00 -1.05705409e+01  8.73648762e+00\n",
      " -2.16515273e+01 -2.45804694e+00  7.79522323e+00 -1.46634524e+01\n",
      "  9.79909640e-01 -2.58234010e+00 -1.65547221e+00 -8.57444577e+00\n",
      "  1.88967194e+00 -6.81573330e+00 -3.16239096e+00  1.77653472e+00\n",
      "  2.32827959e+00  6.06711522e+00  5.08899643e+00  3.83279099e+00\n",
      "  2.25068949e+00  3.68635703e+00  3.38819876e+00  4.85506551e+00\n",
      " -3.33565562e+00  2.52256976e+00 -1.36131902e+00  1.33456239e+00\n",
      "  5.00340795e+00  2.81506342e+00  9.94547178e-01 -9.68073933e-01\n",
      "  5.31810502e-01  5.42326092e-01  5.28039082e-01  5.33402485e-01\n",
      "  5.85850501e-01  6.24779590e-01  6.30424011e-01  5.99913891e-01\n",
      "  5.03658904e-01  5.20352709e-01  5.20747187e-01  4.82079399e-01\n",
      "  4.34668520e-01  4.05163350e-02  2.35105153e-02  1.86527023e-02\n",
      "  7.52499414e-01  3.52537832e+00  1.12570457e+01  1.73226628e+01\n",
      "  1.61717368e+01  1.87432278e+01  1.47165541e+01  4.26653480e+00\n",
      "  3.36502115e+00  6.52562425e+00  8.28925555e+00  3.22344386e+00\n",
      "  5.80605738e+00  9.76705796e+00  1.28277062e+01  8.44742178e+00\n",
      "  6.72144908e+00  5.77531229e+00  1.81058344e+00  2.94828042e+00\n",
      "  1.96767476e+00  2.04638931e+00  2.76977050e+00  3.48844527e+00\n",
      "  1.08888910e+01  1.31829236e+01  9.44113186e+00  9.09604104e+00\n",
      "  5.63689573e+00  4.94528595e+00  8.06779593e+00  5.29855033e+00\n",
      "  1.43518339e+00  1.02999307e+00  1.59726025e+00  2.70350637e+00\n",
      "  6.40786658e+00  7.35491909e+00  5.67676163e+00  3.08058191e+00\n",
      "  2.67605820e+00  2.52404008e+00  9.50848961e-01  9.65893703e-01\n",
      "  5.60355268e+00  1.52418526e+01  6.19633861e+00  2.71619115e+00\n",
      "  1.29410791e+00  1.82847352e+00  3.73594332e+00  3.50760298e+00\n",
      "  1.90695730e+00  1.33453026e+00  5.53487563e-01  7.58575085e-01\n",
      "  8.78942981e-01  5.54967137e-01  3.10456184e-01  1.27126692e-01\n",
      "  1.73598107e-01  1.39940510e-01  1.12723914e-01  1.50567159e-01\n",
      "  1.43977417e-01  2.94465422e-01  3.18504809e-01  4.91267042e-01\n",
      "  5.44314943e-01  5.74877689e-01  6.65545056e-01  6.08617632e-01\n",
      "  8.80993410e-01  8.39166261e-01  2.89456631e-01  1.79911732e-01\n",
      "  1.04805326e-01  4.70505991e-02  4.86456791e-02  6.08518418e-02\n",
      "  3.50108603e-02  4.36533707e-02  5.34220984e-02  5.78168238e-02\n",
      "  7.08672187e-02  1.13118704e-01  1.36835069e-01  1.37222940e-01\n",
      "  8.46284137e-02  6.02035865e-02  3.67458533e-02  1.56900549e-02\n",
      "  7.94329605e-03  1.26711814e-02  1.13301799e-02  1.43726185e-02\n",
      "  9.16441621e-03  5.44078491e-03  4.82740948e-03  2.77171212e-03\n",
      "  3.11335901e-03  2.02838065e-03  9.89134503e-04  4.67107085e-04\n",
      "  1.49387042e-03  1.38895441e-03  7.06178717e-04  1.13810641e-03\n",
      "  1.00790738e-03  7.47336362e-04  1.74738154e-03  1.95149768e-03\n",
      "  1.36355614e-03  5.10343048e-04  6.77548240e-06  6.10435023e-09\n",
      "  6.23838822e-09  5.91806916e-09  6.36528298e-09  5.70949645e-09\n",
      "  5.73176848e-09  4.81359557e-09  2.59677772e-09  1.40983567e-09\n",
      "  1.84297733e+01  1.85471101e+01  2.11033032e+01  1.72451944e+01\n",
      "  2.06876898e+01  3.61424262e+01  3.47131978e+01 -4.91649261e-03\n",
      "  9.45823118e-03  2.46190536e-02 -1.45391199e-02  9.17684563e-03\n",
      " -3.58538210e-04]\n",
      "1\n",
      "-------------\n",
      "[-3.41392976e+02  1.41911408e+02  1.33390502e-01  1.66023721e+01\n",
      " -9.54078115e+00 -6.60036193e+00  1.28936526e+01 -2.13826391e+00\n",
      " -1.22311570e+01  2.06333460e-01 -3.93198742e+00 -5.06221656e+00\n",
      " -4.91029358e+00 -2.23745024e+00  1.88442073e-01 -9.10002039e+00\n",
      " -4.67739894e+00  2.69131182e-02 -5.20125866e+00 -5.14914005e+00\n",
      "  5.90709388e-01 -6.93701722e+00 -6.02843741e+00 -4.46134417e-01\n",
      " -4.05754627e+00 -3.35635247e-02  2.40578689e+00  3.50774747e+00\n",
      "  6.20724127e+00  1.57425114e-01  1.13002776e+00  8.93036517e-01\n",
      " -3.16102453e+00  4.38336301e+00  2.74566876e+00  4.99809825e+00\n",
      "  5.95502288e+00  3.91159610e+00  1.55756838e+00 -2.16749704e+00\n",
      "  5.18678515e-01  4.68273557e-01  4.01330117e-01  3.03272327e-01\n",
      "  3.27858407e-01  4.19611736e-01  4.77920780e-01  5.17664439e-01\n",
      "  4.36289036e-01  4.84534329e-01  5.89257544e-01  6.32284628e-01\n",
      "  1.99276447e-01  5.01799294e-02  2.65509435e-02  2.83161037e-01\n",
      "  3.73836658e+00  4.26811897e+00  2.07566427e+01  3.01396567e+01\n",
      "  1.15257609e+02  4.85192443e+01  4.34585287e+00  8.68551216e-01\n",
      "  5.61313571e-01  1.05437570e+00  5.70648604e+00  3.52957907e+00\n",
      "  2.84422546e+00  1.86677923e+00  2.00497018e+00  1.48462456e+00\n",
      "  4.34196420e+00  7.89162282e+00  5.07812376e+00  8.13621080e-01\n",
      "  6.26885267e-01  9.19152932e-01  1.67480507e+00  2.76498617e+00\n",
      "  2.78433781e+00  1.72564017e+01  9.70152174e+00  1.06188495e+00\n",
      "  1.05696264e+00  1.10432934e+00  1.15293289e+00  5.51748524e-01\n",
      "  8.34982736e+00  3.15534375e+01  5.02650813e+00  2.36020105e+00\n",
      "  1.12680254e+00  7.77285139e-01  5.81034901e-01  2.52068266e+00\n",
      "  5.68666034e+00  6.43742413e-01  2.13705661e-01  9.23814891e-02\n",
      "  8.97017050e-02  1.73447927e+00  2.43963803e+00  2.49096298e-01\n",
      "  1.80167536e-01  9.40777981e-02  2.72845360e+00  3.90459869e+00\n",
      "  5.42400549e-01  1.63046422e-01  1.65885127e-01  7.40552799e-01\n",
      "  2.50961958e-01  1.41816799e-01  4.32729030e-02  9.48090526e-02\n",
      "  3.15462311e-02  1.82525676e-02  2.24678849e-02  2.78594686e-02\n",
      "  9.68274821e-03  6.17207268e-03  9.97582338e-03  9.84672458e-03\n",
      "  1.03132935e-02  1.69311318e-02  1.63569400e-02  3.12998166e-02\n",
      "  3.31321456e-02  5.32253982e-02  5.09062246e-02  2.39936149e-02\n",
      "  1.56985279e-02  1.30559183e-02  3.53771331e-03  4.60713061e-03\n",
      "  4.06453448e-03  3.41481165e-03  2.97110088e-03  3.17490703e-03\n",
      "  2.90288587e-03  4.12499262e-03  4.55473393e-03  4.47771955e-03\n",
      "  5.33234606e-03  4.11055473e-03  1.86956543e-03  5.96057917e-04\n",
      "  2.58354522e-04  1.40786432e-04  3.45736965e-04  4.86052779e-04\n",
      "  3.77321559e-04  2.89909814e-04  6.55191136e-04  2.50347092e-04\n",
      "  7.13805701e-05  4.84020045e-05  3.71504903e-05  6.20590232e-05\n",
      "  2.26741658e-04  1.58442734e-04  7.39323409e-05  3.46993073e-05\n",
      "  1.17192458e-05  2.60977567e-05  1.42636688e-04  2.47312638e-04\n",
      "  3.62990087e-05  1.72827387e-06  2.75281372e-08  5.25552573e-09\n",
      "  5.42824277e-09  5.17932139e-09  5.23688556e-09  5.26430531e-09\n",
      "  4.86085305e-09  3.92776739e-09  1.51763943e-09  6.88311282e-10\n",
      "  1.82851775e+01  1.83587204e+01  1.92794564e+01  1.81588703e+01\n",
      "  2.16561429e+01  3.09318578e+01  3.28690731e+01  1.57362755e-03\n",
      "  4.26498075e-03  2.18555001e-02 -4.09365521e-02  1.00695939e-02\n",
      " -7.42323018e-03]\n",
      "2\n",
      "-------------\n",
      "[-3.15569206e+02  1.49827110e+02 -1.80582913e+01  1.51975190e+01\n",
      " -4.58790399e+00 -5.38985562e+00  1.43346710e+00 -4.36783094e+00\n",
      " -1.11638833e+01  4.14881590e+00 -3.14165973e+00 -2.74024634e-01\n",
      " -1.15080929e+01  3.03317135e+00 -3.93876567e-01 -1.15487275e+01\n",
      "  1.64297850e+00 -3.22614045e+00 -2.19703847e+00 -8.77793542e+00\n",
      "  5.44425536e-01 -7.41556145e+00 -6.91030687e+00  1.07857909e+00\n",
      " -3.61121967e+00 -3.50083357e+00 -1.59101534e+00 -2.93363088e+00\n",
      " -1.03267530e+00 -2.29144528e+00 -5.97687288e+00  5.29132033e+00\n",
      "  1.43985110e+00  2.65300056e+00  8.21784465e+00  1.17857857e+00\n",
      " -7.77173653e-01  9.07875925e-01 -2.15108225e+00  4.10311251e-01\n",
      "  3.73685402e-01  4.24530344e-01  5.32174243e-01  5.86813274e-01\n",
      "  5.14306582e-01  4.23079275e-01  4.59893152e-01  5.79401050e-01\n",
      "  5.79409993e-01  5.14984780e-01  4.62905018e-01  4.41054858e-01\n",
      "  2.13337729e-01  5.44003242e-02  3.57857130e-02  6.08180149e-02\n",
      "  2.28693228e+00  1.53573911e+01  2.11165308e+01  6.07550035e+01\n",
      "  1.99413467e+01  8.66945605e-01  2.11276073e+00  6.03480528e+00\n",
      "  6.67752781e+00  4.24441893e+00  4.84831639e+00  7.61306285e+00\n",
      "  5.72636970e+00  1.06305014e+01  1.70972636e+01  1.21839470e+01\n",
      "  4.83178497e+00  2.90178814e+00  5.45662231e+00  1.39345832e+01\n",
      "  1.00509285e+01  1.81297842e+00  9.78104511e-01  2.48109970e+00\n",
      "  1.27284312e+00  7.67442377e+00  1.88287315e+01  7.92890152e+00\n",
      "  4.09285263e+00  1.93704712e+00  6.39969747e-01  2.95757295e-01\n",
      "  6.93794399e-01  7.56828795e+00  3.89203002e+00  1.27158656e+00\n",
      "  4.03720652e-01  2.12487590e-01  2.68730232e-01  6.55741852e-01\n",
      "  3.02100627e+00  1.44154767e+00  5.22955854e-01  3.12763327e-01\n",
      "  2.91346009e-01  2.72856032e-01  1.80746953e+00  1.09645362e+00\n",
      "  6.57788526e-01  2.52919634e-01  1.02127805e-01  2.03460134e-01\n",
      "  5.69733740e-01  3.70903859e-01  9.34685243e-02  1.62416136e-01\n",
      "  1.25011319e-01  5.67453295e-02  8.64097171e-02  2.53785661e-02\n",
      "  2.01295135e-02  2.74929499e-02  9.97959959e-03  8.80175125e-03\n",
      "  1.41370460e-02  7.64641419e-03  7.40915978e-03  6.38683150e-03\n",
      "  1.13838824e-02  1.59798755e-02  2.09212186e-02  1.98522240e-02\n",
      "  3.72012348e-02  1.83752847e-02  2.79231421e-02  3.51041114e-02\n",
      "  1.65932108e-02  1.48499714e-02  4.94700122e-03  3.88484892e-03\n",
      "  2.43251246e-03  1.53365446e-03  1.76656768e-03  2.16088011e-03\n",
      "  2.38947612e-03  2.68339547e-03  3.62246871e-03  2.72473968e-03\n",
      "  1.05392989e-03  4.79565760e-04  8.96733551e-04  1.72294211e-03\n",
      "  1.83641238e-03  2.22386642e-03  1.78213790e-03  1.30806137e-03\n",
      "  1.73107940e-03  9.66789588e-04  2.06884674e-04  1.34145271e-04\n",
      "  5.77220952e-05  5.74898490e-05  7.83604571e-05  7.21226600e-05\n",
      "  6.74004718e-05  1.29216898e-04  6.04497661e-05  2.16148777e-04\n",
      "  5.10408651e-04  6.40229755e-04  2.24176675e-04  6.05131923e-05\n",
      "  1.77710044e-04  1.85341369e-04  3.78838604e-06  5.41669013e-09\n",
      "  5.33061348e-09  5.66942030e-09  5.41339241e-09  5.18709165e-09\n",
      "  4.54837358e-09  3.76821548e-09  1.50025091e-09  5.58412733e-10\n",
      "  1.93211543e+01  1.85717265e+01  2.08332809e+01  1.87932277e+01\n",
      "  2.05245442e+01  3.11528544e+01  3.67156342e+01  5.78497711e-03\n",
      "  9.35695788e-03  3.63964992e-02  9.64542778e-02  2.73742746e-03\n",
      " -3.49986007e-03]\n",
      "3\n",
      "-------------\n",
      "[-3.41323101e+02  1.18770448e+02  1.13776508e+01  4.86614652e+01\n",
      "  1.09483887e+00 -5.69245106e+00 -8.38748685e+00 -3.08402453e+00\n",
      " -1.20416710e+01 -6.21484040e+00 -5.23729818e+00  2.95659957e+00\n",
      " -6.14761486e+00 -5.84672154e-01 -3.12447612e+00 -7.75351887e+00\n",
      " -1.69896612e+00 -1.98930309e+00 -1.44820077e+00 -9.43527594e+00\n",
      "  3.70188055e+00 -1.01060478e+01 -4.28109398e+00 -8.10257723e-01\n",
      " -6.21575781e+00  2.45045393e+00  3.72346993e-01  2.94180086e+00\n",
      "  3.97711113e+00  4.81019752e+00  4.56243522e+00  3.95717106e+00\n",
      "  2.40480828e+00  4.77159869e+00  9.61644821e-01  7.71947104e-01\n",
      "  4.98712652e-01  1.44243936e+00  1.92195109e+00  4.44716056e-01\n",
      "  4.32782644e-01  3.80216816e-01  3.90783029e-01  4.27129258e-01\n",
      "  4.54390398e-01  4.38330434e-01  4.34460799e-01  4.21832965e-01\n",
      "  4.72957559e-01  5.15677850e-01  5.49538082e-01  5.12857464e-01\n",
      "  3.17682587e-01  5.61579118e-02  3.13358454e-02  1.56571154e-01\n",
      "  3.41139634e+00  1.30328850e+01  1.72235769e+01  5.66869061e+01\n",
      "  4.92372526e+01  1.73872553e+01  2.38020314e+00  4.52180141e+00\n",
      "  7.63111302e+00  1.60656918e+00  2.55233645e+00  3.86493979e+01\n",
      "  4.85474148e+01  3.86370023e+01  1.48299578e+01  6.79830852e+00\n",
      "  1.66973305e+00  1.41522146e+00  9.20439638e-01  3.55878201e-01\n",
      "  4.27756942e-01  1.64678491e+00  7.32983453e+00  2.55336594e+01\n",
      "  2.49638214e+01  7.54946442e+00  1.33948161e+00  5.03973461e-01\n",
      "  4.10583323e-01  1.35699263e-01  2.89107786e-01  3.24329679e-01\n",
      "  1.12013897e+00  1.60074399e+00  7.98535627e-01  9.70023891e-02\n",
      "  1.59724791e-02  1.61954656e-02  1.26839919e-02  1.53372046e-02\n",
      "  4.25570539e-02  4.12867126e-02  1.05291051e-01  1.45415813e-01\n",
      "  5.68992293e-02  2.24550465e-02  9.83000872e-03  6.98671580e-03\n",
      "  1.05201783e-02  2.33599386e-02  2.58694737e-02  7.32926185e-03\n",
      "  7.75503601e-03  1.14595802e-02  1.91222706e-02  1.49474300e-02\n",
      "  1.97532055e-02  3.79667127e-02  2.35877534e-02  1.06200951e-02\n",
      "  1.74769203e-02  1.25359252e-02  7.82601021e-03  9.62767104e-03\n",
      "  1.37151125e-02  2.38258743e-02  2.68909718e-02  2.52665243e-02\n",
      "  3.48576125e-02  2.87887197e-02  1.35972201e-02  3.53834877e-02\n",
      "  5.94376971e-02  2.26146759e-02  3.15087320e-02  1.67495228e-02\n",
      "  1.12045082e-02  7.56726501e-03  2.97765920e-03  2.83133428e-03\n",
      "  1.88160307e-03  2.25291059e-03  2.64692319e-03  3.38632026e-03\n",
      "  6.44928642e-03  5.29632537e-03  5.50837677e-03  7.36505094e-03\n",
      "  6.46278140e-03  3.55782919e-03  2.68244396e-03  2.56431418e-03\n",
      "  1.57876772e-03  7.32587291e-04  9.17276349e-04  1.68904396e-03\n",
      "  2.52112527e-03  2.15882860e-03  1.83328746e-03  1.10753428e-03\n",
      "  2.33511179e-03  1.99137909e-03  1.01654576e-03  1.88807229e-03\n",
      "  8.90044830e-04  1.77876056e-03  3.23227679e-03  5.43476819e-03\n",
      "  4.12881486e-03  3.91463833e-03  4.09157874e-03  7.77354634e-03\n",
      "  5.45016361e-03  2.62484030e-03  3.23912309e-05  5.45885094e-09\n",
      "  5.51990955e-09  5.50326394e-09  5.78898637e-09  5.17287264e-09\n",
      "  5.33357479e-09  3.82459331e-09  1.44428949e-09  5.81503172e-10\n",
      "  1.73864968e+01  1.79998504e+01  2.12198501e+01  1.79798822e+01\n",
      "  2.07972254e+01  2.82782576e+01  3.64791417e+01 -1.34080120e-02\n",
      "  4.21779382e-03 -3.36196293e-02  4.19712835e-02 -1.08901089e-04\n",
      "  5.52272851e-03]\n",
      "4\n",
      "-------------\n",
      "[-2.94597002e+02  9.89123554e+01  6.22521682e+00  6.02901008e+01\n",
      " -6.90437942e+00 -3.27645137e+00 -1.17199195e+01  1.26574215e+00\n",
      " -1.16633971e+01 -6.89493079e+00 -7.57189929e+00 -2.92833682e+00\n",
      " -7.33063493e+00 -1.95183490e-01 -2.05881803e+00 -7.50491955e+00\n",
      " -5.14457233e+00 -1.47708537e+00 -2.18718201e-01 -1.11864092e+01\n",
      "  2.45582541e+00 -1.07468765e+01 -2.65385035e+00  6.90218710e-01\n",
      " -5.28574568e+00 -1.29626925e+00  1.40762785e+00  1.41710371e+00\n",
      "  3.86734842e+00  1.51627081e+00 -2.31876730e+00  3.78693914e+00\n",
      " -2.94938150e+00  4.61278401e+00  3.55745088e+00  4.05457411e+00\n",
      "  7.81215116e+00  8.11284678e+00  7.38283817e+00  6.34280486e+00\n",
      "  4.79203626e-01  4.07586387e-01  3.76710568e-01  3.76480505e-01\n",
      "  4.96702469e-01  5.91203910e-01  6.23549099e-01  5.55714912e-01\n",
      "  4.52837554e-01  4.37009165e-01  4.67843009e-01  5.04841529e-01\n",
      "  2.22308858e-01  7.04462535e-02  4.15958715e-02  6.18098079e-02\n",
      "  4.04928249e-01  1.93237616e+01  5.86038837e+01  3.31265559e+01\n",
      "  5.42173592e+01  7.81008948e+01  7.81895835e+00  2.21180852e+00\n",
      "  9.21822762e+00  2.48425704e+01  3.00132023e+01  5.80257855e+00\n",
      "  4.41065956e+00  7.36718739e+00  7.71604983e+00  1.10801232e+01\n",
      "  7.45951877e+00  5.44042969e+00  4.06502801e+00  1.87325658e+00\n",
      "  1.17439177e+00  2.75176332e+00  1.90741996e+00  6.10868698e+00\n",
      "  7.08677696e+00  2.71998366e+00  1.02983533e+00  2.86344086e-01\n",
      "  2.20013893e-01  1.49963984e-01  2.47200006e+00  2.86059328e+00\n",
      "  3.64359050e-01  1.29331584e-01  9.20156121e-02  5.21802263e-02\n",
      "  2.89498199e-02  3.89246460e-01  4.79910915e-01  4.91221108e-02\n",
      "  4.97198856e-02  1.62811536e-02  1.48681412e-02  1.43243447e-01\n",
      "  8.85254735e-02  2.13407933e-02  9.09301962e-03  1.12893519e-02\n",
      "  1.39969641e-01  1.08190570e-01  2.75627004e-02  3.41698904e-02\n",
      "  6.69320764e-02  2.35898673e-01  5.94440488e-02  4.85345625e-02\n",
      "  3.37421226e-02  5.52633545e-02  4.67805465e-02  3.07836973e-02\n",
      "  3.88493033e-02  3.14548905e-02  2.11363040e-02  2.58845217e-02\n",
      "  6.36549360e-02  6.71743297e-02  4.14536149e-02  2.75748736e-02\n",
      "  3.62874797e-02  5.65584591e-02  7.06711665e-02  6.96503654e-02\n",
      "  1.04029397e-01  1.01763975e-01  5.76796370e-02  5.27986084e-02\n",
      "  4.62944887e-02  4.42079199e-02  2.04910570e-02  1.87146323e-02\n",
      "  2.73100396e-02  4.28895103e-02  6.47440190e-02  7.56703263e-02\n",
      "  1.10969573e-01  1.38987087e-01  1.03391986e-01  9.45639123e-02\n",
      "  5.33133708e-02  2.33061586e-02  1.89621246e-02  3.13584013e-02\n",
      "  2.43115574e-02  2.98379834e-02  2.56896963e-02  2.65351521e-02\n",
      "  3.78541577e-02  4.32467326e-02  4.09306269e-02  2.56132579e-02\n",
      "  2.48121500e-02  3.89563612e-02  4.02771888e-02  2.50657163e-02\n",
      "  1.64836811e-02  1.16584389e-02  1.02299027e-02  1.50184732e-02\n",
      "  1.90952163e-02  9.83491796e-03  9.52579950e-03  1.37916802e-02\n",
      "  1.13609891e-02  5.29543494e-03  1.19124141e-04  7.08184567e-09\n",
      "  6.41648948e-09  6.93014185e-09  6.68492882e-09  6.65566131e-09\n",
      "  5.88096851e-09  4.88370093e-09  2.29329424e-09  1.06318398e-09\n",
      "  1.96083410e+01  1.75155553e+01  2.06478760e+01  1.91496961e+01\n",
      "  2.16568263e+01  2.63619233e+01  3.71252523e+01 -4.77307553e-03\n",
      "  2.81647998e-03 -1.91516180e-02 -5.94674802e-03  5.44307807e-03\n",
      "  8.07878804e-03]\n",
      "5\n",
      "-------------\n",
      "[-3.16836595e+02  1.50469920e+02 -2.30992229e+01  2.41387150e+01\n",
      " -1.75513973e+01 -2.69376862e+00 -6.22151028e+00 -8.36667512e+00\n",
      " -3.04699874e+00 -3.51457820e+00 -3.54543150e+00  2.94747321e+00\n",
      " -1.57876723e+01  3.70792781e+00  6.54293548e+00 -1.46718241e+01\n",
      " -1.12466621e+00 -1.59719549e+00 -3.06496840e+00 -6.94148888e+00\n",
      "  2.25864249e+00 -8.71708788e+00 -4.78735385e+00  1.18129036e+00\n",
      " -4.33391173e+00  2.63408999e+00  2.02653975e-01  3.44778649e+00\n",
      "  5.47251979e+00  6.36075960e+00  6.11234332e+00  6.54166624e+00\n",
      "  2.66374026e+00  2.76876416e+00  1.47338379e+00  2.51522782e+00\n",
      "  7.30378288e+00  5.89979468e+00  5.04458246e+00  1.40549015e+00\n",
      "  4.61079430e-01  4.24449596e-01  3.97244296e-01  3.99376240e-01\n",
      "  4.26167766e-01  4.62746878e-01  5.18596649e-01  4.98339549e-01\n",
      "  4.87629456e-01  4.91318601e-01  6.21810566e-01  5.81315041e-01\n",
      "  3.35199059e-01  4.34329655e-02  2.16639939e-02  2.01287057e-02\n",
      "  8.54886494e-01  8.41095001e+00  2.07615719e+01  3.38808316e+01\n",
      "  4.97030630e+01  2.08665579e+01  7.18971893e+00  5.50022455e+00\n",
      "  7.80049873e+00  9.95389227e+00  9.54201144e+00  6.17260695e+00\n",
      "  1.08941776e+01  2.58265206e+01  1.57937243e+01  1.31683424e+01\n",
      "  1.77580512e+01  1.46951997e+01  7.86090152e+00  5.75973469e+00\n",
      "  4.31137194e+00  3.29860928e+00  1.59345873e+01  1.98749482e+01\n",
      "  8.29996261e+00  9.02899266e+00  9.49312135e+00  7.42112130e+00\n",
      "  2.96512717e+00  9.74097300e+00  1.52005755e+01  5.16397827e+00\n",
      "  2.03490030e+00  9.14091941e-01  2.91944258e-01  6.71910600e-01\n",
      "  1.29511099e+00  1.00464529e+00  8.16215277e-01  6.05995294e-01\n",
      "  1.78611957e-01  1.90058081e-01  3.98460554e-01  4.31424529e-01\n",
      "  5.06315680e-01  3.78680491e-01  3.58041970e-01  5.28100995e-01\n",
      "  7.11958402e-01  5.68303713e-01  7.00561351e-01  1.17922225e+00\n",
      "  1.06898649e+00  4.81932714e-01  2.81061456e-01  3.60329214e-01\n",
      "  1.63386247e-01  9.98358738e-02  5.36931648e-02  2.85543278e-02\n",
      "  1.81923415e-02  2.34809996e-02  2.36598498e-02  1.35102181e-02\n",
      "  8.37451323e-03  1.36360423e-02  2.04725622e-02  4.01730696e-02\n",
      "  4.99650989e-02  4.66504077e-02  4.94125176e-02  4.12567993e-02\n",
      "  6.82075038e-02  8.41576247e-02  3.72993102e-02  2.60044560e-02\n",
      "  1.67191846e-02  7.16696734e-03  3.58643161e-03  3.52115141e-03\n",
      "  2.01850573e-03  2.48017848e-03  3.20047450e-03  6.11100225e-03\n",
      "  8.33918305e-03  1.77274748e-02  1.32467176e-02  1.00555266e-02\n",
      "  9.24724019e-03  9.25519585e-03  6.55545144e-03  6.24780567e-03\n",
      "  3.19287395e-03  4.10037542e-03  4.87355482e-03  3.43652071e-03\n",
      "  2.94081070e-03  5.25328541e-03  1.91816791e-03  8.08620542e-04\n",
      "  1.42399294e-03  1.39175549e-03  8.19029262e-04  7.01532690e-04\n",
      "  5.76346774e-04  7.09970661e-04  6.32802128e-04  8.35386560e-04\n",
      "  1.56941654e-03  1.22981771e-03  9.29813994e-04  3.52800336e-04\n",
      "  1.15694446e-04  7.50186231e-05  3.14709127e-07  5.70306773e-09\n",
      "  5.81086621e-09  5.28682134e-09  5.56858855e-09  5.10897821e-09\n",
      "  5.05527670e-09  4.12182890e-09  1.56957456e-09  8.10981477e-10\n",
      "  1.86795060e+01  1.96182212e+01  1.95886635e+01  1.82405054e+01\n",
      "  2.12221655e+01  3.13214150e+01  3.37130362e+01 -5.45815274e-03\n",
      "  3.87426555e-02 -3.31600690e-02 -1.48585818e-02  1.31730872e-02\n",
      "  1.08033319e-02]\n",
      "6\n",
      "-------------\n",
      "[-3.27635391e+02  1.06318526e+02 -4.27248023e+01  4.72983705e+01\n",
      " -2.02417912e+00 -7.46002659e+00  5.21245171e+00 -1.10439792e+01\n",
      " -7.22621142e+00 -3.36777738e+00 -1.31983308e+01  9.53398297e+00\n",
      " -9.44385515e+00 -1.62152556e+00  5.98876562e+00 -9.98172145e+00\n",
      " -5.33122374e-01 -3.21415817e+00 -1.04900906e+00 -3.32307099e+00\n",
      " -2.08950131e+00 -2.38246207e+00 -2.47041120e-01 -8.72773565e-01\n",
      "  1.56435790e+00  1.24292637e+00  2.47182082e+00  1.49080889e+00\n",
      "  6.15471403e+00  6.59752845e+00  1.79895984e+00  4.03150964e+00\n",
      " -1.29746430e+00  1.90413568e+00  1.66693387e+00  3.25893191e-01\n",
      " -1.20611624e+00  9.08757833e-01 -2.74197731e+00  1.59511741e+00\n",
      "  5.04045507e-01  4.72738840e-01  5.55376187e-01  5.50048617e-01\n",
      "  4.44333969e-01  4.23475424e-01  3.85450061e-01  3.77839978e-01\n",
      "  4.00316015e-01  4.36361435e-01  5.19416122e-01  4.93222668e-01\n",
      "  3.48466314e-01  5.22865153e-02  3.06612979e-02  3.19180341e-02\n",
      "  7.42104986e-01  3.88462115e+00  6.08826152e+00  1.71849841e+01\n",
      "  2.01592546e+01  2.01755511e+01  3.07378208e+01  3.34221125e+01\n",
      "  5.71143232e+00  1.69057641e+00  9.23552564e-01  8.39319091e-01\n",
      "  3.26094608e+00  2.60091185e+01  1.69525866e+01  6.74156640e+00\n",
      "  3.65966512e+00  1.98108506e+00  2.36029169e+00  2.34212501e+00\n",
      "  5.78820673e-01  1.59695286e+00  7.99394049e+00  3.17654613e+00\n",
      "  1.75382434e+00  3.24800552e+00  4.08317702e+00  2.89945873e+00\n",
      "  1.38467941e+00  1.31587368e+00  6.87371286e-01  5.97516605e-01\n",
      "  7.66027239e-01  8.41183061e-01  1.05931241e+00  2.74842683e+00\n",
      "  3.62071600e+00  3.23335207e+00  1.54416813e+00  1.05372373e+00\n",
      "  8.28899722e-01  4.19888983e-01  8.49466119e-01  7.72466755e-01\n",
      "  5.31881065e-01  4.58524447e-01  1.02340693e+00  1.24134925e+00\n",
      "  7.54537041e-01  4.07673834e-01  4.72669331e-01  8.09052444e-01\n",
      "  3.28265205e+00  1.84374628e+00  3.96243079e-01  1.44595921e-01\n",
      "  2.17600201e-01  2.16513286e+00  1.21797838e+00  1.18582194e-01\n",
      "  4.46167362e-02  3.06073204e-01  5.65813193e-01  9.78595894e-02\n",
      "  8.50589059e-02  3.78203157e-01  6.18565749e-01  2.15408435e-01\n",
      "  2.37046234e-01  3.41175553e-01  2.63340981e-01  1.65151613e-01\n",
      "  1.73190620e-01  2.52516995e-01  2.06700442e-01  9.55634620e-02\n",
      "  7.45895849e-02  8.10686997e-02  8.52359608e-02  5.24517591e-02\n",
      "  4.61487624e-02  4.77036117e-02  3.55394970e-02  5.51606370e-02\n",
      "  5.69936232e-02  7.50827754e-02  9.67669109e-02  7.76673101e-02\n",
      "  5.87433289e-02  2.96415109e-02  1.90336856e-02  1.54544234e-02\n",
      "  9.66829373e-03  5.83128999e-03  7.53571409e-03  3.79307561e-03\n",
      "  3.26629318e-03  1.44785500e-03  8.06237268e-04  1.13363582e-03\n",
      "  4.84226862e-04  7.63062561e-04  4.60053895e-04  8.86804165e-04\n",
      "  5.67050384e-04  5.41341848e-04  6.55862435e-04  7.95990856e-04\n",
      "  1.21864359e-03  1.35522604e-03  1.99313128e-03  2.86344786e-03\n",
      "  2.39144774e-03  1.23681413e-03  3.06740400e-05  5.50343196e-09\n",
      "  4.96425694e-09  5.08374593e-09  5.46037059e-09  5.38838863e-09\n",
      "  5.41323462e-09  4.19914234e-09  1.78724053e-09  9.03681846e-10\n",
      "  1.80090663e+01  1.92096496e+01  2.15039911e+01  1.80638294e+01\n",
      "  2.09422159e+01  3.24119994e+01  3.30109747e+01 -1.72399721e-02\n",
      "  2.97549814e-02  1.16761344e-02 -2.35894759e-03  2.61542298e-03\n",
      "  8.76604980e-03]\n",
      "7\n",
      "-------------\n",
      "[-2.94412704e+02  1.27411621e+02 -5.06719877e+01  2.88197317e+01\n",
      "  6.79102947e+00 -7.75972634e+00 -7.67871645e+00 -2.01379023e+01\n",
      " -6.21293875e+00  4.60149300e+00 -1.16243489e+01  1.25685463e+01\n",
      " -8.93013493e+00 -1.75479992e+00  9.26651395e+00 -1.02110839e+01\n",
      " -3.55669852e+00 -1.57310092e+00 -2.93039446e-01 -6.68370453e+00\n",
      " -2.32501077e-01 -4.78454418e+00 -3.34951666e+00 -2.47256098e+00\n",
      "  6.15671023e-01 -1.43704498e+00  1.18456576e+00  3.93008544e+00\n",
      " -1.75911820e+00  4.06588652e+00 -9.02048579e-01 -4.15555646e-01\n",
      " -1.64796974e+00 -2.37337962e+00  1.83849931e+00  2.81602744e+00\n",
      "  4.84795210e+00  8.28034995e+00  3.88057074e+00  5.88508475e+00\n",
      "  5.23075685e-01  5.07633864e-01  5.72732402e-01  4.52969859e-01\n",
      "  4.55527741e-01  5.05589043e-01  4.60825748e-01  4.75004063e-01\n",
      "  4.47665872e-01  4.44355909e-01  4.83428060e-01  5.01893018e-01\n",
      "  2.87340471e-01  3.74983166e-02  2.04489779e-02  3.13251009e-02\n",
      "  1.04994225e+00  4.31964779e+00  9.75704711e+00  6.77008209e+00\n",
      "  2.91636096e+00  7.93719799e+00  5.23667139e+00  3.85264434e+00\n",
      "  3.59670382e+00  5.00280845e+00  3.99957080e+00  2.72781464e+00\n",
      "  3.11619003e+00  8.27599256e+00  1.73791982e+01  5.02823738e+01\n",
      "  2.99537600e+01  9.65562845e+00  1.17354443e+01  4.12734422e+00\n",
      "  3.70661987e+00  1.39194156e+01  1.61739932e+01  1.05510093e+01\n",
      "  7.97926094e+00  8.37962385e+00  6.03589409e+00  2.49486233e+00\n",
      "  1.42765613e+00  9.18022068e-01  9.05696708e-01  7.73262746e-01\n",
      "  5.52772056e-01  1.02972391e+00  1.05420192e+00  8.07179107e-01\n",
      "  8.02140067e-01  4.39251830e-01  5.96222854e-01  9.81052890e-01\n",
      "  8.76301118e-01  6.99478728e-01  5.24641126e-01  4.46879534e-01\n",
      "  9.32406015e-01  9.09212938e-01  8.63357293e-01  8.69756639e-01\n",
      "  6.93981982e-01  8.66993242e-01  1.46839046e+00  4.66551266e+00\n",
      "  2.40680148e+00  1.46241859e+00  1.47196684e+00  1.20166671e+00\n",
      "  1.56812236e+00  1.71596802e+00  1.13905911e+00  6.35742321e-01\n",
      "  2.89683490e-01  4.01004407e-01  6.27176380e-01  2.89594213e-01\n",
      "  1.17474009e-01  3.62027987e-01  5.08947365e-01  3.02883032e-01\n",
      "  2.39662884e-01  4.39174388e-01  4.03468134e-01  2.42105944e-01\n",
      "  2.54759370e-01  2.68091212e-01  8.97251350e-02  1.25312418e-01\n",
      "  4.51653287e-02  2.14621621e-02  2.61059578e-02  2.53364146e-02\n",
      "  2.14976508e-02  3.22082075e-02  3.15322420e-02  7.43235084e-02\n",
      "  8.37523734e-02  8.49598648e-02  1.14816290e-01  4.23462060e-02\n",
      "  2.78101513e-02  1.91778546e-02  2.11835792e-02  1.32730835e-02\n",
      "  8.25603051e-03  4.55804342e-03  2.75733962e-03  1.32895340e-03\n",
      "  1.44284128e-03  2.27752032e-03  3.65268349e-03  4.24184901e-03\n",
      "  6.60986695e-03  7.90339286e-03  4.62662053e-03  1.10797181e-03\n",
      "  3.06802003e-03  4.26665498e-03  9.35673123e-03  6.41213208e-03\n",
      "  8.54526929e-03  9.86786617e-03  7.16844296e-03  1.25287777e-02\n",
      "  8.87866684e-03  2.18094738e-02  1.17406568e-03  5.53297348e-09\n",
      "  5.99665699e-09  6.45971683e-09  6.16510477e-09  5.84628529e-09\n",
      "  6.12990663e-09  5.15557189e-09  2.15652957e-09  1.21986542e-09\n",
      "  2.07218499e+01  1.90777477e+01  2.10605738e+01  1.88551681e+01\n",
      "  2.21861150e+01  3.31263274e+01  3.04074989e+01 -4.53502850e-04\n",
      "  3.90960023e-02 -3.45911413e-02  1.47922211e-02 -6.07025116e-03\n",
      "  1.48697931e-02]\n",
      "8\n",
      "-------------\n",
      "[-3.28441961e+02  1.32395061e+02 -5.55896605e+01  3.82268551e+01\n",
      " -2.16999131e+00 -1.43580892e+01  1.76134134e+01 -9.83626732e+00\n",
      " -1.07127492e+01 -4.63444152e+00 -1.46949331e+01  6.29135355e+00\n",
      " -1.47736277e+01 -1.58319051e+00  8.31416060e+00 -1.31491308e+01\n",
      "  2.05163873e+00 -3.78783790e+00  1.66019306e-01 -5.36907209e+00\n",
      " -1.38789363e+00 -2.54423773e+00 -7.64603443e+00  1.45579484e+00\n",
      " -4.21772974e-01  3.61934744e+00  6.64019100e+00  8.61345645e+00\n",
      "  2.63054548e+00  7.48598556e+00 -3.71244738e-01  9.86220191e-01\n",
      "  3.82488678e+00 -1.95630458e+00  5.25734370e+00 -4.04420131e+00\n",
      " -6.34319076e-01  1.89558037e+00 -8.82565117e-01  1.13448990e+00\n",
      "  5.27368771e-01  4.69668868e-01  5.00326357e-01  5.41606058e-01\n",
      "  5.10649285e-01  4.94447373e-01  4.79905612e-01  5.01907207e-01\n",
      "  4.16375202e-01  4.23431850e-01  5.65108450e-01  5.35626931e-01\n",
      "  1.53350596e-01  2.80674964e-02  1.04025286e-02  1.74277768e-02\n",
      "  6.83217428e-01  3.94235420e+00  5.97208952e+00  7.67265997e+00\n",
      "  1.32895424e+01  3.70088676e+01  1.09844675e+01  4.89991054e+00\n",
      "  4.32974539e+00  3.15317703e+00  1.71234751e+00  1.36511913e+00\n",
      "  1.80401856e+00  9.40809439e+00  4.66583117e+00  9.75694598e+00\n",
      "  7.31817251e+00  2.17116269e-01  7.32692995e-01  2.66177581e+00\n",
      "  4.62811497e+00  5.41983786e+00  6.23699360e+00  7.07191003e+00\n",
      "  9.77481880e+00  3.19833466e+01  2.25110985e+01  1.20963521e+00\n",
      "  2.10635815e+00  5.91917958e+00  8.02495080e+00  1.13331968e+01\n",
      "  1.29987993e+01  1.00539876e+01  6.88805590e+00  9.40380115e+00\n",
      "  1.01066172e+01  2.35222069e+00  1.18204170e+00  4.11927202e+00\n",
      "  5.98066151e+00  5.67161940e+00  4.81553391e+00  5.84960640e+00\n",
      "  2.15763079e+00  1.60062135e+00  2.89381395e+00  2.46933397e+00\n",
      "  2.17342396e+00  1.56673769e+00  2.25806745e+00  1.12622005e+00\n",
      "  8.83666857e-01  1.08364879e+00  7.87818564e-01  2.92987470e-01\n",
      "  1.21718947e-01  1.38705834e-01  1.81287677e-01  1.11544045e-01\n",
      "  3.76369221e-02  5.51192219e-02  5.10779459e-02  4.80831317e-02\n",
      "  6.19180877e-02  8.16688364e-02  1.14034634e-01  1.47558996e-01\n",
      "  1.43127335e-01  1.47383799e-01  2.44517372e-01  2.66838480e-01\n",
      "  1.50785310e-01  2.47354032e-01  1.42018606e-01  6.21193965e-02\n",
      "  6.11605285e-02  3.30884496e-02  2.73423316e-02  2.12680784e-02\n",
      "  2.02407395e-02  1.85057776e-02  2.21959935e-02  1.76184569e-02\n",
      "  2.79027782e-02  2.90286496e-02  3.21877084e-02  2.98341068e-02\n",
      "  1.86110879e-02  1.02071864e-02  9.66231156e-03  1.24629285e-02\n",
      "  8.35780857e-03  6.83467411e-03  3.57238893e-03  5.57448979e-04\n",
      "  1.49499689e-04  5.90147524e-05  5.99790895e-05  4.45189206e-05\n",
      "  1.39373540e-07  7.68579342e-08  1.24433000e-07  3.66601725e-07\n",
      "  6.37180186e-07  2.18344240e-05  1.33794082e-04  1.25488794e-04\n",
      "  2.45607478e-04  4.92108248e-04  3.39069753e-04  3.27199158e-05\n",
      "  8.77073505e-09  5.19624856e-09  5.26251748e-09  5.87179077e-09\n",
      "  6.85966505e-09  5.62305777e-09  5.21885367e-09  5.50256737e-09\n",
      "  5.17002835e-09  3.84024003e-09  1.88064266e-09  1.06380754e-09\n",
      "  1.87874926e+01  2.04183820e+01  2.21929180e+01  1.83804095e+01\n",
      "  1.93492277e+01  3.74019962e+01  3.52135125e+01 -6.39325771e-03\n",
      "  2.07828862e-02  1.08732420e-03  2.86995602e-02  1.06717183e-02\n",
      "  1.13705915e-02]\n",
      "9\n",
      "-------------\n",
      "[-3.05951729e+02  8.98695411e+01 -3.95275607e+01  4.75146706e+01\n",
      " -2.45050563e+01  5.28880824e+00  3.36964243e+00 -8.59203392e+00\n",
      " -3.61328858e+00 -5.06567888e+00 -7.84260172e+00  9.72732663e+00\n",
      " -8.89966583e+00 -3.25543843e-01  2.26305116e-01 -1.24694425e+01\n",
      " -9.84830458e-02 -7.42055845e-01 -1.00491682e+00 -5.87824462e+00\n",
      "  6.57169626e+00 -6.37678063e+00 -3.28533740e+00  1.35415514e+00\n",
      " -2.68549969e+00 -9.05762100e-01  7.15536536e-02  6.83190980e-02\n",
      "  2.89477539e+00  1.05095263e+01  5.23904741e+00  3.29892645e+00\n",
      "  1.59077728e+00  2.95382516e+00 -7.28776550e-01  3.58378107e+00\n",
      "  4.98259861e+00  3.50924812e+00  4.17799638e-01 -4.77644662e-01\n",
      "  4.75679188e-01  5.25398266e-01  5.78928783e-01  6.01716713e-01\n",
      "  6.91615404e-01  6.95449154e-01  5.69041439e-01  5.43035328e-01\n",
      "  5.08143054e-01  5.84687310e-01  6.29053915e-01  5.33331675e-01\n",
      "  9.26175055e-02  2.82457541e-02  2.18018402e-02  2.98651164e-02\n",
      "  7.27070412e-02  1.03572184e+00  1.14295967e+01  4.98231327e+01\n",
      "  3.90756237e+01  3.22293045e+00  4.66278346e-01  4.47272252e-01\n",
      "  8.22932486e-01  3.03707531e+00  4.79373217e+00  4.39446555e+00\n",
      "  4.96683512e+00  3.13118098e+00  1.08776626e+00  1.04458963e+00\n",
      "  2.21064914e+00  3.15384529e+00  4.94230872e+00  2.20109989e+00\n",
      "  3.10419744e+00  8.36505438e+00  8.06434360e+00  5.45584890e+00\n",
      "  3.03970273e+00  3.02455951e+00  2.61850071e+00  5.07611061e-01\n",
      "  5.20789235e-01  1.65173709e+00  1.03543512e+01  7.86316213e+00\n",
      "  1.92252658e+00  8.03992398e-01  4.27325265e-01  3.08276003e-01\n",
      "  2.42253202e-01  7.48882968e-01  5.12483834e+00  8.66243030e+00\n",
      "  5.73440627e+00  1.29061786e+00  1.73016303e+00  9.42984473e-01\n",
      "  4.02303412e-01  4.86662808e+00  7.29929572e+00  2.20756870e+00\n",
      "  9.55758725e-02  1.23559994e-01  2.88158333e-01  1.20509093e+00\n",
      "  2.38879214e+00  7.09655511e-01  3.49181993e-01  1.23104782e-01\n",
      "  8.98246698e-02  2.05527662e-01  1.80326095e-01  2.52969260e-02\n",
      "  1.36301215e-02  2.76926458e-02  3.86084206e-02  1.83519310e-02\n",
      "  4.34065921e-02  1.54218869e-01  1.50640315e-01  2.42414774e-01\n",
      "  5.08265163e-01  3.75230801e-01  1.91149838e-01  2.77196164e-01\n",
      "  3.37559647e-01  2.52312091e-01  2.32266829e-01  2.28127813e-01\n",
      "  9.36614136e-02  7.48243423e-02  4.65386879e-02  2.80909987e-02\n",
      "  4.14823834e-02  5.63954961e-02  1.21816379e-01  1.66305459e-01\n",
      "  1.87436410e-01  1.86600154e-01  1.85901178e-01  1.28127496e-01\n",
      "  7.50326738e-02  7.94816130e-02  6.22324988e-02  3.59622401e-02\n",
      "  3.05495443e-02  3.61346059e-02  2.46557322e-02  3.41311711e-02\n",
      "  3.96382744e-02  7.05187642e-02  6.11068602e-02  3.08769308e-02\n",
      "  4.16252907e-02  3.51931894e-02  2.45493453e-02  1.75260079e-02\n",
      "  1.63651732e-02  1.16476161e-02  8.65866511e-03  1.42082766e-02\n",
      "  1.21072787e-02  1.01911212e-02  1.09299933e-02  8.42154383e-03\n",
      "  5.71469599e-03  1.99544290e-03  2.58089210e-05  7.06106947e-09\n",
      "  7.25356207e-09  7.42775337e-09  6.67902204e-09  6.57698269e-09\n",
      "  6.80858432e-09  5.10859583e-09  2.76737548e-09  1.51367943e-09\n",
      "  1.69651665e+01  1.58184949e+01  1.81232188e+01  1.79585598e+01\n",
      "  2.27011089e+01  3.29079729e+01  3.07558907e+01 -1.18186052e-02\n",
      "  2.12904983e-02 -2.78796781e-02 -1.23319146e-02  1.74862858e-03\n",
      "  1.36326854e-02]\n",
      "10\n",
      "-------------\n",
      "[-3.31372774e+02  1.38788237e+02 -5.01725126e+01  5.30413193e+01\n",
      "  7.72476825e+00 -1.60651024e+01  3.77014329e+00 -1.72205172e+01\n",
      "  2.23788236e-01 -6.41982718e-01 -1.10284235e+01  1.20203835e+01\n",
      " -1.42395440e+01 -2.29732357e+00  7.98486914e+00 -1.40653613e+01\n",
      " -2.06206657e+00 -1.27349887e+00 -1.72597887e+00 -3.12204262e+00\n",
      "  7.17825195e-01 -2.92058961e+00 -6.54924754e+00  5.44047402e-01\n",
      "  1.24915770e+00 -1.49600191e+00 -1.31656338e-01 -2.94364866e+00\n",
      " -2.18125468e+00  2.37116536e-01 -4.24547508e+00 -1.77244613e-02\n",
      " -3.64023311e+00 -9.41297028e-01  5.07476632e+00  3.21810731e+00\n",
      "  6.04336797e+00  4.73634996e+00 -2.46247862e-02  1.76190880e+00\n",
      "  4.27408852e-01  4.16693973e-01  5.03296307e-01  5.98308495e-01\n",
      "  6.26297170e-01  5.00748733e-01  4.35845300e-01  4.79200061e-01\n",
      "  4.01908291e-01  3.45901003e-01  3.70295146e-01  4.31576542e-01\n",
      "  2.99757590e-01  3.33555434e-02  2.03100571e-02  2.70567609e-02\n",
      "  7.56993779e-01  7.67272601e+00  2.40502378e+01  1.73294128e+01\n",
      "  7.36420978e-01  1.65677042e-01  5.25626018e+00  1.30116539e+01\n",
      "  6.12340939e+00  5.84168721e+00  8.89313053e+00  4.10628640e+00\n",
      "  1.41939096e+00  3.34236785e+00  8.19201955e+00  9.07464843e+00\n",
      "  9.13356759e+00  1.06226118e+01  3.88341590e+00  8.89146365e+00\n",
      "  1.59428105e+01  1.60764183e+01  8.72004693e+00  2.52360802e+00\n",
      "  1.08012605e+01  9.17071802e+00  3.34864121e+00  1.71212146e+00\n",
      "  9.31327462e-01  6.45662790e-01  2.92905365e-01  7.88352346e-01\n",
      "  8.49711014e-01  3.61236629e-01  1.84435653e-01  3.61670006e-01\n",
      "  5.10425940e-01  3.72105635e-01  1.15097757e+00  1.04790949e+00\n",
      "  4.92513091e-01  2.48664729e-01  3.00707178e-01  3.60849205e-01\n",
      "  6.62423466e-01  5.37819338e-01  4.54870731e-01  7.50875682e-01\n",
      "  1.52618584e+00  2.30629330e+00  6.44032295e-01  8.51202831e-01\n",
      "  8.81664194e-01  1.32346774e+00  9.99268608e-01  8.79961186e-01\n",
      "  6.34245700e-01  5.58713343e-01  6.07231035e-01  3.41298298e-01\n",
      "  2.19350058e-01  6.00448446e-01  3.52970178e-01  6.04250365e-02\n",
      "  3.12935997e-01  5.39760980e-01  1.38323938e-01  2.86506544e-01\n",
      "  4.73328242e-01  5.06625463e-01  3.90600435e-01  4.37224921e-01\n",
      "  3.14271801e-01  1.83682222e-01  9.98939682e-02  3.63146266e-02\n",
      "  2.50942284e-02  1.89704378e-02  1.41970892e-02  1.80907617e-02\n",
      "  1.84198307e-02  2.05491524e-02  2.60907886e-02  4.02953075e-02\n",
      "  5.72402023e-02  1.28962344e-01  1.04394093e-01  7.03434668e-02\n",
      "  3.23520869e-02  1.20646843e-02  5.06081681e-03  5.58186050e-03\n",
      "  5.78142324e-03  5.11985017e-03  4.48157197e-03  1.39864387e-03\n",
      "  5.00874466e-04  1.09615276e-04  2.59124738e-05  1.53147467e-05\n",
      "  5.61796900e-07  5.82156199e-07  1.11946822e-05  2.43931912e-05\n",
      "  5.91721031e-05  4.14320671e-05  3.19312976e-05  3.37705174e-05\n",
      "  1.34866679e-04  2.44169579e-04  1.00039867e-04  9.20509185e-05\n",
      "  1.47844686e-04  2.35695617e-05  3.12976080e-07  5.04808042e-09\n",
      "  5.41649147e-09  5.35215083e-09  5.40981962e-09  5.93186865e-09\n",
      "  5.18226857e-09  4.03575230e-09  1.81274683e-09  6.13097570e-10\n",
      "  2.00597909e+01  1.99274212e+01  2.18401096e+01  1.87741512e+01\n",
      "  2.11481242e+01  3.73058612e+01  2.95403871e+01 -2.13200736e-03\n",
      "  1.68565261e-02  8.57255037e-02  2.47659754e-02  1.59836653e-02\n",
      "  9.36596661e-03]\n",
      "11\n",
      "-------------\n",
      "[-3.17905062e+02  8.27191728e+01 -1.68181013e+01  5.02115669e+01\n",
      " -3.35290449e+00 -1.08843510e+00 -9.19584167e+00  2.26329522e+00\n",
      " -1.63172748e+01 -5.48776022e+00 -1.02701616e+01  7.37469304e+00\n",
      " -1.31447773e+01 -3.95761576e-01  1.52960528e+00 -1.33278378e+01\n",
      "  4.73939078e+00 -7.64043508e+00 -3.77514207e+00 -6.16042207e+00\n",
      "  3.10543638e+00 -5.82195005e+00  6.44617659e-01  2.94589084e+00\n",
      "  2.65959397e+00  6.29278408e+00  4.49275287e+00  9.48594130e-01\n",
      "  1.93427632e+00  5.44926058e+00  1.17023653e+00  4.76350795e+00\n",
      "  2.31580576e+00  6.51164438e+00  2.78732870e-01  6.78766856e-01\n",
      "  5.22817980e-01  5.69305537e-01  7.57506588e-01  7.90147614e-01\n",
      "  4.84859296e-01  4.73627586e-01  4.45434755e-01  4.16143907e-01\n",
      "  3.95438812e-01  3.91947231e-01  4.07593567e-01  4.13805990e-01\n",
      "  4.29008748e-01  4.36862912e-01  4.65183466e-01  5.07281924e-01\n",
      "  1.83059174e-01  3.15861237e-02  2.23004487e-02  2.28832961e-02\n",
      "  6.68345977e-01  1.70302988e+00  9.84889089e+00  2.79621372e+01\n",
      "  4.58878802e+01  4.99540927e+01  4.67763609e+01  1.31847520e+01\n",
      "  2.46425192e+00  1.27592896e+00  1.45770363e+00  4.94276938e+00\n",
      "  3.12964512e+00  2.92220920e+00  3.08631529e+00  4.96544901e+00\n",
      "  5.74779814e+00  3.80951184e+00  3.87407988e+00  1.51305156e+01\n",
      "  1.74498391e+01  1.03398069e+00  2.90038459e-01  2.13311921e-01\n",
      "  3.55000276e-01  1.34188804e+00  1.75545593e+00  1.81253145e+00\n",
      "  2.20483645e+00  1.09066402e+00  1.34216645e+00  1.88443521e+00\n",
      "  8.73092307e-01  3.27363838e-01  3.78801528e-01  5.61515737e-01\n",
      "  4.92770492e-01  7.15250495e-01  1.47746056e+00  1.76888056e+00\n",
      "  1.70439224e+00  6.07577462e-01  2.62514401e-01  1.59949597e-01\n",
      "  1.01149957e-01  1.43092295e-01  2.13066749e-01  4.65494375e-01\n",
      "  1.01761395e+00  7.06113053e-01  2.21867399e-01  1.77410230e-01\n",
      "  8.24845607e-02  7.87286695e-02  9.40917605e-02  1.98449195e-01\n",
      "  2.21128195e-01  5.69875113e-01  7.20821092e-01  1.92160800e-01\n",
      "  1.38508138e-01  3.28528458e-01  2.11948006e-01  1.02049018e-01\n",
      "  2.57875330e-01  4.61852851e-01  2.83189786e-01  5.92356820e-01\n",
      "  5.92260385e-01  3.27552950e-01  3.63394419e-01  1.02575158e+00\n",
      "  6.86093530e-01  2.95742268e-01  4.58976025e-01  2.14368387e-01\n",
      "  1.27305023e-01  9.77509538e-02  4.34189241e-02  4.98028932e-02\n",
      "  1.61083920e-02  2.91548918e-02  2.45381325e-02  2.78698904e-02\n",
      "  4.26913312e-02  5.69620377e-02  6.75563204e-02  1.96786639e-01\n",
      "  9.49074409e-02  4.63438838e-02  2.55552224e-02  1.01253387e-02\n",
      "  1.55314438e-02  1.58505432e-02  1.31030450e-02  1.15848530e-02\n",
      "  1.36963547e-02  9.83845004e-03  1.11646694e-02  9.39979853e-03\n",
      "  1.22968948e-02  1.89585351e-02  6.70550244e-02  5.13804758e-02\n",
      "  4.96305645e-02  2.32489054e-02  3.69844996e-02  5.11229620e-02\n",
      "  2.50755629e-02  2.59690518e-02  2.94374137e-02  6.40269937e-02\n",
      "  8.01551162e-02  5.03565500e-02  9.80705406e-04  1.00300924e-08\n",
      "  8.04979521e-09  7.12743338e-09  7.47289469e-09  8.24737860e-09\n",
      "  1.10076007e-08  9.47472792e-09  3.27144048e-09  1.88637950e-09\n",
      "  1.78108827e+01  1.96247309e+01  2.24269515e+01  1.90955407e+01\n",
      "  2.14679714e+01  3.10512232e+01  3.76405239e+01 -1.20708721e-02\n",
      "  2.40518127e-02 -3.26681968e-02  2.99594584e-02  4.90012420e-03\n",
      "  9.97547791e-03]\n"
     ]
    }
   ],
   "source": [
    "parent_dir = 'F:/4 year/NLP/project/file/train'\n",
    "tr_sub_dirs = [\"hindi\",\"eng\"]\n",
    "\n",
    "test_parent_dir = 'F:/4 year/NLP/project/file/test'\n",
    "test_sub_dirs = [\"hindi\", \"eng\"]\n",
    "\n",
    "\n",
    "\n",
    "tr_features, tr_labels = parse_audio_files(parent_dir,tr_sub_dirs)\n",
    "\n",
    "tst_features, tst_labels = parse_audio_files(test_parent_dir,test_sub_dirs)\n",
    "\n",
    "\n",
    "#print(tr_features.shape,tr_labels.shape)\n",
    "print(tr_labels)\n",
    "\n",
    "\n",
    "for i,data in enumerate(tr_features):\n",
    "    print(i)\n",
    "    print(\"-------------\")\n",
    "    print(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output tensor([0.4146], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([0.])\n",
      "tensor(0.5354, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 1 Loss: 0.535424\n",
      "Output tensor([0.3104], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([0.])\n",
      "tensor(0.3717, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 1 Loss: 0.371664\n",
      "Output tensor([0.1560], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([0.])\n",
      "tensor(0.1696, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 1 Loss: 0.169565\n",
      "Output tensor([0.1008], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([0.])\n",
      "tensor(0.1063, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 1 Loss: 0.106296\n",
      "Output tensor([0.0675], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([0.])\n",
      "tensor(0.0699, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 1 Loss: 0.069933\n",
      "Output tensor([0.0454], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([0.])\n",
      "tensor(0.0464, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 1 Loss: 0.046434\n",
      "Output tensor([0.0428], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([1.])\n",
      "tensor(3.1512, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 1 Loss: 3.151193\n",
      "Output tensor([0.1281], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([1.])\n",
      "tensor(2.0553, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 1 Loss: 2.055328\n",
      "Output tensor([0.4457], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([1.])\n",
      "tensor(0.8082, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 1 Loss: 0.808218\n",
      "Output tensor([0.7581], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([1.])\n",
      "tensor(0.2769, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 1 Loss: 0.276940\n",
      "Output tensor([0.8940], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([1.])\n",
      "tensor(0.1120, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 1 Loss: 0.112043\n",
      "Output tensor([0.9342], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([1.])\n",
      "tensor(0.0681, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 1 Loss: 0.068080\n",
      "\n",
      "\n",
      "Train Epoch: 1 Loss: 3.078414\n",
      "Train Epoch: 1 Loss: 2.849856\n",
      "Train Epoch: 1 Loss: 3.080837\n",
      "Train Epoch: 1 Loss: 3.032516\n",
      "Train Epoch: 1 Loss: 3.010741\n",
      "Train Epoch: 1 Loss: 3.111201\n",
      "Train Epoch: 1 Loss: 0.042961\n",
      "Train Epoch: 1 Loss: 0.044592\n",
      "Train Epoch: 1 Loss: 0.043499\n",
      "Train Epoch: 1 Loss: 0.046827\n",
      "Train Epoch: 1 Loss: 0.044147\n",
      "Train Epoch: 1 Loss: 0.046846\n",
      "\n",
      "Test set: Average loss: 1.6757, Accuracy: 6/11 (55%)\n",
      "\n",
      "------------------\n",
      "Output tensor([0.9540], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([0.])\n",
      "tensor(3.0784, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 2 Loss: 3.078414\n",
      "Output tensor([0.8630], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([0.])\n",
      "tensor(1.9875, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 2 Loss: 1.987545\n",
      "Output tensor([0.6285], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([0.])\n",
      "tensor(0.9901, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 2 Loss: 0.990105\n",
      "Output tensor([0.3004], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([0.])\n",
      "tensor(0.3572, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 2 Loss: 0.357216\n",
      "Output tensor([0.1569], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([0.])\n",
      "tensor(0.1707, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 2 Loss: 0.170707\n",
      "Output tensor([0.0925], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([0.])\n",
      "tensor(0.0970, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 2 Loss: 0.097025\n",
      "Output tensor([0.0683], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([1.])\n",
      "tensor(2.6837, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 2 Loss: 2.683702\n",
      "Output tensor([0.1316], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([1.])\n",
      "tensor(2.0277, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 2 Loss: 2.027699\n",
      "Output tensor([0.3297], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([1.])\n",
      "tensor(1.1097, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 2 Loss: 1.109707\n",
      "Output tensor([0.5835], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([1.])\n",
      "tensor(0.5387, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 2 Loss: 0.538681\n",
      "Output tensor([0.7775], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([1.])\n",
      "tensor(0.2516, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 2 Loss: 0.251649\n",
      "Output tensor([0.8587], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([1.])\n",
      "tensor(0.1523, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 2 Loss: 0.152307\n",
      "\n",
      "\n",
      "Train Epoch: 2 Loss: 2.367085\n",
      "Train Epoch: 2 Loss: 2.315558\n",
      "Train Epoch: 2 Loss: 2.379408\n",
      "Train Epoch: 2 Loss: 2.288100\n",
      "Train Epoch: 2 Loss: 2.274308\n",
      "Train Epoch: 2 Loss: 2.384018\n",
      "Train Epoch: 2 Loss: 0.096643\n",
      "Train Epoch: 2 Loss: 0.097229\n",
      "Train Epoch: 2 Loss: 0.096646\n",
      "Train Epoch: 2 Loss: 0.096687\n",
      "Train Epoch: 2 Loss: 0.096635\n",
      "Train Epoch: 2 Loss: 0.099932\n",
      "\n",
      "Test set: Average loss: 1.3266, Accuracy: 6/11 (55%)\n",
      "\n",
      "------------------\n",
      "Output tensor([0.9062], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([0.])\n",
      "tensor(2.3671, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 3 Loss: 2.367085\n",
      "Output tensor([0.8408], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([0.])\n",
      "tensor(1.8375, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 3 Loss: 1.837514\n",
      "Output tensor([0.6643], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([0.])\n",
      "tensor(1.0915, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 3 Loss: 1.091497\n",
      "Output tensor([0.4368], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([0.])\n",
      "tensor(0.5741, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 3 Loss: 0.574078\n",
      "Output tensor([0.2786], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([0.])\n",
      "tensor(0.3265, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 3 Loss: 0.326529\n",
      "Output tensor([0.1841], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([0.])\n",
      "tensor(0.2034, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 3 Loss: 0.203409\n",
      "Output tensor([0.1327], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([1.])\n",
      "tensor(2.0195, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 3 Loss: 2.019513\n",
      "Output tensor([0.1882], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([1.])\n",
      "tensor(1.6704, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 3 Loss: 1.670391\n",
      "Output tensor([0.3199], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([1.])\n",
      "tensor(1.1396, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 3 Loss: 1.139599\n",
      "Output tensor([0.4955], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([1.])\n",
      "tensor(0.7022, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 3 Loss: 0.702182\n",
      "Output tensor([0.6493], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([1.])\n",
      "tensor(0.4319, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 3 Loss: 0.431911\n",
      "Output tensor([0.7540], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([1.])\n",
      "tensor(0.2824, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 3 Loss: 0.282402\n",
      "\n",
      "\n",
      "Train Epoch: 3 Loss: 1.692920\n",
      "Train Epoch: 3 Loss: 1.683156\n",
      "Train Epoch: 3 Loss: 1.713083\n",
      "Train Epoch: 3 Loss: 1.621648\n",
      "Train Epoch: 3 Loss: 1.685892\n",
      "Train Epoch: 3 Loss: 1.711048\n",
      "Train Epoch: 3 Loss: 0.198785\n",
      "Train Epoch: 3 Loss: 0.203456\n",
      "Train Epoch: 3 Loss: 0.201044\n",
      "Train Epoch: 3 Loss: 0.198262\n",
      "Train Epoch: 3 Loss: 0.198619\n",
      "Train Epoch: 3 Loss: 0.198884\n",
      "\n",
      "Test set: Average loss: 1.0279, Accuracy: 6/11 (55%)\n",
      "\n",
      "------------------\n",
      "Output tensor([0.8160], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([0.])\n",
      "tensor(1.6929, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 4 Loss: 1.692920\n",
      "Output tensor([0.7690], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([0.])\n",
      "tensor(1.4654, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 4 Loss: 1.465395\n",
      "Output tensor([0.6584], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([0.])\n",
      "tensor(1.0741, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 4 Loss: 1.074123\n",
      "Output tensor([0.5019], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([0.])\n",
      "tensor(0.6970, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 4 Loss: 0.696965\n",
      "Output tensor([0.3927], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([0.])\n",
      "tensor(0.4987, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 4 Loss: 0.498658\n",
      "Output tensor([0.2930], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([0.])\n",
      "tensor(0.3467, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 4 Loss: 0.346707\n",
      "Output tensor([0.2301], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([1.])\n",
      "tensor(1.4691, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 4 Loss: 1.469125\n",
      "Output tensor([0.2674], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([1.])\n",
      "tensor(1.3188, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 4 Loss: 1.318831\n",
      "Output tensor([0.3552], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([1.])\n",
      "tensor(1.0351, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 4 Loss: 1.035097\n",
      "Output tensor([0.4654], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([1.])\n",
      "tensor(0.7649, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 4 Loss: 0.764893\n",
      "Output tensor([0.5720], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([1.])\n",
      "tensor(0.5586, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 4 Loss: 0.558591\n",
      "Output tensor([0.6591], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([1.])\n",
      "tensor(0.4169, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 4 Loss: 0.416917\n",
      "\n",
      "\n",
      "Train Epoch: 4 Loss: 1.303704\n",
      "Train Epoch: 4 Loss: 1.297786\n",
      "Train Epoch: 4 Loss: 1.293050\n",
      "Train Epoch: 4 Loss: 1.303756\n",
      "Train Epoch: 4 Loss: 1.282080\n",
      "Train Epoch: 4 Loss: 1.303873\n",
      "Train Epoch: 4 Loss: 0.316720\n",
      "Train Epoch: 4 Loss: 0.317838\n",
      "Train Epoch: 4 Loss: 0.320016\n",
      "Train Epoch: 4 Loss: 0.319005\n",
      "Train Epoch: 4 Loss: 0.316545\n",
      "Train Epoch: 4 Loss: 0.316896\n",
      "\n",
      "Test set: Average loss: 0.8810, Accuracy: 6/11 (55%)\n",
      "\n",
      "------------------\n",
      "Output tensor([0.7285], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([0.])\n",
      "tensor(1.3037, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 5 Loss: 1.303704\n",
      "Output tensor([0.6988], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([0.])\n",
      "tensor(1.1999, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 5 Loss: 1.199861\n",
      "Output tensor([0.6214], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([0.])\n",
      "tensor(0.9712, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 5 Loss: 0.971179\n",
      "Output tensor([0.5317], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([0.])\n",
      "tensor(0.7588, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 5 Loss: 0.758752\n",
      "Output tensor([0.4419], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([0.])\n",
      "tensor(0.5831, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 5 Loss: 0.583146\n",
      "Output tensor([0.3716], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([0.])\n",
      "tensor(0.4646, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 5 Loss: 0.464642\n",
      "Output tensor([0.3117], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([1.])\n",
      "tensor(1.1656, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 5 Loss: 1.165563\n",
      "Output tensor([0.3323], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([1.])\n",
      "tensor(1.1017, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 5 Loss: 1.101703\n",
      "Output tensor([0.3974], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([1.])\n",
      "tensor(0.9229, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 5 Loss: 0.922874\n",
      "Output tensor([0.4646], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([1.])\n",
      "tensor(0.7665, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 5 Loss: 0.766515\n",
      "Output tensor([0.5315], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([1.])\n",
      "tensor(0.6321, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 5 Loss: 0.632115\n",
      "Output tensor([0.5934], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([1.])\n",
      "tensor(0.5218, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 5 Loss: 0.521823\n",
      "\n",
      "\n",
      "Train Epoch: 5 Loss: 1.051925\n",
      "Train Epoch: 5 Loss: 1.034863\n",
      "Train Epoch: 5 Loss: 1.049924\n",
      "Train Epoch: 5 Loss: 1.027024\n",
      "Train Epoch: 5 Loss: 1.035712\n",
      "Train Epoch: 5 Loss: 1.050513\n",
      "Train Epoch: 5 Loss: 0.430327\n",
      "Train Epoch: 5 Loss: 0.428717\n",
      "Train Epoch: 5 Loss: 0.429994\n",
      "Train Epoch: 5 Loss: 0.430427\n",
      "Train Epoch: 5 Loss: 0.426990\n",
      "Train Epoch: 5 Loss: 0.430524\n",
      "\n",
      "Test set: Average loss: 0.8024, Accuracy: 6/11 (55%)\n",
      "\n",
      "------------------\n",
      "Output tensor([0.6507], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([0.])\n",
      "tensor(1.0519, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 6 Loss: 1.051925\n",
      "Output tensor([0.6307], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([0.])\n",
      "tensor(0.9962, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 6 Loss: 0.996215\n",
      "Output tensor([0.5819], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([0.])\n",
      "tensor(0.8722, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 6 Loss: 0.872152\n",
      "Output tensor([0.5167], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([0.])\n",
      "tensor(0.7271, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 6 Loss: 0.727080\n",
      "Output tensor([0.4628], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([0.])\n",
      "tensor(0.6214, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 6 Loss: 0.621432\n",
      "Output tensor([0.4092], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([0.])\n",
      "tensor(0.5262, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 6 Loss: 0.526209\n",
      "Output tensor([0.3606], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([1.])\n",
      "tensor(1.0200, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 6 Loss: 1.020023\n",
      "Output tensor([0.3759], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([1.])\n",
      "tensor(0.9784, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 6 Loss: 0.978448\n",
      "Output tensor([0.4223], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([1.])\n",
      "tensor(0.8621, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 6 Loss: 0.862150\n",
      "Output tensor([0.4705], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([1.])\n",
      "tensor(0.7539, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 6 Loss: 0.753948\n",
      "Output tensor([0.5229], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([1.])\n",
      "tensor(0.6484, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 6 Loss: 0.648447\n",
      "Output tensor([0.5666], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([1.])\n",
      "tensor(0.5681, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 6 Loss: 0.568132\n",
      "\n",
      "\n",
      "Train Epoch: 6 Loss: 0.944700\n",
      "Train Epoch: 6 Loss: 0.925044\n",
      "Train Epoch: 6 Loss: 0.946369\n",
      "Train Epoch: 6 Loss: 0.941106\n",
      "Train Epoch: 6 Loss: 0.927096\n",
      "Train Epoch: 6 Loss: 0.945451\n",
      "Train Epoch: 6 Loss: 0.491727\n",
      "Train Epoch: 6 Loss: 0.489606\n",
      "Train Epoch: 6 Loss: 0.491263\n",
      "Train Epoch: 6 Loss: 0.490284\n",
      "Train Epoch: 6 Loss: 0.486380\n",
      "Train Epoch: 6 Loss: 0.492047\n",
      "\n",
      "Test set: Average loss: 0.7792, Accuracy: 6/11 (55%)\n",
      "\n",
      "------------------\n",
      "Output tensor([0.6112], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([0.])\n",
      "tensor(0.9447, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 7 Loss: 0.944700\n",
      "Output tensor([0.5946], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([0.])\n",
      "tensor(0.9028, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 7 Loss: 0.902816\n",
      "Output tensor([0.5609], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([0.])\n",
      "tensor(0.8230, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 7 Loss: 0.823048\n",
      "Output tensor([0.5200], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([0.])\n",
      "tensor(0.7340, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 7 Loss: 0.733990\n",
      "Output tensor([0.4752], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([0.])\n",
      "tensor(0.6447, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 7 Loss: 0.644712\n",
      "Output tensor([0.4417], grad_fn=<SigmoidBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target tensor([0.])\n",
      "tensor(0.5828, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 7 Loss: 0.582838\n",
      "Output tensor([0.4064], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([1.])\n",
      "tensor(0.9005, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 7 Loss: 0.900535\n",
      "Output tensor([0.4128], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([1.])\n",
      "tensor(0.8849, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 7 Loss: 0.884912\n",
      "Output tensor([0.4378], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([1.])\n",
      "tensor(0.8261, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 7 Loss: 0.826081\n",
      "Output tensor([0.4686], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([1.])\n",
      "tensor(0.7579, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 7 Loss: 0.757932\n",
      "Output tensor([0.5103], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([1.])\n",
      "tensor(0.6728, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 7 Loss: 0.672771\n",
      "Output tensor([0.5348], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([1.])\n",
      "tensor(0.6258, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 7 Loss: 0.625801\n",
      "\n",
      "\n",
      "Train Epoch: 7 Loss: 0.864615\n",
      "Train Epoch: 7 Loss: 0.838388\n",
      "Train Epoch: 7 Loss: 0.851415\n",
      "Train Epoch: 7 Loss: 0.838473\n",
      "Train Epoch: 7 Loss: 0.831119\n",
      "Train Epoch: 7 Loss: 0.845812\n",
      "Train Epoch: 7 Loss: 0.553901\n",
      "Train Epoch: 7 Loss: 0.546356\n",
      "Train Epoch: 7 Loss: 0.546688\n",
      "Train Epoch: 7 Loss: 0.564356\n",
      "Train Epoch: 7 Loss: 0.539950\n",
      "Train Epoch: 7 Loss: 0.566404\n",
      "\n",
      "Test set: Average loss: 0.7625, Accuracy: 6/11 (55%)\n",
      "\n",
      "------------------\n",
      "Output tensor([0.5788], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([0.])\n",
      "tensor(0.8646, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 8 Loss: 0.864615\n",
      "Output tensor([0.5628], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([0.])\n",
      "tensor(0.8274, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 8 Loss: 0.827441\n",
      "Output tensor([0.5446], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([0.])\n",
      "tensor(0.7866, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 8 Loss: 0.786613\n",
      "Output tensor([0.5131], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([0.])\n",
      "tensor(0.7197, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 8 Loss: 0.719741\n",
      "Output tensor([0.4754], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([0.])\n",
      "tensor(0.6452, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 8 Loss: 0.645153\n",
      "Output tensor([0.4577], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([0.])\n",
      "tensor(0.6120, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 8 Loss: 0.612002\n",
      "Output tensor([0.4272], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([1.])\n",
      "tensor(0.8505, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 8 Loss: 0.850501\n",
      "Output tensor([0.4342], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([1.])\n",
      "tensor(0.8343, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 8 Loss: 0.834286\n",
      "Output tensor([0.4561], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([1.])\n",
      "tensor(0.7851, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 8 Loss: 0.785142\n",
      "Output tensor([0.4747], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([1.])\n",
      "tensor(0.7452, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 8 Loss: 0.745164\n",
      "Output tensor([0.5101], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([1.])\n",
      "tensor(0.6732, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 8 Loss: 0.673183\n",
      "Output tensor([0.5244], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([1.])\n",
      "tensor(0.6455, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 8 Loss: 0.645465\n",
      "\n",
      "\n",
      "Train Epoch: 8 Loss: 0.816305\n",
      "Train Epoch: 8 Loss: 0.816477\n",
      "Train Epoch: 8 Loss: 0.816487\n",
      "Train Epoch: 8 Loss: 0.817431\n",
      "Train Epoch: 8 Loss: 0.795926\n",
      "Train Epoch: 8 Loss: 0.816531\n",
      "Train Epoch: 8 Loss: 0.582389\n",
      "Train Epoch: 8 Loss: 0.582161\n",
      "Train Epoch: 8 Loss: 0.579741\n",
      "Train Epoch: 8 Loss: 0.579170\n",
      "Train Epoch: 8 Loss: 0.572764\n",
      "Train Epoch: 8 Loss: 0.583457\n",
      "\n",
      "Test set: Average loss: 0.7599, Accuracy: 6/11 (55%)\n",
      "\n",
      "------------------\n",
      "Output tensor([0.5579], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([0.])\n",
      "tensor(0.8163, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 9 Loss: 0.816305\n",
      "Output tensor([0.5536], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([0.])\n",
      "tensor(0.8064, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 9 Loss: 0.806442\n",
      "Output tensor([0.5347], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([0.])\n",
      "tensor(0.7652, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 9 Loss: 0.765156\n",
      "Output tensor([0.5110], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([0.])\n",
      "tensor(0.7155, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 9 Loss: 0.715492\n",
      "Output tensor([0.4795], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([0.])\n",
      "tensor(0.6530, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 9 Loss: 0.652952\n",
      "Output tensor([0.4632], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([0.])\n",
      "tensor(0.6222, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 9 Loss: 0.622172\n",
      "Output tensor([0.4407], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([1.])\n",
      "tensor(0.8193, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 9 Loss: 0.819318\n",
      "Output tensor([0.4440], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([1.])\n",
      "tensor(0.8118, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 9 Loss: 0.811846\n",
      "Output tensor([0.4576], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([1.])\n",
      "tensor(0.7817, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 9 Loss: 0.781664\n",
      "Output tensor([0.4769], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([1.])\n",
      "tensor(0.7405, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 9 Loss: 0.740456\n",
      "Output tensor([0.5047], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([1.])\n",
      "tensor(0.6837, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 9 Loss: 0.683733\n",
      "Output tensor([0.5183], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([1.])\n",
      "tensor(0.6573, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 9 Loss: 0.657288\n",
      "\n",
      "\n",
      "Train Epoch: 9 Loss: 0.779274\n",
      "Train Epoch: 9 Loss: 0.775998\n",
      "Train Epoch: 9 Loss: 0.781555\n",
      "Train Epoch: 9 Loss: 0.775687\n",
      "Train Epoch: 9 Loss: 0.754926\n",
      "Train Epoch: 9 Loss: 0.779068\n",
      "Train Epoch: 9 Loss: 0.612662\n",
      "Train Epoch: 9 Loss: 0.608081\n",
      "Train Epoch: 9 Loss: 0.611936\n",
      "Train Epoch: 9 Loss: 0.616313\n",
      "Train Epoch: 9 Loss: 0.598794\n",
      "Train Epoch: 9 Loss: 0.616592\n",
      "\n",
      "Test set: Average loss: 0.7555, Accuracy: 6/11 (55%)\n",
      "\n",
      "------------------\n",
      "Output tensor([0.5413], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([0.])\n",
      "tensor(0.7793, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 10 Loss: 0.779274\n",
      "Output tensor([0.5380], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([0.])\n",
      "tensor(0.7721, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 10 Loss: 0.772129\n",
      "Output tensor([0.5214], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([0.])\n",
      "tensor(0.7370, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 10 Loss: 0.736971\n",
      "Output tensor([0.5058], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([0.])\n",
      "tensor(0.7049, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 10 Loss: 0.704891\n",
      "Output tensor([0.4722], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([0.])\n",
      "tensor(0.6391, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 10 Loss: 0.639064\n",
      "Output tensor([0.4587], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([0.])\n",
      "tensor(0.6138, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 10 Loss: 0.613773\n",
      "Output tensor([0.4361], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([1.])\n",
      "tensor(0.8299, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 10 Loss: 0.829883\n",
      "Output tensor([0.4402], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([1.])\n",
      "tensor(0.8204, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 10 Loss: 0.820440\n",
      "Output tensor([0.4562], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([1.])\n",
      "tensor(0.7848, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 10 Loss: 0.784758\n",
      "Output tensor([0.4776], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([1.])\n",
      "tensor(0.7390, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 10 Loss: 0.738991\n",
      "Output tensor([0.5052], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([1.])\n",
      "tensor(0.6829, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 10 Loss: 0.682873\n",
      "Output tensor([0.5166], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([1.])\n",
      "tensor(0.6604, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 10 Loss: 0.660443\n",
      "\n",
      "\n",
      "Train Epoch: 10 Loss: 0.790092\n",
      "Train Epoch: 10 Loss: 0.765224\n",
      "Train Epoch: 10 Loss: 0.781063\n",
      "Train Epoch: 10 Loss: 0.766981\n",
      "Train Epoch: 10 Loss: 0.758713\n",
      "Train Epoch: 10 Loss: 0.775718\n",
      "Train Epoch: 10 Loss: 0.601698\n",
      "Train Epoch: 10 Loss: 0.599267\n",
      "Train Epoch: 10 Loss: 0.606310\n",
      "Train Epoch: 10 Loss: 0.620083\n",
      "Train Epoch: 10 Loss: 0.599361\n",
      "Train Epoch: 10 Loss: 0.620111\n",
      "\n",
      "Test set: Average loss: 0.7531, Accuracy: 6/11 (55%)\n",
      "\n",
      "------------------\n",
      "Output tensor([0.5462], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([0.])\n",
      "tensor(0.7901, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 11 Loss: 0.790092\n",
      "Output tensor([0.5331], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([0.])\n",
      "tensor(0.7617, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 11 Loss: 0.761741\n",
      "Output tensor([0.5275], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([0.])\n",
      "tensor(0.7498, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 11 Loss: 0.749758\n",
      "Output tensor([0.5061], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([0.])\n",
      "tensor(0.7055, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 11 Loss: 0.705481\n",
      "Output tensor([0.4820], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([0.])\n",
      "tensor(0.6579, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 11 Loss: 0.657870\n",
      "Output tensor([0.4716], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([0.])\n",
      "tensor(0.6379, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 11 Loss: 0.637902\n",
      "Output tensor([0.4604], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([1.])\n",
      "tensor(0.7756, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 11 Loss: 0.775624\n",
      "Output tensor([0.4550], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([1.])\n",
      "tensor(0.7875, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 11 Loss: 0.787510\n",
      "Output tensor([0.4666], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([1.])\n",
      "tensor(0.7624, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 11 Loss: 0.762359\n",
      "Output tensor([0.4865], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([1.])\n",
      "tensor(0.7206, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 11 Loss: 0.720554\n",
      "Output tensor([0.5038], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([1.])\n",
      "tensor(0.6856, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 11 Loss: 0.685612\n",
      "Output tensor([0.5200], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([1.])\n",
      "tensor(0.6540, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 11 Loss: 0.654004\n",
      "\n",
      "\n",
      "Train Epoch: 11 Loss: 0.754565\n",
      "Train Epoch: 11 Loss: 0.750859\n",
      "Train Epoch: 11 Loss: 0.778084\n",
      "Train Epoch: 11 Loss: 0.757768\n",
      "Train Epoch: 11 Loss: 0.724486\n",
      "Train Epoch: 11 Loss: 0.752909\n",
      "Train Epoch: 11 Loss: 0.617377\n",
      "Train Epoch: 11 Loss: 0.635609\n",
      "Train Epoch: 11 Loss: 0.616121\n",
      "Train Epoch: 11 Loss: 0.617631\n",
      "Train Epoch: 11 Loss: 0.614726\n",
      "Train Epoch: 11 Loss: 0.617934\n",
      "\n",
      "Test set: Average loss: 0.7489, Accuracy: 6/11 (55%)\n",
      "\n",
      "------------------\n",
      "Output tensor([0.5298], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([0.])\n",
      "tensor(0.7546, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 12 Loss: 0.754565\n",
      "Output tensor([0.5268], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([0.])\n",
      "tensor(0.7482, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 12 Loss: 0.748159\n",
      "Output tensor([0.5301], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([0.])\n",
      "tensor(0.7551, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 12 Loss: 0.755135\n",
      "Output tensor([0.5075], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([0.])\n",
      "tensor(0.7082, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 12 Loss: 0.708219\n",
      "Output tensor([0.4790], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([0.])\n",
      "tensor(0.6519, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 12 Loss: 0.651933\n",
      "Output tensor([0.4737], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([0.])\n",
      "tensor(0.6419, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 12 Loss: 0.641946\n",
      "Output tensor([0.4585], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([1.])\n",
      "tensor(0.7797, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 12 Loss: 0.779721\n",
      "Output tensor([0.4612], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([1.])\n",
      "tensor(0.7739, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 12 Loss: 0.773851\n",
      "Output tensor([0.4709], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([1.])\n",
      "tensor(0.7531, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 12 Loss: 0.753121\n",
      "Output tensor([0.4845], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([1.])\n",
      "tensor(0.7247, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 12 Loss: 0.724721\n",
      "Output tensor([0.4992], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([1.])\n",
      "tensor(0.6948, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 12 Loss: 0.694785\n",
      "Output tensor([0.5141], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([1.])\n",
      "tensor(0.6653, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 12 Loss: 0.665278\n",
      "\n",
      "\n",
      "Train Epoch: 12 Loss: 0.756804\n",
      "Train Epoch: 12 Loss: 0.754468\n",
      "Train Epoch: 12 Loss: 0.757068\n",
      "Train Epoch: 12 Loss: 0.756183\n",
      "Train Epoch: 12 Loss: 0.720196\n",
      "Train Epoch: 12 Loss: 0.755479\n",
      "Train Epoch: 12 Loss: 0.635341\n",
      "Train Epoch: 12 Loss: 0.633041\n",
      "Train Epoch: 12 Loss: 0.635221\n",
      "Train Epoch: 12 Loss: 0.635036\n",
      "Train Epoch: 12 Loss: 0.635032\n",
      "Train Epoch: 12 Loss: 0.635439\n",
      "\n",
      "Test set: Average loss: 0.7554, Accuracy: 6/11 (55%)\n",
      "\n",
      "------------------\n",
      "Output tensor([0.5308], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([0.])\n",
      "tensor(0.7568, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 13 Loss: 0.756804\n",
      "Output tensor([0.5286], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([0.])\n",
      "tensor(0.7520, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 13 Loss: 0.752049\n",
      "Output tensor([0.5186], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([0.])\n",
      "tensor(0.7311, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 13 Loss: 0.731095\n",
      "Output tensor([0.5062], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([0.])\n",
      "tensor(0.7057, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 13 Loss: 0.705669\n",
      "Output tensor([0.4828], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([0.])\n",
      "tensor(0.6594, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 13 Loss: 0.659412\n",
      "Output tensor([0.4775], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([0.])\n",
      "tensor(0.6492, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 13 Loss: 0.649213\n",
      "Output tensor([0.4650], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([1.])\n",
      "tensor(0.7656, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 13 Loss: 0.765640\n",
      "Output tensor([0.4645], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([1.])\n",
      "tensor(0.7669, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 13 Loss: 0.766870\n",
      "Output tensor([0.4782], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([1.])\n",
      "tensor(0.7376, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 13 Loss: 0.737636\n",
      "Output tensor([0.4886], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([1.])\n",
      "tensor(0.7161, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 13 Loss: 0.716127\n",
      "Output tensor([0.5001], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([1.])\n",
      "tensor(0.6929, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 13 Loss: 0.692883\n",
      "Output tensor([0.5121], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([1.])\n",
      "tensor(0.6693, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 13 Loss: 0.669284\n",
      "\n",
      "\n",
      "Train Epoch: 13 Loss: 0.744567\n",
      "Train Epoch: 13 Loss: 0.744508\n",
      "Train Epoch: 13 Loss: 0.744521\n",
      "Train Epoch: 13 Loss: 0.744164\n",
      "Train Epoch: 13 Loss: 0.711791\n",
      "Train Epoch: 13 Loss: 0.744301\n",
      "Train Epoch: 13 Loss: 0.644226\n",
      "Train Epoch: 13 Loss: 0.644275\n",
      "Train Epoch: 13 Loss: 0.643551\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 13 Loss: 0.642726\n",
      "Train Epoch: 13 Loss: 0.644302\n",
      "Train Epoch: 13 Loss: 0.644380\n",
      "\n",
      "Test set: Average loss: 0.7543, Accuracy: 6/11 (55%)\n",
      "\n",
      "------------------\n",
      "Output tensor([0.5251], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([0.])\n",
      "tensor(0.7446, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 14 Loss: 0.744567\n",
      "Output tensor([0.5240], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([0.])\n",
      "tensor(0.7423, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 14 Loss: 0.742308\n",
      "Output tensor([0.5163], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([0.])\n",
      "tensor(0.7264, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 14 Loss: 0.726379\n",
      "Output tensor([0.5061], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([0.])\n",
      "tensor(0.7054, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 14 Loss: 0.705397\n",
      "Output tensor([0.4845], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([0.])\n",
      "tensor(0.6626, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 14 Loss: 0.662584\n",
      "Output tensor([0.4849], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([0.])\n",
      "tensor(0.6634, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 14 Loss: 0.663425\n",
      "Output tensor([0.4721], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([1.])\n",
      "tensor(0.7506, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 14 Loss: 0.750640\n",
      "Output tensor([0.4756], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([1.])\n",
      "tensor(0.7432, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 14 Loss: 0.743164\n",
      "Output tensor([0.4816], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([1.])\n",
      "tensor(0.7306, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 14 Loss: 0.730617\n",
      "Output tensor([0.4901], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([1.])\n",
      "tensor(0.7132, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 14 Loss: 0.713190\n",
      "Output tensor([0.4994], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([1.])\n",
      "tensor(0.6944, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 14 Loss: 0.694381\n",
      "Output tensor([0.5089], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([1.])\n",
      "tensor(0.6756, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 14 Loss: 0.675553\n",
      "\n",
      "\n",
      "Train Epoch: 14 Loss: 0.734188\n",
      "Train Epoch: 14 Loss: 0.733898\n",
      "Train Epoch: 14 Loss: 0.733950\n",
      "Train Epoch: 14 Loss: 0.735916\n",
      "Train Epoch: 14 Loss: 0.727659\n",
      "Train Epoch: 14 Loss: 0.734137\n",
      "Train Epoch: 14 Loss: 0.651743\n",
      "Train Epoch: 14 Loss: 0.653997\n",
      "Train Epoch: 14 Loss: 0.649530\n",
      "Train Epoch: 14 Loss: 0.649398\n",
      "Train Epoch: 14 Loss: 0.653925\n",
      "Train Epoch: 14 Loss: 0.654104\n",
      "\n",
      "Test set: Average loss: 0.7557, Accuracy: 6/11 (55%)\n",
      "\n",
      "------------------\n",
      "Output tensor([0.5201], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([0.])\n",
      "tensor(0.7342, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 15 Loss: 0.734188\n",
      "Output tensor([0.5192], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([0.])\n",
      "tensor(0.7322, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 15 Loss: 0.732214\n",
      "Output tensor([0.5129], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([0.])\n",
      "tensor(0.7193, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 15 Loss: 0.719335\n",
      "Output tensor([0.5047], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([0.])\n",
      "tensor(0.7026, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 15 Loss: 0.702582\n",
      "Output tensor([0.4948], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([0.])\n",
      "tensor(0.6828, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 15 Loss: 0.682779\n",
      "Output tensor([0.4868], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([0.])\n",
      "tensor(0.6670, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 15 Loss: 0.667028\n",
      "Output tensor([0.4781], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([1.])\n",
      "tensor(0.7379, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 15 Loss: 0.737933\n",
      "Output tensor([0.4785], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([1.])\n",
      "tensor(0.7371, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 15 Loss: 0.737142\n",
      "Output tensor([0.4832], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([1.])\n",
      "tensor(0.7272, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 15 Loss: 0.727249\n",
      "Output tensor([0.4900], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([1.])\n",
      "tensor(0.7134, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 15 Loss: 0.713402\n",
      "Output tensor([0.4977], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([1.])\n",
      "tensor(0.6978, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 15 Loss: 0.697778\n",
      "Output tensor([0.5053], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([1.])\n",
      "tensor(0.6825, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 15 Loss: 0.682538\n",
      "\n",
      "\n",
      "Train Epoch: 15 Loss: 0.722141\n",
      "Train Epoch: 15 Loss: 0.722086\n",
      "Train Epoch: 15 Loss: 0.722113\n",
      "Train Epoch: 15 Loss: 0.722058\n",
      "Train Epoch: 15 Loss: 0.712549\n",
      "Train Epoch: 15 Loss: 0.722131\n",
      "Train Epoch: 15 Loss: 0.664975\n",
      "Train Epoch: 15 Loss: 0.664961\n",
      "Train Epoch: 15 Loss: 0.664895\n",
      "Train Epoch: 15 Loss: 0.664697\n",
      "Train Epoch: 15 Loss: 0.664979\n",
      "Train Epoch: 15 Loss: 0.665172\n",
      "\n",
      "Test set: Average loss: 0.7557, Accuracy: 6/11 (55%)\n",
      "\n",
      "------------------\n",
      "Output tensor([0.5143], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([0.])\n",
      "tensor(0.7221, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 16 Loss: 0.722141\n",
      "Output tensor([0.5138], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([0.])\n",
      "tensor(0.7211, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 16 Loss: 0.721114\n",
      "Output tensor([0.5090], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([0.])\n",
      "tensor(0.7113, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 16 Loss: 0.711268\n",
      "Output tensor([0.5023], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([0.])\n",
      "tensor(0.6978, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 16 Loss: 0.697815\n",
      "Output tensor([0.4924], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([0.])\n",
      "tensor(0.6781, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 16 Loss: 0.678091\n",
      "Output tensor([0.4879], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([0.])\n",
      "tensor(0.6693, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 16 Loss: 0.669318\n",
      "Output tensor([0.4807], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([1.])\n",
      "tensor(0.7326, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 16 Loss: 0.732589\n",
      "Output tensor([0.4810], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([1.])\n",
      "tensor(0.7318, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 16 Loss: 0.731806\n",
      "Output tensor([0.4852], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([1.])\n",
      "tensor(0.7232, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 16 Loss: 0.723200\n",
      "Output tensor([0.4913], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([1.])\n",
      "tensor(0.7108, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 16 Loss: 0.710779\n",
      "Output tensor([0.4978], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([1.])\n",
      "tensor(0.6976, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 16 Loss: 0.697637\n",
      "Output tensor([0.5047], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([1.])\n",
      "tensor(0.6837, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 16 Loss: 0.683702\n",
      "\n",
      "\n",
      "Train Epoch: 16 Loss: 0.718006\n",
      "Train Epoch: 16 Loss: 0.717922\n",
      "Train Epoch: 16 Loss: 0.717947\n",
      "Train Epoch: 16 Loss: 0.718084\n",
      "Train Epoch: 16 Loss: 0.703552\n",
      "Train Epoch: 16 Loss: 0.717979\n",
      "Train Epoch: 16 Loss: 0.668590\n",
      "Train Epoch: 16 Loss: 0.668908\n",
      "Train Epoch: 16 Loss: 0.666458\n",
      "Train Epoch: 16 Loss: 0.665497\n",
      "Train Epoch: 16 Loss: 0.668928\n",
      "Train Epoch: 16 Loss: 0.669090\n",
      "\n",
      "Test set: Average loss: 0.7546, Accuracy: 6/11 (55%)\n",
      "\n",
      "------------------\n",
      "Output tensor([0.5123], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([0.])\n",
      "tensor(0.7180, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 17 Loss: 0.718006\n",
      "Output tensor([0.5119], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([0.])\n",
      "tensor(0.7172, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 17 Loss: 0.717169\n",
      "Output tensor([0.5077], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([0.])\n",
      "tensor(0.7087, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 17 Loss: 0.708721\n",
      "Output tensor([0.5020], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([0.])\n",
      "tensor(0.6972, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 17 Loss: 0.697189\n",
      "Output tensor([0.4903], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([0.])\n",
      "tensor(0.6740, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 17 Loss: 0.674004\n",
      "Output tensor([0.4893], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([0.])\n",
      "tensor(0.6720, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 17 Loss: 0.671966\n",
      "Output tensor([0.4828], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([1.])\n",
      "tensor(0.7282, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 17 Loss: 0.728194\n",
      "Output tensor([0.4831], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([1.])\n",
      "tensor(0.7275, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 17 Loss: 0.727481\n",
      "Output tensor([0.4869], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([1.])\n",
      "tensor(0.7197, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 17 Loss: 0.719681\n",
      "Output tensor([0.4922], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([1.])\n",
      "tensor(0.7089, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 17 Loss: 0.708859\n",
      "Output tensor([0.4983], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([1.])\n",
      "tensor(0.6965, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 17 Loss: 0.696520\n",
      "Output tensor([0.5046], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([1.])\n",
      "tensor(0.6841, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 17 Loss: 0.684083\n",
      "\n",
      "\n",
      "Train Epoch: 17 Loss: 0.716653\n",
      "Train Epoch: 17 Loss: 0.716487\n",
      "Train Epoch: 17 Loss: 0.716517\n",
      "Train Epoch: 17 Loss: 0.717047\n",
      "Train Epoch: 17 Loss: 0.697972\n",
      "Train Epoch: 17 Loss: 0.716590\n",
      "Train Epoch: 17 Loss: 0.669179\n",
      "Train Epoch: 17 Loss: 0.670269\n",
      "Train Epoch: 17 Loss: 0.666456\n",
      "Train Epoch: 17 Loss: 0.666067\n",
      "Train Epoch: 17 Loss: 0.670276\n",
      "Train Epoch: 17 Loss: 0.670512\n",
      "\n",
      "Test set: Average loss: 0.7540, Accuracy: 6/11 (55%)\n",
      "\n",
      "------------------\n",
      "Output tensor([0.5116], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([0.])\n",
      "tensor(0.7167, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 18 Loss: 0.716653\n",
      "Output tensor([0.5112], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([0.])\n",
      "tensor(0.7158, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 18 Loss: 0.715797\n",
      "Output tensor([0.5074], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([0.])\n",
      "tensor(0.7081, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 18 Loss: 0.708072\n",
      "Output tensor([0.5023], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([0.])\n",
      "tensor(0.6977, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 18 Loss: 0.697660\n",
      "Output tensor([0.4895], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([0.])\n",
      "tensor(0.6723, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 18 Loss: 0.672278\n",
      "Output tensor([0.4905], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([0.])\n",
      "tensor(0.6744, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 18 Loss: 0.674423\n",
      "Output tensor([0.4845], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([1.])\n",
      "tensor(0.7246, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 18 Loss: 0.724587\n",
      "Output tensor([0.4848], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([1.])\n",
      "tensor(0.7239, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 18 Loss: 0.723939\n",
      "Output tensor([0.4883], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([1.])\n",
      "tensor(0.7168, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 18 Loss: 0.716772\n",
      "Output tensor([0.4935], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([1.])\n",
      "tensor(0.7063, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 18 Loss: 0.706323\n",
      "Output tensor([0.4989], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([1.])\n",
      "tensor(0.6953, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 18 Loss: 0.695291\n",
      "Output tensor([0.5047], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([1.])\n",
      "tensor(0.6839, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 18 Loss: 0.683857\n",
      "\n",
      "\n",
      "Train Epoch: 18 Loss: 0.716060\n",
      "Train Epoch: 18 Loss: 0.715946\n",
      "Train Epoch: 18 Loss: 0.715941\n",
      "Train Epoch: 18 Loss: 0.715980\n",
      "Train Epoch: 18 Loss: 0.692377\n",
      "Train Epoch: 18 Loss: 0.716022\n",
      "Train Epoch: 18 Loss: 0.670455\n",
      "Train Epoch: 18 Loss: 0.670737\n",
      "Train Epoch: 18 Loss: 0.668299\n",
      "Train Epoch: 18 Loss: 0.667317\n",
      "Train Epoch: 18 Loss: 0.670783\n",
      "Train Epoch: 18 Loss: 0.671045\n",
      "\n",
      "Test set: Average loss: 0.7537, Accuracy: 6/11 (55%)\n",
      "\n",
      "------------------\n",
      "Output tensor([0.5113], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([0.])\n",
      "tensor(0.7161, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 19 Loss: 0.716060\n",
      "Output tensor([0.5109], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([0.])\n",
      "tensor(0.7153, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 19 Loss: 0.715288\n",
      "Output tensor([0.5074], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([0.])\n",
      "tensor(0.7080, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 19 Loss: 0.707990\n",
      "Output tensor([0.5024], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([0.])\n",
      "tensor(0.6980, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 19 Loss: 0.698017\n",
      "Output tensor([0.4886], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([0.])\n",
      "tensor(0.6707, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 19 Loss: 0.670685\n",
      "Output tensor([0.4916], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([0.])\n",
      "tensor(0.6766, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 19 Loss: 0.676564\n",
      "Output tensor([0.4860], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([1.])\n",
      "tensor(0.7215, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 19 Loss: 0.721509\n",
      "Output tensor([0.4863], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([1.])\n",
      "tensor(0.7209, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 19 Loss: 0.720915\n",
      "Output tensor([0.4897], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([1.])\n",
      "tensor(0.7140, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 19 Loss: 0.713954\n",
      "Output tensor([0.4950], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([1.])\n",
      "tensor(0.7033, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 19 Loss: 0.703251\n",
      "Output tensor([0.5000], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([1.])\n",
      "tensor(0.6931, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 19 Loss: 0.693136\n",
      "Output tensor([0.5080], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([1.])\n",
      "tensor(0.6773, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 19 Loss: 0.677320\n",
      "\n",
      "\n",
      "Train Epoch: 19 Loss: 0.728026\n",
      "Train Epoch: 19 Loss: 0.727948\n",
      "Train Epoch: 19 Loss: 0.727463\n",
      "Train Epoch: 19 Loss: 0.727485\n",
      "Train Epoch: 19 Loss: 0.701904\n",
      "Train Epoch: 19 Loss: 0.727969\n",
      "Train Epoch: 19 Loss: 0.659460\n",
      "Train Epoch: 19 Loss: 0.659356\n",
      "Train Epoch: 19 Loss: 0.659446\n",
      "Train Epoch: 19 Loss: 0.659515\n",
      "Train Epoch: 19 Loss: 0.659459\n",
      "Train Epoch: 19 Loss: 0.659648\n",
      "\n",
      "Test set: Average loss: 0.7543, Accuracy: 6/11 (55%)\n",
      "\n",
      "------------------\n",
      "Output tensor([0.5171], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([0.])\n",
      "tensor(0.7280, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 20 Loss: 0.728026\n",
      "Output tensor([0.5156], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([0.])\n",
      "tensor(0.7249, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 20 Loss: 0.724880\n",
      "Output tensor([0.5094], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([0.])\n",
      "tensor(0.7122, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 20 Loss: 0.712193\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output tensor([0.5008], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([0.])\n",
      "tensor(0.6946, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 20 Loss: 0.694649\n",
      "Output tensor([0.4855], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([0.])\n",
      "tensor(0.6646, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 20 Loss: 0.664580\n",
      "Output tensor([0.4816], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([0.])\n",
      "tensor(0.6570, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 20 Loss: 0.656975\n",
      "Output tensor([0.4766], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([1.])\n",
      "tensor(0.7411, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 20 Loss: 0.741060\n",
      "Output tensor([0.4732], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([1.])\n",
      "tensor(0.7483, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 20 Loss: 0.748335\n",
      "Output tensor([0.4834], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([1.])\n",
      "tensor(0.7270, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 20 Loss: 0.726959\n",
      "Output tensor([0.4909], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([1.])\n",
      "tensor(0.7115, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 20 Loss: 0.711504\n",
      "Output tensor([0.4990], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([1.])\n",
      "tensor(0.6952, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 20 Loss: 0.695151\n",
      "Output tensor([0.5064], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([1.])\n",
      "tensor(0.6804, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 20 Loss: 0.680355\n",
      "\n",
      "\n",
      "Train Epoch: 20 Loss: 0.724354\n",
      "Train Epoch: 20 Loss: 0.724275\n",
      "Train Epoch: 20 Loss: 0.724345\n",
      "Train Epoch: 20 Loss: 0.724256\n",
      "Train Epoch: 20 Loss: 0.698246\n",
      "Train Epoch: 20 Loss: 0.724338\n",
      "Train Epoch: 20 Loss: 0.662900\n",
      "Train Epoch: 20 Loss: 0.662777\n",
      "Train Epoch: 20 Loss: 0.662888\n",
      "Train Epoch: 20 Loss: 0.662955\n",
      "Train Epoch: 20 Loss: 0.662898\n",
      "Train Epoch: 20 Loss: 0.663023\n",
      "\n",
      "Test set: Average loss: 0.7543, Accuracy: 6/11 (55%)\n",
      "\n",
      "------------------\n",
      "Output tensor([0.5154], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([0.])\n",
      "tensor(0.7244, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 21 Loss: 0.724354\n",
      "Output tensor([0.5148], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([0.])\n",
      "tensor(0.7233, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 21 Loss: 0.723282\n",
      "Output tensor([0.5102], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([0.])\n",
      "tensor(0.7139, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 21 Loss: 0.713859\n",
      "Output tensor([0.5039], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([0.])\n",
      "tensor(0.7011, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 21 Loss: 0.701071\n",
      "Output tensor([0.4962], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([0.])\n",
      "tensor(0.6856, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 21 Loss: 0.685566\n",
      "Output tensor([0.4905], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([0.])\n",
      "tensor(0.6744, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 21 Loss: 0.674378\n",
      "Output tensor([0.4837], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([1.])\n",
      "tensor(0.7263, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 21 Loss: 0.726258\n",
      "Output tensor([0.4841], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([1.])\n",
      "tensor(0.7255, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 21 Loss: 0.725473\n",
      "Output tensor([0.4879], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([1.])\n",
      "tensor(0.7176, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 21 Loss: 0.717621\n",
      "Output tensor([0.4932], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([1.])\n",
      "tensor(0.7069, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 21 Loss: 0.706937\n",
      "Output tensor([0.4990], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([1.])\n",
      "tensor(0.6951, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 21 Loss: 0.695128\n",
      "Output tensor([0.5043], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([1.])\n",
      "tensor(0.6845, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 21 Loss: 0.684513\n",
      "\n",
      "\n",
      "Train Epoch: 21 Loss: 0.715914\n",
      "Train Epoch: 21 Loss: 0.715799\n",
      "Train Epoch: 21 Loss: 0.715906\n",
      "Train Epoch: 21 Loss: 0.715769\n",
      "Train Epoch: 21 Loss: 0.692067\n",
      "Train Epoch: 21 Loss: 0.715899\n",
      "Train Epoch: 21 Loss: 0.670899\n",
      "Train Epoch: 21 Loss: 0.670755\n",
      "Train Epoch: 21 Loss: 0.670890\n",
      "Train Epoch: 21 Loss: 0.670966\n",
      "Train Epoch: 21 Loss: 0.670897\n",
      "Train Epoch: 21 Loss: 0.671124\n",
      "\n",
      "Test set: Average loss: 0.7543, Accuracy: 6/11 (55%)\n",
      "\n",
      "------------------\n",
      "Output tensor([0.5113], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([0.])\n",
      "tensor(0.7159, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 22 Loss: 0.715914\n",
      "Output tensor([0.5109], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([0.])\n",
      "tensor(0.7151, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 22 Loss: 0.715132\n",
      "Output tensor([0.5074], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([0.])\n",
      "tensor(0.7081, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 22 Loss: 0.708099\n",
      "Output tensor([0.5026], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([0.])\n",
      "tensor(0.6983, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 22 Loss: 0.698314\n",
      "Output tensor([0.4903], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([0.])\n",
      "tensor(0.6739, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 22 Loss: 0.673932\n",
      "Output tensor([0.4925], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([0.])\n",
      "tensor(0.6782, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 22 Loss: 0.678170\n",
      "Output tensor([0.4872], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([1.])\n",
      "tensor(0.7192, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 22 Loss: 0.719170\n",
      "Output tensor([0.4875], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([1.])\n",
      "tensor(0.7185, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 22 Loss: 0.718481\n",
      "Output tensor([0.4906], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([1.])\n",
      "tensor(0.7122, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 22 Loss: 0.712212\n",
      "Output tensor([0.4947], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([1.])\n",
      "tensor(0.7037, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 22 Loss: 0.703745\n",
      "Output tensor([0.4999], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([1.])\n",
      "tensor(0.6934, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 22 Loss: 0.693368\n",
      "Output tensor([0.5047], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([1.])\n",
      "tensor(0.6838, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 22 Loss: 0.683831\n",
      "\n",
      "\n",
      "Train Epoch: 22 Loss: 0.714987\n",
      "Train Epoch: 22 Loss: 0.714889\n",
      "Train Epoch: 22 Loss: 0.714983\n",
      "Train Epoch: 22 Loss: 0.714842\n",
      "Train Epoch: 22 Loss: 0.689764\n",
      "Train Epoch: 22 Loss: 0.714975\n",
      "Train Epoch: 22 Loss: 0.671785\n",
      "Train Epoch: 22 Loss: 0.671523\n",
      "Train Epoch: 22 Loss: 0.671774\n",
      "Train Epoch: 22 Loss: 0.671852\n",
      "Train Epoch: 22 Loss: 0.671782\n",
      "Train Epoch: 22 Loss: 0.671953\n",
      "\n",
      "Test set: Average loss: 0.7541, Accuracy: 6/11 (55%)\n",
      "\n",
      "------------------\n",
      "Output tensor([0.5108], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([0.])\n",
      "tensor(0.7150, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 23 Loss: 0.714987\n",
      "Output tensor([0.5104], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([0.])\n",
      "tensor(0.7142, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 23 Loss: 0.714166\n",
      "Output tensor([0.5072], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([0.])\n",
      "tensor(0.7076, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 23 Loss: 0.707606\n",
      "Output tensor([0.5027], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([0.])\n",
      "tensor(0.6985, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 23 Loss: 0.698546\n",
      "Output tensor([0.4899], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([0.])\n",
      "tensor(0.6732, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 23 Loss: 0.673171\n",
      "Output tensor([0.4935], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([0.])\n",
      "tensor(0.6802, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 23 Loss: 0.680216\n",
      "Output tensor([0.4888], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([1.])\n",
      "tensor(0.7159, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 23 Loss: 0.715896\n",
      "Output tensor([0.4890], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([1.])\n",
      "tensor(0.7153, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 23 Loss: 0.715336\n",
      "Output tensor([0.4916], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([1.])\n",
      "tensor(0.7100, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 23 Loss: 0.710019\n",
      "Output tensor([0.4955], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([1.])\n",
      "tensor(0.7023, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 23 Loss: 0.702277\n",
      "Output tensor([0.5000], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([1.])\n",
      "tensor(0.6931, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 23 Loss: 0.693131\n",
      "Output tensor([0.5048], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([1.])\n",
      "tensor(0.6836, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 23 Loss: 0.683560\n",
      "\n",
      "\n",
      "Train Epoch: 23 Loss: 0.713795\n",
      "Train Epoch: 23 Loss: 0.713705\n",
      "Train Epoch: 23 Loss: 0.713793\n",
      "Train Epoch: 23 Loss: 0.713669\n",
      "Train Epoch: 23 Loss: 0.688530\n",
      "Train Epoch: 23 Loss: 0.713784\n",
      "Train Epoch: 23 Loss: 0.672926\n",
      "Train Epoch: 23 Loss: 0.672008\n",
      "Train Epoch: 23 Loss: 0.672914\n",
      "Train Epoch: 23 Loss: 0.672984\n",
      "Train Epoch: 23 Loss: 0.672917\n",
      "Train Epoch: 23 Loss: 0.673083\n",
      "\n",
      "Test set: Average loss: 0.7540, Accuracy: 6/11 (55%)\n",
      "\n",
      "------------------\n",
      "Output tensor([0.5102], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([0.])\n",
      "tensor(0.7138, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 24 Loss: 0.713795\n",
      "Output tensor([0.5098], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([0.])\n",
      "tensor(0.7130, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 24 Loss: 0.712986\n",
      "Output tensor([0.5067], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([0.])\n",
      "tensor(0.7067, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 24 Loss: 0.706719\n",
      "Output tensor([0.5025], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([0.])\n",
      "tensor(0.6981, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 24 Loss: 0.698097\n",
      "Output tensor([0.4889], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([0.])\n",
      "tensor(0.6711, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 24 Loss: 0.671129\n",
      "Output tensor([0.4939], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([0.])\n",
      "tensor(0.6810, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 24 Loss: 0.680963\n",
      "Output tensor([0.4895], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([1.])\n",
      "tensor(0.7144, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 24 Loss: 0.714395\n",
      "Output tensor([0.4900], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([1.])\n",
      "tensor(0.7134, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 24 Loss: 0.713434\n",
      "Output tensor([0.4921], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([1.])\n",
      "tensor(0.7091, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 24 Loss: 0.709068\n",
      "Output tensor([0.4956], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([1.])\n",
      "tensor(0.7019, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 24 Loss: 0.701895\n",
      "Output tensor([0.5003], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([1.])\n",
      "tensor(0.6926, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 24 Loss: 0.692598\n",
      "Output tensor([0.5042], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([1.])\n",
      "tensor(0.6848, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 24 Loss: 0.684785\n",
      "\n",
      "\n",
      "Train Epoch: 24 Loss: 0.712940\n",
      "Train Epoch: 24 Loss: 0.712833\n",
      "Train Epoch: 24 Loss: 0.713309\n",
      "Train Epoch: 24 Loss: 0.712799\n",
      "Train Epoch: 24 Loss: 0.685182\n",
      "Train Epoch: 24 Loss: 0.712945\n",
      "Train Epoch: 24 Loss: 0.673752\n",
      "Train Epoch: 24 Loss: 0.667903\n",
      "Train Epoch: 24 Loss: 0.673744\n",
      "Train Epoch: 24 Loss: 0.673558\n",
      "Train Epoch: 24 Loss: 0.668056\n",
      "Train Epoch: 24 Loss: 0.673897\n",
      "\n",
      "Test set: Average loss: 0.7528, Accuracy: 6/11 (55%)\n",
      "\n",
      "------------------\n",
      "Output tensor([0.5098], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([0.])\n",
      "tensor(0.7129, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 25 Loss: 0.712940\n",
      "Output tensor([0.5094], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([0.])\n",
      "tensor(0.7121, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 25 Loss: 0.712065\n",
      "Output tensor([0.5069], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([0.])\n",
      "tensor(0.7069, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 25 Loss: 0.706949\n",
      "Output tensor([0.5021], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([0.])\n",
      "tensor(0.6974, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 25 Loss: 0.697430\n",
      "Output tensor([0.4879], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([0.])\n",
      "tensor(0.6692, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 25 Loss: 0.669228\n",
      "Output tensor([0.4939], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([0.])\n",
      "tensor(0.6810, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 25 Loss: 0.681025\n",
      "Output tensor([0.4897], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([1.])\n",
      "tensor(0.7139, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 25 Loss: 0.713865\n",
      "Output tensor([0.4912], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([1.])\n",
      "tensor(0.7108, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 25 Loss: 0.710845\n",
      "Output tensor([0.4922], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([1.])\n",
      "tensor(0.7089, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 25 Loss: 0.708884\n",
      "Output tensor([0.4956], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([1.])\n",
      "tensor(0.7021, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 25 Loss: 0.702064\n",
      "Output tensor([0.5036], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([1.])\n",
      "tensor(0.6860, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 25 Loss: 0.685978\n",
      "Output tensor([0.5041], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([1.])\n",
      "tensor(0.6849, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 25 Loss: 0.684895\n",
      "\n",
      "\n",
      "Train Epoch: 25 Loss: 0.713478\n",
      "Train Epoch: 25 Loss: 0.712602\n",
      "Train Epoch: 25 Loss: 0.723335\n",
      "Train Epoch: 25 Loss: 0.713061\n",
      "Train Epoch: 25 Loss: 0.680821\n",
      "Train Epoch: 25 Loss: 0.715379\n",
      "Train Epoch: 25 Loss: 0.673228\n",
      "Train Epoch: 25 Loss: 0.660349\n",
      "Train Epoch: 25 Loss: 0.673921\n",
      "Train Epoch: 25 Loss: 0.665205\n",
      "Train Epoch: 25 Loss: 0.660353\n",
      "Train Epoch: 25 Loss: 0.674046\n",
      "\n",
      "Test set: Average loss: 0.7514, Accuracy: 6/11 (55%)\n",
      "\n",
      "------------------\n",
      "Output tensor([0.5101], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([0.])\n",
      "tensor(0.7135, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 26 Loss: 0.713478\n",
      "Output tensor([0.5092], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([0.])\n",
      "tensor(0.7118, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 26 Loss: 0.711769\n",
      "Output tensor([0.5067], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([0.])\n",
      "tensor(0.7066, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 26 Loss: 0.706633\n",
      "Output tensor([0.5017], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([0.])\n",
      "tensor(0.6966, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 26 Loss: 0.696554\n",
      "Output tensor([0.4862], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([0.])\n",
      "tensor(0.6660, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 26 Loss: 0.665987\n",
      "Output tensor([0.4932], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([0.])\n",
      "tensor(0.6797, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 26 Loss: 0.679650\n",
      "Output tensor([0.4889], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([1.])\n",
      "tensor(0.7156, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 26 Loss: 0.715605\n",
      "Output tensor([0.4893], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([1.])\n",
      "tensor(0.7149, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 26 Loss: 0.714867\n",
      "Output tensor([0.4915], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([1.])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.7103, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 26 Loss: 0.710331\n",
      "Output tensor([0.4948], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([1.])\n",
      "tensor(0.7037, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 26 Loss: 0.703699\n",
      "Output tensor([0.4992], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([1.])\n",
      "tensor(0.6948, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 26 Loss: 0.694753\n",
      "Output tensor([0.5034], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([1.])\n",
      "tensor(0.6863, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 26 Loss: 0.686285\n",
      "\n",
      "\n",
      "Train Epoch: 26 Loss: 0.711074\n",
      "Train Epoch: 26 Loss: 0.710942\n",
      "Train Epoch: 26 Loss: 0.711069\n",
      "Train Epoch: 26 Loss: 0.710905\n",
      "Train Epoch: 26 Loss: 0.677837\n",
      "Train Epoch: 26 Loss: 0.711057\n",
      "Train Epoch: 26 Loss: 0.675550\n",
      "Train Epoch: 26 Loss: 0.670807\n",
      "Train Epoch: 26 Loss: 0.675535\n",
      "Train Epoch: 26 Loss: 0.675585\n",
      "Train Epoch: 26 Loss: 0.675505\n",
      "Train Epoch: 26 Loss: 0.675685\n",
      "\n",
      "Test set: Average loss: 0.7529, Accuracy: 6/11 (55%)\n",
      "\n",
      "------------------\n",
      "Output tensor([0.5089], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([0.])\n",
      "tensor(0.7111, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 27 Loss: 0.711074\n",
      "Output tensor([0.5084], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([0.])\n",
      "tensor(0.7102, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 27 Loss: 0.710163\n",
      "Output tensor([0.5054], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([0.])\n",
      "tensor(0.7040, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 27 Loss: 0.704050\n",
      "Output tensor([0.5013], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([0.])\n",
      "tensor(0.6958, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 27 Loss: 0.695761\n",
      "Output tensor([0.4861], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([0.])\n",
      "tensor(0.6658, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 27 Loss: 0.665809\n",
      "Output tensor([0.4933], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([0.])\n",
      "tensor(0.6799, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 27 Loss: 0.679911\n",
      "Output tensor([0.4893], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([1.])\n",
      "tensor(0.7148, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 27 Loss: 0.714755\n",
      "Output tensor([0.4918], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([1.])\n",
      "tensor(0.7097, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 27 Loss: 0.709660\n",
      "Output tensor([0.4916], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([1.])\n",
      "tensor(0.7100, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 27 Loss: 0.710029\n",
      "Output tensor([0.5009], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([1.])\n",
      "tensor(0.6914, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 27 Loss: 0.691418\n",
      "Output tensor([0.5067], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([1.])\n",
      "tensor(0.6799, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 27 Loss: 0.679890\n",
      "Output tensor([0.5137], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([1.])\n",
      "tensor(0.6662, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 27 Loss: 0.666160\n",
      "\n",
      "\n",
      "Train Epoch: 27 Loss: 0.739306\n",
      "Train Epoch: 27 Loss: 0.711641\n",
      "Train Epoch: 27 Loss: 0.739292\n",
      "Train Epoch: 27 Loss: 0.739020\n",
      "Train Epoch: 27 Loss: 0.702648\n",
      "Train Epoch: 27 Loss: 0.739276\n",
      "Train Epoch: 27 Loss: 0.649049\n",
      "Train Epoch: 27 Loss: 0.649033\n",
      "Train Epoch: 27 Loss: 0.655972\n",
      "Train Epoch: 27 Loss: 0.649056\n",
      "Train Epoch: 27 Loss: 0.649044\n",
      "Train Epoch: 27 Loss: 0.649299\n",
      "\n",
      "Test set: Average loss: 0.7521, Accuracy: 6/11 (55%)\n",
      "\n",
      "------------------\n",
      "Output tensor([0.5226], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([0.])\n",
      "tensor(0.7393, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 28 Loss: 0.739306\n",
      "Output tensor([0.5084], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([0.])\n",
      "tensor(0.7100, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 28 Loss: 0.710047\n",
      "Output tensor([0.5172], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([0.])\n",
      "tensor(0.7281, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 28 Loss: 0.728067\n",
      "Output tensor([0.5103], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([0.])\n",
      "tensor(0.7139, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 28 Loss: 0.713948\n",
      "Output tensor([0.4904], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([0.])\n",
      "tensor(0.6742, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 28 Loss: 0.674152\n",
      "Output tensor([0.4972], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([0.])\n",
      "tensor(0.6875, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 28 Loss: 0.687464\n",
      "Output tensor([0.4911], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([1.])\n",
      "tensor(0.7110, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 28 Loss: 0.711030\n",
      "Output tensor([0.4912], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([1.])\n",
      "tensor(0.7109, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 28 Loss: 0.710913\n",
      "Output tensor([0.4939], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([1.])\n",
      "tensor(0.7053, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 28 Loss: 0.705338\n",
      "Output tensor([0.4985], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([1.])\n",
      "tensor(0.6961, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 28 Loss: 0.696106\n",
      "Output tensor([0.5042], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([1.])\n",
      "tensor(0.6847, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 28 Loss: 0.684684\n",
      "Output tensor([0.5102], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([1.])\n",
      "tensor(0.6730, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 28 Loss: 0.673012\n",
      "\n",
      "\n",
      "Train Epoch: 28 Loss: 0.730062\n",
      "Train Epoch: 28 Loss: 0.704808\n",
      "Train Epoch: 28 Loss: 0.730044\n",
      "Train Epoch: 28 Loss: 0.729860\n",
      "Train Epoch: 28 Loss: 0.691595\n",
      "Train Epoch: 28 Loss: 0.730032\n",
      "Train Epoch: 28 Loss: 0.657572\n",
      "Train Epoch: 28 Loss: 0.657558\n",
      "Train Epoch: 28 Loss: 0.657565\n",
      "Train Epoch: 28 Loss: 0.657546\n",
      "Train Epoch: 28 Loss: 0.657569\n",
      "Train Epoch: 28 Loss: 0.657699\n",
      "\n",
      "Test set: Average loss: 0.7511, Accuracy: 6/11 (55%)\n",
      "\n",
      "------------------\n",
      "Output tensor([0.5181], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([0.])\n",
      "tensor(0.7301, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 29 Loss: 0.730062\n",
      "Output tensor([0.5052], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([0.])\n",
      "tensor(0.7035, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 29 Loss: 0.703510\n",
      "Output tensor([0.5137], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([0.])\n",
      "tensor(0.7209, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 29 Loss: 0.720878\n",
      "Output tensor([0.5079], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([0.])\n",
      "tensor(0.7091, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 29 Loss: 0.709074\n",
      "Output tensor([0.4904], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([0.])\n",
      "tensor(0.6742, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 29 Loss: 0.674175\n",
      "Output tensor([0.4967], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([0.])\n",
      "tensor(0.6865, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 29 Loss: 0.686502\n",
      "Output tensor([0.4916], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([1.])\n",
      "tensor(0.7102, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 29 Loss: 0.710184\n",
      "Output tensor([0.4915], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([1.])\n",
      "tensor(0.7102, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 29 Loss: 0.710216\n",
      "Output tensor([0.4940], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([1.])\n",
      "tensor(0.7052, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 29 Loss: 0.705216\n",
      "Output tensor([0.4978], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([1.])\n",
      "tensor(0.6975, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 29 Loss: 0.697498\n",
      "Output tensor([0.5026], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([1.])\n",
      "tensor(0.6880, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 29 Loss: 0.687956\n",
      "Output tensor([0.5081], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([1.])\n",
      "tensor(0.6771, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 29 Loss: 0.677119\n",
      "\n",
      "\n",
      "Train Epoch: 29 Loss: 0.722798\n",
      "Train Epoch: 29 Loss: 0.697742\n",
      "Train Epoch: 29 Loss: 0.722782\n",
      "Train Epoch: 29 Loss: 0.722536\n",
      "Train Epoch: 29 Loss: 0.687597\n",
      "Train Epoch: 29 Loss: 0.722770\n",
      "Train Epoch: 29 Loss: 0.664375\n",
      "Train Epoch: 29 Loss: 0.664362\n",
      "Train Epoch: 29 Loss: 0.664404\n",
      "Train Epoch: 29 Loss: 0.664289\n",
      "Train Epoch: 29 Loss: 0.664372\n",
      "Train Epoch: 29 Loss: 0.664510\n",
      "\n",
      "Test set: Average loss: 0.7511, Accuracy: 6/11 (55%)\n",
      "\n",
      "------------------\n",
      "Output tensor([0.5146], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([0.])\n",
      "tensor(0.7228, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 30 Loss: 0.722798\n",
      "Output tensor([0.5018], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([0.])\n",
      "tensor(0.6968, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 30 Loss: 0.696769\n",
      "Output tensor([0.5108], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([0.])\n",
      "tensor(0.7149, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 30 Loss: 0.714902\n",
      "Output tensor([0.5056], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([0.])\n",
      "tensor(0.7044, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 30 Loss: 0.704351\n",
      "Output tensor([0.4872], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([0.])\n",
      "tensor(0.6679, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 30 Loss: 0.667874\n",
      "Output tensor([0.4955], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([0.])\n",
      "tensor(0.6843, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 30 Loss: 0.684265\n",
      "Output tensor([0.4911], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([1.])\n",
      "tensor(0.7112, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 30 Loss: 0.711200\n",
      "Output tensor([0.4911], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([1.])\n",
      "tensor(0.7111, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 30 Loss: 0.711072\n",
      "Output tensor([0.4935], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([1.])\n",
      "tensor(0.7062, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 30 Loss: 0.706200\n",
      "Output tensor([0.4921], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([1.])\n",
      "tensor(0.7091, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 30 Loss: 0.709053\n",
      "Output tensor([0.5015], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([1.])\n",
      "tensor(0.6902, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 30 Loss: 0.690175\n",
      "Output tensor([0.5040], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([1.])\n",
      "tensor(0.6853, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 30 Loss: 0.685256\n",
      "\n",
      "\n",
      "Train Epoch: 30 Loss: 0.717653\n",
      "Train Epoch: 30 Loss: 0.691596\n",
      "Train Epoch: 30 Loss: 0.717583\n",
      "Train Epoch: 30 Loss: 0.716225\n",
      "Train Epoch: 30 Loss: 0.684416\n",
      "Train Epoch: 30 Loss: 0.716264\n",
      "Train Epoch: 30 Loss: 0.669731\n",
      "Train Epoch: 30 Loss: 0.669276\n",
      "Train Epoch: 30 Loss: 0.669272\n",
      "Train Epoch: 30 Loss: 0.670117\n",
      "Train Epoch: 30 Loss: 0.669248\n",
      "Train Epoch: 30 Loss: 0.670707\n",
      "\n",
      "Test set: Average loss: 0.7511, Accuracy: 6/11 (55%)\n",
      "\n",
      "------------------\n",
      "Output tensor([0.5121], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([0.])\n",
      "tensor(0.7177, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 31 Loss: 0.717653\n",
      "Output tensor([0.4999], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([0.])\n",
      "tensor(0.6929, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 31 Loss: 0.692874\n",
      "Output tensor([0.5086], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([0.])\n",
      "tensor(0.7105, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 31 Loss: 0.710450\n",
      "Output tensor([0.5035], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([0.])\n",
      "tensor(0.7002, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 31 Loss: 0.700196\n",
      "Output tensor([0.4855], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([0.])\n",
      "tensor(0.6646, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 31 Loss: 0.664565\n",
      "Output tensor([0.4912], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([0.])\n",
      "tensor(0.6756, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 31 Loss: 0.675646\n",
      "Output tensor([0.4881], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([1.])\n",
      "tensor(0.7172, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 31 Loss: 0.717155\n",
      "Output tensor([0.4916], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([1.])\n",
      "tensor(0.7101, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 31 Loss: 0.710079\n",
      "Output tensor([0.4944], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([1.])\n",
      "tensor(0.7045, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 31 Loss: 0.704490\n",
      "Output tensor([0.4987], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([1.])\n",
      "tensor(0.6957, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 31 Loss: 0.695654\n",
      "Output tensor([0.5036], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([1.])\n",
      "tensor(0.6859, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 31 Loss: 0.685879\n",
      "Output tensor([0.5086], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([1.])\n",
      "tensor(0.6761, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 31 Loss: 0.676125\n",
      "\n",
      "\n",
      "Train Epoch: 31 Loss: 0.726782\n",
      "Train Epoch: 31 Loss: 0.701122\n",
      "Train Epoch: 31 Loss: 0.726767\n",
      "Train Epoch: 31 Loss: 0.726705\n",
      "Train Epoch: 31 Loss: 0.694552\n",
      "Train Epoch: 31 Loss: 0.726755\n",
      "Train Epoch: 31 Loss: 0.660629\n",
      "Train Epoch: 31 Loss: 0.660611\n",
      "Train Epoch: 31 Loss: 0.660651\n",
      "Train Epoch: 31 Loss: 0.657709\n",
      "Train Epoch: 31 Loss: 0.660627\n",
      "Train Epoch: 31 Loss: 0.660707\n",
      "\n",
      "Test set: Average loss: 0.7512, Accuracy: 6/11 (55%)\n",
      "\n",
      "------------------\n",
      "Output tensor([0.5165], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([0.])\n",
      "tensor(0.7268, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 32 Loss: 0.726782\n",
      "Output tensor([0.5034], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([0.])\n",
      "tensor(0.7000, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 32 Loss: 0.699958\n",
      "Output tensor([0.5118], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([0.])\n",
      "tensor(0.7171, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 32 Loss: 0.717092\n",
      "Output tensor([0.5062], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([0.])\n",
      "tensor(0.7056, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 32 Loss: 0.705586\n",
      "Output tensor([0.4884], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([0.])\n",
      "tensor(0.6702, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 32 Loss: 0.670210\n",
      "Output tensor([0.4952], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([0.])\n",
      "tensor(0.6836, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 32 Loss: 0.683557\n",
      "Output tensor([0.4901], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([1.])\n",
      "tensor(0.7132, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 32 Loss: 0.713177\n",
      "Output tensor([0.4901], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([1.])\n",
      "tensor(0.7131, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 32 Loss: 0.713084\n",
      "Output tensor([0.4927], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([1.])\n",
      "tensor(0.7079, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 32 Loss: 0.707918\n",
      "Output tensor([0.4981], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([1.])\n",
      "tensor(0.6970, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 32 Loss: 0.697029\n",
      "Output tensor([0.5013], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([1.])\n",
      "tensor(0.6906, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 32 Loss: 0.690587\n",
      "Output tensor([0.5068], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([1.])\n",
      "tensor(0.6796, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 32 Loss: 0.679574\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 32 Loss: 0.719554\n",
      "Train Epoch: 32 Loss: 0.697555\n",
      "Train Epoch: 32 Loss: 0.719554\n",
      "Train Epoch: 32 Loss: 0.722571\n",
      "Train Epoch: 32 Loss: 0.688272\n",
      "Train Epoch: 32 Loss: 0.719531\n",
      "Train Epoch: 32 Loss: 0.667252\n",
      "Train Epoch: 32 Loss: 0.667424\n",
      "Train Epoch: 32 Loss: 0.667458\n",
      "Train Epoch: 32 Loss: 0.658918\n",
      "Train Epoch: 32 Loss: 0.667436\n",
      "Train Epoch: 32 Loss: 0.666064\n",
      "\n",
      "Test set: Average loss: 0.7511, Accuracy: 6/11 (55%)\n",
      "\n",
      "------------------\n",
      "Output tensor([0.5130], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([0.])\n",
      "tensor(0.7196, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 33 Loss: 0.719554\n",
      "Output tensor([0.5029], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([0.])\n",
      "tensor(0.6991, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 33 Loss: 0.699051\n",
      "Output tensor([0.5089], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([0.])\n",
      "tensor(0.7112, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 33 Loss: 0.711173\n",
      "Output tensor([0.5038], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([0.])\n",
      "tensor(0.7009, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 33 Loss: 0.700872\n",
      "Output tensor([0.4865], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([0.])\n",
      "tensor(0.6666, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 33 Loss: 0.666567\n",
      "Output tensor([0.4937], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([0.])\n",
      "tensor(0.6807, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 33 Loss: 0.680701\n",
      "Output tensor([0.4888], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([1.])\n",
      "tensor(0.7157, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 33 Loss: 0.715735\n",
      "Output tensor([0.4890], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([1.])\n",
      "tensor(0.7155, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 33 Loss: 0.715476\n",
      "Output tensor([0.4915], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([1.])\n",
      "tensor(0.7102, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 33 Loss: 0.710215\n",
      "Output tensor([0.4954], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([1.])\n",
      "tensor(0.7024, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 33 Loss: 0.702371\n",
      "Output tensor([0.5000], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([1.])\n",
      "tensor(0.6931, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 33 Loss: 0.693136\n",
      "Output tensor([0.5051], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([1.])\n",
      "tensor(0.6829, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 33 Loss: 0.682906\n",
      "\n",
      "\n",
      "Train Epoch: 33 Loss: 0.715446\n",
      "Train Epoch: 33 Loss: 0.690218\n",
      "Train Epoch: 33 Loss: 0.715433\n",
      "Train Epoch: 33 Loss: 0.715285\n",
      "Train Epoch: 33 Loss: 0.684032\n",
      "Train Epoch: 33 Loss: 0.715422\n",
      "Train Epoch: 33 Loss: 0.671356\n",
      "Train Epoch: 33 Loss: 0.671340\n",
      "Train Epoch: 33 Loss: 0.671369\n",
      "Train Epoch: 33 Loss: 0.671366\n",
      "Train Epoch: 33 Loss: 0.671353\n",
      "Train Epoch: 33 Loss: 0.671421\n",
      "\n",
      "Test set: Average loss: 0.7513, Accuracy: 6/11 (55%)\n",
      "\n",
      "------------------\n",
      "Output tensor([0.5110], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([0.])\n",
      "tensor(0.7154, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 34 Loss: 0.715446\n",
      "Output tensor([0.4983], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([0.])\n",
      "tensor(0.6897, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 34 Loss: 0.689694\n",
      "Output tensor([0.5074], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([0.])\n",
      "tensor(0.7081, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 34 Loss: 0.708077\n",
      "Output tensor([0.5027], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([0.])\n",
      "tensor(0.6985, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 34 Loss: 0.698527\n",
      "Output tensor([0.4856], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([0.])\n",
      "tensor(0.6648, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 34 Loss: 0.664780\n",
      "Output tensor([0.4931], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([0.])\n",
      "tensor(0.6794, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 34 Loss: 0.679443\n",
      "Output tensor([0.4883], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([1.])\n",
      "tensor(0.7168, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 34 Loss: 0.716835\n",
      "Output tensor([0.4885], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([1.])\n",
      "tensor(0.7165, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 34 Loss: 0.716486\n",
      "Output tensor([0.4911], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([1.])\n",
      "tensor(0.7112, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 34 Loss: 0.711164\n",
      "Output tensor([0.4949], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([1.])\n",
      "tensor(0.7033, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 34 Loss: 0.703324\n",
      "Output tensor([0.4995], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([1.])\n",
      "tensor(0.6942, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 34 Loss: 0.694196\n",
      "Output tensor([0.5045], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([1.])\n",
      "tensor(0.6842, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 34 Loss: 0.684179\n",
      "\n",
      "\n",
      "Train Epoch: 34 Loss: 0.713765\n",
      "Train Epoch: 34 Loss: 0.688527\n",
      "Train Epoch: 34 Loss: 0.713752\n",
      "Train Epoch: 34 Loss: 0.713318\n",
      "Train Epoch: 34 Loss: 0.682201\n",
      "Train Epoch: 34 Loss: 0.713741\n",
      "Train Epoch: 34 Loss: 0.672967\n",
      "Train Epoch: 34 Loss: 0.672952\n",
      "Train Epoch: 34 Loss: 0.672978\n",
      "Train Epoch: 34 Loss: 0.672969\n",
      "Train Epoch: 34 Loss: 0.672964\n",
      "Train Epoch: 34 Loss: 0.673030\n",
      "\n",
      "Test set: Average loss: 0.7512, Accuracy: 6/11 (55%)\n",
      "\n",
      "------------------\n",
      "Output tensor([0.5102], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([0.])\n",
      "tensor(0.7138, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 35 Loss: 0.713765\n",
      "Output tensor([0.4975], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([0.])\n",
      "tensor(0.6881, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 35 Loss: 0.688116\n",
      "Output tensor([0.5067], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([0.])\n",
      "tensor(0.7067, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 35 Loss: 0.706735\n",
      "Output tensor([0.5020], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([0.])\n",
      "tensor(0.6972, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 35 Loss: 0.697245\n",
      "Output tensor([0.4851], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([0.])\n",
      "tensor(0.6638, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 35 Loss: 0.663824\n",
      "Output tensor([0.4928], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([0.])\n",
      "tensor(0.6788, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 35 Loss: 0.678839\n",
      "Output tensor([0.4880], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([1.])\n",
      "tensor(0.7174, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 35 Loss: 0.717426\n",
      "Output tensor([0.4882], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([1.])\n",
      "tensor(0.7170, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 35 Loss: 0.717017\n",
      "Output tensor([0.4909], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([1.])\n",
      "tensor(0.7116, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 35 Loss: 0.711614\n",
      "Output tensor([0.4947], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([1.])\n",
      "tensor(0.7037, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 35 Loss: 0.703718\n",
      "Output tensor([0.4993], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([1.])\n",
      "tensor(0.6946, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 35 Loss: 0.694596\n",
      "Output tensor([0.5043], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([1.])\n",
      "tensor(0.6846, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 35 Loss: 0.684647\n",
      "\n",
      "\n",
      "Train Epoch: 35 Loss: 0.713127\n",
      "Train Epoch: 35 Loss: 0.687804\n",
      "Train Epoch: 35 Loss: 0.713116\n",
      "Train Epoch: 35 Loss: 0.708865\n",
      "Train Epoch: 35 Loss: 0.681259\n",
      "Train Epoch: 35 Loss: 0.713103\n",
      "Train Epoch: 35 Loss: 0.673580\n",
      "Train Epoch: 35 Loss: 0.673565\n",
      "Train Epoch: 35 Loss: 0.673588\n",
      "Train Epoch: 35 Loss: 0.673576\n",
      "Train Epoch: 35 Loss: 0.673576\n",
      "Train Epoch: 35 Loss: 0.673642\n",
      "\n",
      "Test set: Average loss: 0.7508, Accuracy: 6/11 (55%)\n",
      "\n",
      "------------------\n",
      "Output tensor([0.5099], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([0.])\n",
      "tensor(0.7131, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 36 Loss: 0.713127\n",
      "Output tensor([0.4971], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([0.])\n",
      "tensor(0.6875, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 36 Loss: 0.687453\n",
      "Output tensor([0.5065], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([0.])\n",
      "tensor(0.7062, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 36 Loss: 0.706239\n",
      "Output tensor([0.5002], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([0.])\n",
      "tensor(0.6936, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 36 Loss: 0.693565\n",
      "Output tensor([0.4842], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([0.])\n",
      "tensor(0.6620, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 36 Loss: 0.662003\n",
      "Output tensor([0.4904], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([0.])\n",
      "tensor(0.6741, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 36 Loss: 0.674137\n",
      "Output tensor([0.4819], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([1.])\n",
      "tensor(0.7300, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 36 Loss: 0.730047\n",
      "Output tensor([0.4832], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([1.])\n",
      "tensor(0.7272, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 36 Loss: 0.727238\n",
      "Output tensor([0.4885], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([1.])\n",
      "tensor(0.7165, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 36 Loss: 0.716484\n",
      "Output tensor([0.4954], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([1.])\n",
      "tensor(0.7025, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 36 Loss: 0.702467\n",
      "Output tensor([0.5030], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([1.])\n",
      "tensor(0.6872, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 36 Loss: 0.687181\n",
      "Output tensor([0.5112], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([1.])\n",
      "tensor(0.6710, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 36 Loss: 0.671046\n",
      "\n",
      "\n",
      "Train Epoch: 36 Loss: 0.734437\n",
      "Train Epoch: 36 Loss: 0.709578\n",
      "Train Epoch: 36 Loss: 0.734410\n",
      "Train Epoch: 36 Loss: 0.734372\n",
      "Train Epoch: 36 Loss: 0.704362\n",
      "Train Epoch: 36 Loss: 0.734407\n",
      "Train Epoch: 36 Loss: 0.653518\n",
      "Train Epoch: 36 Loss: 0.653508\n",
      "Train Epoch: 36 Loss: 0.653527\n",
      "Train Epoch: 36 Loss: 0.653513\n",
      "Train Epoch: 36 Loss: 0.653518\n",
      "Train Epoch: 36 Loss: 0.653577\n",
      "\n",
      "Test set: Average loss: 0.7521, Accuracy: 6/11 (55%)\n",
      "\n",
      "------------------\n",
      "Output tensor([0.5202], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([0.])\n",
      "tensor(0.7344, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 37 Loss: 0.734437\n",
      "Output tensor([0.5075], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([0.])\n",
      "tensor(0.7082, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 37 Loss: 0.708195\n",
      "Output tensor([0.5140], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([0.])\n",
      "tensor(0.7215, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 37 Loss: 0.721487\n",
      "Output tensor([0.5067], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([0.])\n",
      "tensor(0.7067, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 37 Loss: 0.706665\n",
      "Output tensor([0.4874], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([0.])\n",
      "tensor(0.6683, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 37 Loss: 0.668253\n",
      "Output tensor([0.4923], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([0.])\n",
      "tensor(0.6779, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 37 Loss: 0.677864\n",
      "Output tensor([0.4853], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([1.])\n",
      "tensor(0.7229, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 37 Loss: 0.722892\n",
      "Output tensor([0.4856], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([1.])\n",
      "tensor(0.7224, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 37 Loss: 0.722353\n",
      "Output tensor([0.4893], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([1.])\n",
      "tensor(0.7147, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 37 Loss: 0.714746\n",
      "Output tensor([0.4946], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([1.])\n",
      "tensor(0.7040, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 37 Loss: 0.703958\n",
      "Output tensor([0.5007], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([1.])\n",
      "tensor(0.6918, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 37 Loss: 0.691820\n",
      "Output tensor([0.5072], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([1.])\n",
      "tensor(0.6789, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 37 Loss: 0.678863\n",
      "\n",
      "\n",
      "Train Epoch: 37 Loss: 0.722456\n",
      "Train Epoch: 37 Loss: 0.697541\n",
      "Train Epoch: 37 Loss: 0.722434\n",
      "Train Epoch: 37 Loss: 0.722361\n",
      "Train Epoch: 37 Loss: 0.692054\n",
      "Train Epoch: 37 Loss: 0.722427\n",
      "Train Epoch: 37 Loss: 0.664696\n",
      "Train Epoch: 37 Loss: 0.664684\n",
      "Train Epoch: 37 Loss: 0.664703\n",
      "Train Epoch: 37 Loss: 0.664691\n",
      "Train Epoch: 37 Loss: 0.664694\n",
      "Train Epoch: 37 Loss: 0.664760\n",
      "\n",
      "Test set: Average loss: 0.7516, Accuracy: 6/11 (55%)\n",
      "\n",
      "------------------\n",
      "Output tensor([0.5144], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([0.])\n",
      "tensor(0.7225, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 38 Loss: 0.722456\n",
      "Output tensor([0.5018], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([0.])\n",
      "tensor(0.6968, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 38 Loss: 0.696785\n",
      "Output tensor([0.5097], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([0.])\n",
      "tensor(0.7128, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 38 Loss: 0.712798\n",
      "Output tensor([0.5039], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([0.])\n",
      "tensor(0.7010, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 38 Loss: 0.701027\n",
      "Output tensor([0.4858], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([0.])\n",
      "tensor(0.6652, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 38 Loss: 0.665187\n",
      "Output tensor([0.4921], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([0.])\n",
      "tensor(0.6775, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 38 Loss: 0.677477\n",
      "Output tensor([0.4862], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([1.])\n",
      "tensor(0.7212, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 38 Loss: 0.721194\n",
      "Output tensor([0.4864], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([1.])\n",
      "tensor(0.7206, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 38 Loss: 0.720648\n",
      "Output tensor([0.4897], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([1.])\n",
      "tensor(0.7140, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 38 Loss: 0.713970\n",
      "Output tensor([0.4944], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([1.])\n",
      "tensor(0.7045, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 38 Loss: 0.704508\n",
      "Output tensor([0.4997], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([1.])\n",
      "tensor(0.6938, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 38 Loss: 0.693764\n",
      "Output tensor([0.5054], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([1.])\n",
      "tensor(0.6824, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 38 Loss: 0.682354\n",
      "\n",
      "\n",
      "Train Epoch: 38 Loss: 0.717042\n",
      "Train Epoch: 38 Loss: 0.691854\n",
      "Train Epoch: 38 Loss: 0.717023\n",
      "Train Epoch: 38 Loss: 0.716828\n",
      "Train Epoch: 38 Loss: 0.686131\n",
      "Train Epoch: 38 Loss: 0.717014\n",
      "Train Epoch: 38 Loss: 0.669832\n",
      "Train Epoch: 38 Loss: 0.669819\n",
      "Train Epoch: 38 Loss: 0.669838\n",
      "Train Epoch: 38 Loss: 0.669841\n",
      "Train Epoch: 38 Loss: 0.669830\n",
      "Train Epoch: 38 Loss: 0.669914\n",
      "\n",
      "Test set: Average loss: 0.7514, Accuracy: 6/11 (55%)\n",
      "\n",
      "------------------\n",
      "Output tensor([0.5118], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([0.])\n",
      "tensor(0.7170, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 39 Loss: 0.717042\n",
      "Output tensor([0.4991], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([0.])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.6914, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 39 Loss: 0.691405\n",
      "Output tensor([0.5078], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([0.])\n",
      "tensor(0.7089, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 39 Loss: 0.708943\n",
      "Output tensor([0.5026], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([0.])\n",
      "tensor(0.6984, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 39 Loss: 0.698350\n",
      "Output tensor([0.4851], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([0.])\n",
      "tensor(0.6638, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 39 Loss: 0.663828\n",
      "Output tensor([0.4922], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([0.])\n",
      "tensor(0.6776, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 39 Loss: 0.677638\n",
      "Output tensor([0.4868], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([1.])\n",
      "tensor(0.7199, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 39 Loss: 0.719949\n",
      "Output tensor([0.4870], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([1.])\n",
      "tensor(0.7194, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 39 Loss: 0.719391\n",
      "Output tensor([0.4901], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([1.])\n",
      "tensor(0.7132, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 39 Loss: 0.713179\n",
      "Output tensor([0.4934], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([1.])\n",
      "tensor(0.7065, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 39 Loss: 0.706451\n",
      "Output tensor([0.4994], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([1.])\n",
      "tensor(0.6944, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 39 Loss: 0.694387\n",
      "Output tensor([0.5047], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([1.])\n",
      "tensor(0.6838, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 39 Loss: 0.683759\n",
      "\n",
      "\n",
      "Train Epoch: 39 Loss: 0.714618\n",
      "Train Epoch: 39 Loss: 0.689442\n",
      "Train Epoch: 39 Loss: 0.714604\n",
      "Train Epoch: 39 Loss: 0.714596\n",
      "Train Epoch: 39 Loss: 0.683294\n",
      "Train Epoch: 39 Loss: 0.714593\n",
      "Train Epoch: 39 Loss: 0.672147\n",
      "Train Epoch: 39 Loss: 0.672134\n",
      "Train Epoch: 39 Loss: 0.672153\n",
      "Train Epoch: 39 Loss: 0.672121\n",
      "Train Epoch: 39 Loss: 0.672145\n",
      "Train Epoch: 39 Loss: 0.672199\n",
      "\n",
      "Test set: Average loss: 0.7513, Accuracy: 6/11 (55%)\n",
      "\n",
      "------------------\n",
      "Output tensor([0.5106], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([0.])\n",
      "tensor(0.7146, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 40 Loss: 0.714618\n",
      "Output tensor([0.4980], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([0.])\n",
      "tensor(0.6891, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 40 Loss: 0.689100\n",
      "Output tensor([0.5070], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([0.])\n",
      "tensor(0.7072, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 40 Loss: 0.707250\n",
      "Output tensor([0.5023], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([0.])\n",
      "tensor(0.6978, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 40 Loss: 0.697780\n",
      "Output tensor([0.4849], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([0.])\n",
      "tensor(0.6634, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 40 Loss: 0.663386\n",
      "Output tensor([0.4924], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([0.])\n",
      "tensor(0.6780, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 40 Loss: 0.677977\n",
      "Output tensor([0.4872], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([1.])\n",
      "tensor(0.7191, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 40 Loss: 0.719073\n",
      "Output tensor([0.4875], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([1.])\n",
      "tensor(0.7185, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 40 Loss: 0.718530\n",
      "Output tensor([0.4904], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([1.])\n",
      "tensor(0.7126, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 40 Loss: 0.712572\n",
      "Output tensor([0.4946], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([1.])\n",
      "tensor(0.7041, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 40 Loss: 0.704052\n",
      "Output tensor([0.4994], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([1.])\n",
      "tensor(0.6944, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 40 Loss: 0.694447\n",
      "Output tensor([0.5045], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([1.])\n",
      "tensor(0.6841, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 40 Loss: 0.684149\n",
      "\n",
      "\n",
      "Train Epoch: 40 Loss: 0.713818\n",
      "Train Epoch: 40 Loss: 0.688408\n",
      "Train Epoch: 40 Loss: 0.713805\n",
      "Train Epoch: 40 Loss: 0.713798\n",
      "Train Epoch: 40 Loss: 0.681679\n",
      "Train Epoch: 40 Loss: 0.713793\n",
      "Train Epoch: 40 Loss: 0.672915\n",
      "Train Epoch: 40 Loss: 0.672900\n",
      "Train Epoch: 40 Loss: 0.672919\n",
      "Train Epoch: 40 Loss: 0.672886\n",
      "Train Epoch: 40 Loss: 0.672912\n",
      "Train Epoch: 40 Loss: 0.672965\n",
      "\n",
      "Test set: Average loss: 0.7512, Accuracy: 6/11 (55%)\n",
      "\n",
      "------------------\n",
      "Output tensor([0.5102], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([0.])\n",
      "tensor(0.7138, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 41 Loss: 0.713818\n",
      "Output tensor([0.4975], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([0.])\n",
      "tensor(0.6881, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 41 Loss: 0.688126\n",
      "Output tensor([0.5068], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([0.])\n",
      "tensor(0.7068, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 41 Loss: 0.706809\n",
      "Output tensor([0.5023], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([0.])\n",
      "tensor(0.6977, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 41 Loss: 0.697671\n",
      "Output tensor([0.4847], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([0.])\n",
      "tensor(0.6630, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 41 Loss: 0.662960\n",
      "Output tensor([0.4926], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([0.])\n",
      "tensor(0.6785, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 41 Loss: 0.678476\n",
      "Output tensor([0.4876], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([1.])\n",
      "tensor(0.7183, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 41 Loss: 0.718296\n",
      "Output tensor([0.4878], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([1.])\n",
      "tensor(0.7178, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 41 Loss: 0.717755\n",
      "Output tensor([0.4907], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([1.])\n",
      "tensor(0.7119, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 41 Loss: 0.711914\n",
      "Output tensor([0.4948], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([1.])\n",
      "tensor(0.7036, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 41 Loss: 0.703551\n",
      "Output tensor([0.4995], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([1.])\n",
      "tensor(0.6941, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 41 Loss: 0.694105\n",
      "Output tensor([0.5046], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([1.])\n",
      "tensor(0.6840, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 41 Loss: 0.683962\n",
      "\n",
      "\n",
      "Train Epoch: 41 Loss: 0.713845\n",
      "Train Epoch: 41 Loss: 0.688086\n",
      "Train Epoch: 41 Loss: 0.713833\n",
      "Train Epoch: 41 Loss: 0.713808\n",
      "Train Epoch: 41 Loss: 0.678066\n",
      "Train Epoch: 41 Loss: 0.713816\n",
      "Train Epoch: 41 Loss: 0.672888\n",
      "Train Epoch: 41 Loss: 0.672872\n",
      "Train Epoch: 41 Loss: 0.672890\n",
      "Train Epoch: 41 Loss: 0.672859\n",
      "Train Epoch: 41 Loss: 0.672885\n",
      "Train Epoch: 41 Loss: 0.672938\n",
      "\n",
      "Test set: Average loss: 0.7508, Accuracy: 6/11 (55%)\n",
      "\n",
      "------------------\n",
      "Output tensor([0.5102], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([0.])\n",
      "tensor(0.7138, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 42 Loss: 0.713845\n",
      "Output tensor([0.4973], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([0.])\n",
      "tensor(0.6878, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 42 Loss: 0.687824\n",
      "Output tensor([0.5069], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([0.])\n",
      "tensor(0.7070, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 42 Loss: 0.706956\n",
      "Output tensor([0.5024], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([0.])\n",
      "tensor(0.6979, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 42 Loss: 0.697904\n",
      "Output tensor([0.4832], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([0.])\n",
      "tensor(0.6601, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 42 Loss: 0.660127\n",
      "Output tensor([0.4929], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([0.])\n",
      "tensor(0.6790, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 42 Loss: 0.678960\n",
      "Output tensor([0.4879], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([1.])\n",
      "tensor(0.7177, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 42 Loss: 0.717725\n",
      "Output tensor([0.4882], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([1.])\n",
      "tensor(0.7171, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 42 Loss: 0.717127\n",
      "Output tensor([0.4911], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([1.])\n",
      "tensor(0.7112, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 42 Loss: 0.711168\n",
      "Output tensor([0.4953], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([1.])\n",
      "tensor(0.7026, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 42 Loss: 0.702642\n",
      "Output tensor([0.5001], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([1.])\n",
      "tensor(0.6930, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 42 Loss: 0.692966\n",
      "Output tensor([0.5053], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([1.])\n",
      "tensor(0.6826, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 42 Loss: 0.682564\n",
      "\n",
      "\n",
      "Train Epoch: 42 Loss: 0.715662\n",
      "Train Epoch: 42 Loss: 0.684204\n",
      "Train Epoch: 42 Loss: 0.715634\n",
      "Train Epoch: 42 Loss: 0.713941\n",
      "Train Epoch: 42 Loss: 0.672055\n",
      "Train Epoch: 42 Loss: 0.715209\n",
      "Train Epoch: 42 Loss: 0.671153\n",
      "Train Epoch: 42 Loss: 0.671132\n",
      "Train Epoch: 42 Loss: 0.671152\n",
      "Train Epoch: 42 Loss: 0.671136\n",
      "Train Epoch: 42 Loss: 0.671149\n",
      "Train Epoch: 42 Loss: 0.671212\n",
      "\n",
      "Test set: Average loss: 0.7494, Accuracy: 6/11 (55%)\n",
      "\n",
      "------------------\n",
      "Output tensor([0.5111], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([0.])\n",
      "tensor(0.7157, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 43 Loss: 0.715662\n",
      "Output tensor([0.4954], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([0.])\n",
      "tensor(0.6841, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 43 Loss: 0.684061\n",
      "Output tensor([0.5018], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([0.])\n",
      "tensor(0.6968, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 43 Loss: 0.696806\n",
      "Output tensor([0.4957], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([0.])\n",
      "tensor(0.6846, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 43 Loss: 0.684627\n",
      "Output tensor([0.4752], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([0.])\n",
      "tensor(0.6448, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 43 Loss: 0.644793\n",
      "Output tensor([0.4805], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([0.])\n",
      "tensor(0.6549, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 43 Loss: 0.654872\n",
      "Output tensor([0.4714], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([1.])\n",
      "tensor(0.7520, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 43 Loss: 0.751997\n",
      "Output tensor([0.4727], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([1.])\n",
      "tensor(0.7493, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 43 Loss: 0.749319\n",
      "Output tensor([0.4789], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([1.])\n",
      "tensor(0.7364, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 43 Loss: 0.736359\n",
      "Output tensor([0.4867], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([1.])\n",
      "tensor(0.7201, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 43 Loss: 0.720122\n",
      "Output tensor([0.4947], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([1.])\n",
      "tensor(0.7038, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 43 Loss: 0.703830\n",
      "Output tensor([0.5024], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([1.])\n",
      "tensor(0.6884, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 43 Loss: 0.688378\n",
      "\n",
      "\n",
      "Train Epoch: 43 Loss: 0.713415\n",
      "Train Epoch: 43 Loss: 0.687075\n",
      "Train Epoch: 43 Loss: 0.713403\n",
      "Train Epoch: 43 Loss: 0.713390\n",
      "Train Epoch: 43 Loss: 0.680372\n",
      "Train Epoch: 43 Loss: 0.713388\n",
      "Train Epoch: 43 Loss: 0.673301\n",
      "Train Epoch: 43 Loss: 0.673282\n",
      "Train Epoch: 43 Loss: 0.673297\n",
      "Train Epoch: 43 Loss: 0.673271\n",
      "Train Epoch: 43 Loss: 0.673298\n",
      "Train Epoch: 43 Loss: 0.673351\n",
      "\n",
      "Test set: Average loss: 0.7510, Accuracy: 6/11 (55%)\n",
      "\n",
      "------------------\n",
      "Output tensor([0.5100], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([0.])\n",
      "tensor(0.7134, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 44 Loss: 0.713415\n",
      "Output tensor([0.4970], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([0.])\n",
      "tensor(0.6872, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 44 Loss: 0.687172\n",
      "Output tensor([0.5062], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([0.])\n",
      "tensor(0.7056, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 44 Loss: 0.705563\n",
      "Output tensor([0.5006], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([0.])\n",
      "tensor(0.6944, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 44 Loss: 0.694415\n",
      "Output tensor([0.4812], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([0.])\n",
      "tensor(0.6563, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 44 Loss: 0.656323\n",
      "Output tensor([0.4881], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([0.])\n",
      "tensor(0.6696, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 44 Loss: 0.669597\n",
      "Output tensor([0.4811], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([1.])\n",
      "tensor(0.7317, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 44 Loss: 0.731723\n",
      "Output tensor([0.4818], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([1.])\n",
      "tensor(0.7303, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 44 Loss: 0.730324\n",
      "Output tensor([0.4861], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([1.])\n",
      "tensor(0.7213, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 44 Loss: 0.721298\n",
      "Output tensor([0.4920], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([1.])\n",
      "tensor(0.7093, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 44 Loss: 0.709342\n",
      "Output tensor([0.4982], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([1.])\n",
      "tensor(0.6967, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 44 Loss: 0.696698\n",
      "Output tensor([0.5046], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([1.])\n",
      "tensor(0.6840, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 44 Loss: 0.684021\n",
      "\n",
      "\n",
      "Train Epoch: 44 Loss: 0.715831\n",
      "Train Epoch: 44 Loss: 0.689240\n",
      "Train Epoch: 44 Loss: 0.715821\n",
      "Train Epoch: 44 Loss: 0.715805\n",
      "Train Epoch: 44 Loss: 0.682418\n",
      "Train Epoch: 44 Loss: 0.715804\n",
      "Train Epoch: 44 Loss: 0.670984\n",
      "Train Epoch: 44 Loss: 0.670964\n",
      "Train Epoch: 44 Loss: 0.670977\n",
      "Train Epoch: 44 Loss: 0.670953\n",
      "Train Epoch: 44 Loss: 0.670981\n",
      "Train Epoch: 44 Loss: 0.671034\n",
      "\n",
      "Test set: Average loss: 0.7510, Accuracy: 6/11 (55%)\n",
      "\n",
      "------------------\n",
      "Output tensor([0.5112], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([0.])\n",
      "tensor(0.7158, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 45 Loss: 0.715831\n",
      "Output tensor([0.4980], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([0.])\n",
      "tensor(0.6891, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 45 Loss: 0.689107\n",
      "Output tensor([0.5075], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([0.])\n",
      "tensor(0.7083, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 45 Loss: 0.708311\n",
      "Output tensor([0.5025], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([0.])\n",
      "tensor(0.6981, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 45 Loss: 0.698109\n",
      "Output tensor([0.4837], grad_fn=<SigmoidBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target tensor([0.])\n",
      "tensor(0.6611, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 45 Loss: 0.661090\n",
      "Output tensor([0.4914], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([0.])\n",
      "tensor(0.6760, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 45 Loss: 0.676044\n",
      "Output tensor([0.4854], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([1.])\n",
      "tensor(0.7229, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 45 Loss: 0.722860\n",
      "Output tensor([0.4858], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([1.])\n",
      "tensor(0.7219, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 45 Loss: 0.721935\n",
      "Output tensor([0.4894], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([1.])\n",
      "tensor(0.7146, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 45 Loss: 0.714587\n",
      "Output tensor([0.4943], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([1.])\n",
      "tensor(0.7045, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 45 Loss: 0.704517\n",
      "Output tensor([0.4998], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([1.])\n",
      "tensor(0.6935, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 45 Loss: 0.693524\n",
      "Output tensor([0.5055], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([1.])\n",
      "tensor(0.6821, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 45 Loss: 0.682145\n",
      "\n",
      "\n",
      "Train Epoch: 45 Loss: 0.716751\n",
      "Train Epoch: 45 Loss: 0.690019\n",
      "Train Epoch: 45 Loss: 0.716742\n",
      "Train Epoch: 45 Loss: 0.716724\n",
      "Train Epoch: 45 Loss: 0.683165\n",
      "Train Epoch: 45 Loss: 0.716722\n",
      "Train Epoch: 45 Loss: 0.670105\n",
      "Train Epoch: 45 Loss: 0.670083\n",
      "Train Epoch: 45 Loss: 0.670093\n",
      "Train Epoch: 45 Loss: 0.670073\n",
      "Train Epoch: 45 Loss: 0.670101\n",
      "Train Epoch: 45 Loss: 0.670154\n",
      "\n",
      "Test set: Average loss: 0.7510, Accuracy: 6/11 (55%)\n",
      "\n",
      "------------------\n",
      "Output tensor([0.5117], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([0.])\n",
      "tensor(0.7168, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 46 Loss: 0.716751\n",
      "Output tensor([0.4983], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([0.])\n",
      "tensor(0.6898, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 46 Loss: 0.689805\n",
      "Output tensor([0.5081], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([0.])\n",
      "tensor(0.7095, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 46 Loss: 0.709470\n",
      "Output tensor([0.5033], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([0.])\n",
      "tensor(0.6998, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 46 Loss: 0.699778\n",
      "Output tensor([0.4849], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([0.])\n",
      "tensor(0.6633, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 46 Loss: 0.663305\n",
      "Output tensor([0.4929], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([0.])\n",
      "tensor(0.6791, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 46 Loss: 0.679117\n",
      "Output tensor([0.4874], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([1.])\n",
      "tensor(0.7187, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 46 Loss: 0.718651\n",
      "Output tensor([0.4878], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([1.])\n",
      "tensor(0.7179, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 46 Loss: 0.717932\n",
      "Output tensor([0.4910], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([1.])\n",
      "tensor(0.7114, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 46 Loss: 0.711363\n",
      "Output tensor([0.4955], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([1.])\n",
      "tensor(0.7022, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 46 Loss: 0.702185\n",
      "Output tensor([0.5006], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([1.])\n",
      "tensor(0.6920, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 46 Loss: 0.691988\n",
      "Output tensor([0.5060], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([1.])\n",
      "tensor(0.6812, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 46 Loss: 0.681248\n",
      "\n",
      "\n",
      "Train Epoch: 46 Loss: 0.717159\n",
      "Train Epoch: 46 Loss: 0.690323\n",
      "Train Epoch: 46 Loss: 0.717151\n",
      "Train Epoch: 46 Loss: 0.717131\n",
      "Train Epoch: 46 Loss: 0.683470\n",
      "Train Epoch: 46 Loss: 0.717128\n",
      "Train Epoch: 46 Loss: 0.669714\n",
      "Train Epoch: 46 Loss: 0.669689\n",
      "Train Epoch: 46 Loss: 0.669694\n",
      "Train Epoch: 46 Loss: 0.669681\n",
      "Train Epoch: 46 Loss: 0.669710\n",
      "Train Epoch: 46 Loss: 0.669763\n",
      "\n",
      "Test set: Average loss: 0.7510, Accuracy: 6/11 (55%)\n",
      "\n",
      "------------------\n",
      "Output tensor([0.5119], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([0.])\n",
      "tensor(0.7172, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 47 Loss: 0.717159\n",
      "Output tensor([0.4985], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([0.])\n",
      "tensor(0.6901, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 47 Loss: 0.690078\n",
      "Output tensor([0.5084], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([0.])\n",
      "tensor(0.7100, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 47 Loss: 0.710024\n",
      "Output tensor([0.5037], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([0.])\n",
      "tensor(0.7006, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 47 Loss: 0.700605\n",
      "Output tensor([0.4854], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([0.])\n",
      "tensor(0.6644, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 47 Loss: 0.664382\n",
      "Output tensor([0.4937], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([0.])\n",
      "tensor(0.6807, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 47 Loss: 0.680658\n",
      "Output tensor([0.4884], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([1.])\n",
      "tensor(0.7166, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 47 Loss: 0.716558\n",
      "Output tensor([0.4887], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([1.])\n",
      "tensor(0.7159, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 47 Loss: 0.715931\n",
      "Output tensor([0.4918], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([1.])\n",
      "tensor(0.7097, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 47 Loss: 0.709731\n",
      "Output tensor([0.4961], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([1.])\n",
      "tensor(0.7010, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 47 Loss: 0.700990\n",
      "Output tensor([0.5010], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([1.])\n",
      "tensor(0.6912, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 47 Loss: 0.691183\n",
      "Output tensor([0.5062], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([1.])\n",
      "tensor(0.6808, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 47 Loss: 0.680760\n",
      "\n",
      "\n",
      "Train Epoch: 47 Loss: 0.717397\n",
      "Train Epoch: 47 Loss: 0.690460\n",
      "Train Epoch: 47 Loss: 0.717391\n",
      "Train Epoch: 47 Loss: 0.717368\n",
      "Train Epoch: 47 Loss: 0.683619\n",
      "Train Epoch: 47 Loss: 0.717362\n",
      "Train Epoch: 47 Loss: 0.669485\n",
      "Train Epoch: 47 Loss: 0.669457\n",
      "Train Epoch: 47 Loss: 0.669452\n",
      "Train Epoch: 47 Loss: 0.669452\n",
      "Train Epoch: 47 Loss: 0.669481\n",
      "Train Epoch: 47 Loss: 0.669534\n",
      "\n",
      "Test set: Average loss: 0.7510, Accuracy: 6/11 (55%)\n",
      "\n",
      "------------------\n",
      "Output tensor([0.5120], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([0.])\n",
      "tensor(0.7174, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 48 Loss: 0.717397\n",
      "Output tensor([0.4985], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([0.])\n",
      "tensor(0.6902, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 48 Loss: 0.690205\n",
      "Output tensor([0.5085], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([0.])\n",
      "tensor(0.7103, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 48 Loss: 0.710346\n",
      "Output tensor([0.5039], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([0.])\n",
      "tensor(0.7011, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 48 Loss: 0.701067\n",
      "Output tensor([0.4857], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([0.])\n",
      "tensor(0.6649, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 48 Loss: 0.664924\n",
      "Output tensor([0.4941], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([0.])\n",
      "tensor(0.6815, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 48 Loss: 0.681473\n",
      "Output tensor([0.4890], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([1.])\n",
      "tensor(0.7155, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 48 Loss: 0.715473\n",
      "Output tensor([0.4892], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([1.])\n",
      "tensor(0.7149, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 48 Loss: 0.714887\n",
      "Output tensor([0.4922], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([1.])\n",
      "tensor(0.7089, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 48 Loss: 0.708854\n",
      "Output tensor([0.4964], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([1.])\n",
      "tensor(0.7003, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 48 Loss: 0.700332\n",
      "Output tensor([0.5012], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([1.])\n",
      "tensor(0.6907, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 48 Loss: 0.690713\n",
      "Output tensor([0.5064], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([1.])\n",
      "tensor(0.6804, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 48 Loss: 0.680446\n",
      "\n",
      "\n",
      "Train Epoch: 48 Loss: 0.717587\n",
      "Train Epoch: 48 Loss: 0.690541\n",
      "Train Epoch: 48 Loss: 0.717585\n",
      "Train Epoch: 48 Loss: 0.717559\n",
      "Train Epoch: 48 Loss: 0.683714\n",
      "Train Epoch: 48 Loss: 0.717545\n",
      "Train Epoch: 48 Loss: 0.669300\n",
      "Train Epoch: 48 Loss: 0.669265\n",
      "Train Epoch: 48 Loss: 0.669232\n",
      "Train Epoch: 48 Loss: 0.669266\n",
      "Train Epoch: 48 Loss: 0.669297\n",
      "Train Epoch: 48 Loss: 0.669348\n",
      "\n",
      "Test set: Average loss: 0.7509, Accuracy: 6/11 (55%)\n",
      "\n",
      "------------------\n",
      "Output tensor([0.5121], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([0.])\n",
      "tensor(0.7176, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 49 Loss: 0.717587\n",
      "Output tensor([0.4986], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([0.])\n",
      "tensor(0.6903, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 49 Loss: 0.690284\n",
      "Output tensor([0.5086], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([0.])\n",
      "tensor(0.7106, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 49 Loss: 0.710581\n",
      "Output tensor([0.5041], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([0.])\n",
      "tensor(0.7014, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 49 Loss: 0.701368\n",
      "Output tensor([0.4858], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([0.])\n",
      "tensor(0.6652, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 49 Loss: 0.665204\n",
      "Output tensor([0.4944], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([0.])\n",
      "tensor(0.6819, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 49 Loss: 0.681931\n",
      "Output tensor([0.4892], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([1.])\n",
      "tensor(0.7149, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 49 Loss: 0.714882\n",
      "Output tensor([0.4895], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([1.])\n",
      "tensor(0.7143, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 49 Loss: 0.714310\n",
      "Output tensor([0.4925], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([1.])\n",
      "tensor(0.7083, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 49 Loss: 0.708324\n",
      "Output tensor([0.4966], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([1.])\n",
      "tensor(0.6999, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 49 Loss: 0.699933\n",
      "Output tensor([0.5014], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([1.])\n",
      "tensor(0.6904, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 49 Loss: 0.690400\n",
      "Output tensor([0.5065], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([1.])\n",
      "tensor(0.6802, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 49 Loss: 0.680201\n",
      "\n",
      "\n",
      "Train Epoch: 49 Loss: 0.717772\n",
      "Train Epoch: 49 Loss: 0.690607\n",
      "Train Epoch: 49 Loss: 0.717779\n",
      "Train Epoch: 49 Loss: 0.717749\n",
      "Train Epoch: 49 Loss: 0.683790\n",
      "Train Epoch: 49 Loss: 0.717715\n",
      "Train Epoch: 49 Loss: 0.669114\n",
      "Train Epoch: 49 Loss: 0.669060\n",
      "Train Epoch: 49 Loss: 0.668905\n",
      "Train Epoch: 49 Loss: 0.669079\n",
      "Train Epoch: 49 Loss: 0.669111\n",
      "Train Epoch: 49 Loss: 0.669162\n",
      "\n",
      "Test set: Average loss: 0.7509, Accuracy: 6/11 (55%)\n",
      "\n",
      "------------------\n",
      "Output tensor([0.5122], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([0.])\n",
      "tensor(0.7178, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 50 Loss: 0.717772\n",
      "Output tensor([0.4986], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([0.])\n",
      "tensor(0.6904, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 50 Loss: 0.690351\n",
      "Output tensor([0.5087], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([0.])\n",
      "tensor(0.7108, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 50 Loss: 0.710791\n",
      "Output tensor([0.5042], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([0.])\n",
      "tensor(0.7016, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 50 Loss: 0.701601\n",
      "Output tensor([0.4859], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([0.])\n",
      "tensor(0.6653, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 50 Loss: 0.665346\n",
      "Output tensor([0.4945], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([0.])\n",
      "tensor(0.6822, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 50 Loss: 0.682195\n",
      "Output tensor([0.4894], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([1.])\n",
      "tensor(0.7145, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 50 Loss: 0.714540\n",
      "Output tensor([0.4897], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([1.])\n",
      "tensor(0.7140, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 50 Loss: 0.713956\n",
      "Output tensor([0.4927], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([1.])\n",
      "tensor(0.7078, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 50 Loss: 0.707832\n",
      "Output tensor([0.4968], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([1.])\n",
      "tensor(0.6997, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 50 Loss: 0.699662\n",
      "Output tensor([0.5015], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([1.])\n",
      "tensor(0.6902, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 50 Loss: 0.690154\n",
      "Output tensor([0.5066], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([1.])\n",
      "tensor(0.6800, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 50 Loss: 0.679970\n",
      "\n",
      "\n",
      "Train Epoch: 50 Loss: 0.717961\n",
      "Train Epoch: 50 Loss: 0.690678\n",
      "Train Epoch: 50 Loss: 0.718008\n",
      "Train Epoch: 50 Loss: 0.717976\n",
      "Train Epoch: 50 Loss: 0.683859\n",
      "Train Epoch: 50 Loss: 0.717845\n",
      "Train Epoch: 50 Loss: 0.668863\n",
      "Train Epoch: 50 Loss: 0.668453\n",
      "Train Epoch: 50 Loss: 0.666687\n",
      "Train Epoch: 50 Loss: 0.668856\n",
      "Train Epoch: 50 Loss: 0.668889\n",
      "Train Epoch: 50 Loss: 0.668940\n",
      "\n",
      "Test set: Average loss: 0.7506, Accuracy: 6/11 (55%)\n",
      "\n",
      "------------------\n",
      "Output tensor([0.5123], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([0.])\n",
      "tensor(0.7180, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 51 Loss: 0.717961\n",
      "Output tensor([0.4986], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([0.])\n",
      "tensor(0.6904, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 51 Loss: 0.690423\n",
      "Output tensor([0.5089], grad_fn=<SigmoidBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target tensor([0.])\n",
      "tensor(0.7110, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 51 Loss: 0.711010\n",
      "Output tensor([0.5043], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([0.])\n",
      "tensor(0.7018, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 51 Loss: 0.701814\n",
      "Output tensor([0.4859], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([0.])\n",
      "tensor(0.6654, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 51 Loss: 0.665383\n",
      "Output tensor([0.4945], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([0.])\n",
      "tensor(0.6822, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 51 Loss: 0.682178\n",
      "Output tensor([0.4895], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([1.])\n",
      "tensor(0.7143, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 51 Loss: 0.714313\n",
      "Output tensor([0.4901], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([1.])\n",
      "tensor(0.7132, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 51 Loss: 0.713216\n",
      "Output tensor([0.4948], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([1.])\n",
      "tensor(0.7037, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 51 Loss: 0.703697\n",
      "Output tensor([0.4969], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([1.])\n",
      "tensor(0.6993, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 51 Loss: 0.699336\n",
      "Output tensor([0.5017], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([1.])\n",
      "tensor(0.6898, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 51 Loss: 0.689809\n",
      "Output tensor([0.5070], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([1.])\n",
      "tensor(0.6793, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 51 Loss: 0.679318\n",
      "\n",
      "\n",
      "Train Epoch: 51 Loss: 0.717889\n",
      "Train Epoch: 51 Loss: 0.691966\n",
      "Train Epoch: 51 Loss: 0.718858\n",
      "Train Epoch: 51 Loss: 0.719043\n",
      "Train Epoch: 51 Loss: 0.684320\n",
      "Train Epoch: 51 Loss: 0.716604\n",
      "Train Epoch: 51 Loss: 0.657069\n",
      "Train Epoch: 51 Loss: 0.656608\n",
      "Train Epoch: 51 Loss: 0.656585\n",
      "Train Epoch: 51 Loss: 0.665077\n",
      "Train Epoch: 51 Loss: 0.667850\n",
      "Train Epoch: 51 Loss: 0.667891\n",
      "\n",
      "Test set: Average loss: 0.7473, Accuracy: 6/11 (55%)\n",
      "\n",
      "------------------\n",
      "Output tensor([0.5122], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([0.])\n",
      "tensor(0.7179, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 52 Loss: 0.717889\n",
      "Output tensor([0.4992], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([0.])\n",
      "tensor(0.6915, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 52 Loss: 0.691488\n",
      "Output tensor([0.5065], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([0.])\n",
      "tensor(0.7063, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 52 Loss: 0.706285\n",
      "Output tensor([0.5006], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([0.])\n",
      "tensor(0.6944, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 52 Loss: 0.694419\n",
      "Output tensor([0.4841], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([0.])\n",
      "tensor(0.6619, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 52 Loss: 0.661909\n",
      "Output tensor([0.4856], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([0.])\n",
      "tensor(0.6648, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 52 Loss: 0.664826\n",
      "Output tensor([0.4774], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([1.])\n",
      "tensor(0.7395, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 52 Loss: 0.739468\n",
      "Output tensor([0.4833], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([1.])\n",
      "tensor(0.7272, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 52 Loss: 0.727220\n",
      "Output tensor([0.4915], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([1.])\n",
      "tensor(0.7104, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 52 Loss: 0.710370\n",
      "Output tensor([0.5014], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([1.])\n",
      "tensor(0.6903, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 52 Loss: 0.690293\n",
      "Output tensor([0.5126], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([1.])\n",
      "tensor(0.6682, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 52 Loss: 0.668175\n",
      "Output tensor([0.5249], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([1.])\n",
      "tensor(0.6446, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 52 Loss: 0.644571\n",
      "\n",
      "\n",
      "Train Epoch: 52 Loss: 0.773031\n",
      "Train Epoch: 52 Loss: 0.742077\n",
      "Train Epoch: 52 Loss: 0.767203\n",
      "Train Epoch: 52 Loss: 0.772902\n",
      "Train Epoch: 52 Loss: 0.735767\n",
      "Train Epoch: 52 Loss: 0.773042\n",
      "Train Epoch: 52 Loss: 0.619168\n",
      "Train Epoch: 52 Loss: 0.619173\n",
      "Train Epoch: 52 Loss: 0.619179\n",
      "Train Epoch: 52 Loss: 0.619156\n",
      "Train Epoch: 52 Loss: 0.619174\n",
      "Train Epoch: 52 Loss: 0.619232\n",
      "\n",
      "Test set: Average loss: 0.7526, Accuracy: 6/11 (55%)\n",
      "\n",
      "------------------\n",
      "Output tensor([0.5384], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([0.])\n",
      "tensor(0.7730, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 53 Loss: 0.773031\n",
      "Output tensor([0.5222], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([0.])\n",
      "tensor(0.7385, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 53 Loss: 0.738532\n",
      "Output tensor([0.5265], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([0.])\n",
      "tensor(0.7477, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 53 Loss: 0.747688\n",
      "Output tensor([0.5036], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([0.])\n",
      "tensor(0.7003, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 53 Loss: 0.700342\n",
      "Output tensor([0.4812], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([0.])\n",
      "tensor(0.6562, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 53 Loss: 0.656202\n",
      "Output tensor([0.4874], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([0.])\n",
      "tensor(0.6682, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 53 Loss: 0.668231\n",
      "Output tensor([0.4791], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([1.])\n",
      "tensor(0.7359, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 53 Loss: 0.735944\n",
      "Output tensor([0.4844], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([1.])\n",
      "tensor(0.7247, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 53 Loss: 0.724748\n",
      "Output tensor([0.4956], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([1.])\n",
      "tensor(0.7019, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 53 Loss: 0.701895\n",
      "Output tensor([0.5032], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([1.])\n",
      "tensor(0.6867, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 53 Loss: 0.686744\n",
      "Output tensor([0.5122], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([1.])\n",
      "tensor(0.6690, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 53 Loss: 0.668984\n",
      "Output tensor([0.5224], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([1.])\n",
      "tensor(0.6494, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 53 Loss: 0.649386\n",
      "\n",
      "\n",
      "Train Epoch: 53 Loss: 0.763340\n",
      "Train Epoch: 53 Loss: 0.732249\n",
      "Train Epoch: 53 Loss: 0.763361\n",
      "Train Epoch: 53 Loss: 0.763162\n",
      "Train Epoch: 53 Loss: 0.726238\n",
      "Train Epoch: 53 Loss: 0.763350\n",
      "Train Epoch: 53 Loss: 0.627554\n",
      "Train Epoch: 53 Loss: 0.627561\n",
      "Train Epoch: 53 Loss: 0.627565\n",
      "Train Epoch: 53 Loss: 0.627548\n",
      "Train Epoch: 53 Loss: 0.627552\n",
      "Train Epoch: 53 Loss: 0.627608\n",
      "\n",
      "Test set: Average loss: 0.7525, Accuracy: 6/11 (55%)\n",
      "\n",
      "------------------\n",
      "Output tensor([0.5339], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([0.])\n",
      "tensor(0.7633, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 54 Loss: 0.763340\n",
      "Output tensor([0.5178], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([0.])\n",
      "tensor(0.7293, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 54 Loss: 0.729309\n",
      "Output tensor([0.5247], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([0.])\n",
      "tensor(0.7438, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 54 Loss: 0.743828\n",
      "Output tensor([0.5149], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([0.])\n",
      "tensor(0.7233, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 54 Loss: 0.723312\n",
      "Output tensor([0.4911], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([0.])\n",
      "tensor(0.6754, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 54 Loss: 0.675423\n",
      "Output tensor([0.4964], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([0.])\n",
      "tensor(0.6861, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 54 Loss: 0.686060\n",
      "Output tensor([0.4880], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([1.])\n",
      "tensor(0.7175, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 54 Loss: 0.717510\n",
      "Output tensor([0.4882], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([1.])\n",
      "tensor(0.7171, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 54 Loss: 0.717132\n",
      "Output tensor([0.4924], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([1.])\n",
      "tensor(0.7085, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 54 Loss: 0.708469\n",
      "Output tensor([0.4986], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([1.])\n",
      "tensor(0.6960, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 54 Loss: 0.696004\n",
      "Output tensor([0.5057], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([1.])\n",
      "tensor(0.6819, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 54 Loss: 0.681884\n",
      "Output tensor([0.5134], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([1.])\n",
      "tensor(0.6667, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 54 Loss: 0.666665\n",
      "\n",
      "\n",
      "Train Epoch: 54 Loss: 0.738323\n",
      "Train Epoch: 54 Loss: 0.708526\n",
      "Train Epoch: 54 Loss: 0.738319\n",
      "Train Epoch: 54 Loss: 0.737751\n",
      "Train Epoch: 54 Loss: 0.702681\n",
      "Train Epoch: 54 Loss: 0.738309\n",
      "Train Epoch: 54 Loss: 0.649940\n",
      "Train Epoch: 54 Loss: 0.649920\n",
      "Train Epoch: 54 Loss: 0.649950\n",
      "Train Epoch: 54 Loss: 0.649926\n",
      "Train Epoch: 54 Loss: 0.649936\n",
      "Train Epoch: 54 Loss: 0.649997\n",
      "\n",
      "Test set: Average loss: 0.7512, Accuracy: 6/11 (55%)\n",
      "\n",
      "------------------\n",
      "Output tensor([0.5221], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([0.])\n",
      "tensor(0.7383, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 55 Loss: 0.738323\n",
      "Output tensor([0.5070], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([0.])\n",
      "tensor(0.7072, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 55 Loss: 0.707213\n",
      "Output tensor([0.5160], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([0.])\n",
      "tensor(0.7257, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 55 Loss: 0.725664\n",
      "Output tensor([0.5087], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([0.])\n",
      "tensor(0.7107, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 55 Loss: 0.710652\n",
      "Output tensor([0.4874], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([0.])\n",
      "tensor(0.6683, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 55 Loss: 0.668316\n",
      "Output tensor([0.4948], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([0.])\n",
      "tensor(0.6827, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 55 Loss: 0.682750\n",
      "Output tensor([0.4879], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([1.])\n",
      "tensor(0.7177, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 55 Loss: 0.717725\n",
      "Output tensor([0.4881], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([1.])\n",
      "tensor(0.7172, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 55 Loss: 0.717154\n",
      "Output tensor([0.4918], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([1.])\n",
      "tensor(0.7097, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 55 Loss: 0.709664\n",
      "Output tensor([0.4971], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([1.])\n",
      "tensor(0.6989, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 55 Loss: 0.698943\n",
      "Output tensor([0.5032], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([1.])\n",
      "tensor(0.6868, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 55 Loss: 0.686831\n",
      "Output tensor([0.5094], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([1.])\n",
      "tensor(0.6746, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 55 Loss: 0.674561\n",
      "\n",
      "\n",
      "Train Epoch: 55 Loss: 0.727714\n",
      "Train Epoch: 55 Loss: 0.698262\n",
      "Train Epoch: 55 Loss: 0.727676\n",
      "Train Epoch: 55 Loss: 0.722479\n",
      "Train Epoch: 55 Loss: 0.690172\n",
      "Train Epoch: 55 Loss: 0.727667\n",
      "Train Epoch: 55 Loss: 0.659781\n",
      "Train Epoch: 55 Loss: 0.659721\n",
      "Train Epoch: 55 Loss: 0.659791\n",
      "Train Epoch: 55 Loss: 0.659779\n",
      "Train Epoch: 55 Loss: 0.659776\n",
      "Train Epoch: 55 Loss: 0.659899\n",
      "\n",
      "Test set: Average loss: 0.7502, Accuracy: 6/11 (55%)\n",
      "\n",
      "------------------\n",
      "Output tensor([0.5170], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([0.])\n",
      "tensor(0.7277, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 56 Loss: 0.727714\n",
      "Output tensor([0.5022], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([0.])\n",
      "tensor(0.6976, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 56 Loss: 0.697560\n",
      "Output tensor([0.5122], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([0.])\n",
      "tensor(0.7179, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 56 Loss: 0.717858\n",
      "Output tensor([0.5052], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([0.])\n",
      "tensor(0.7036, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 56 Loss: 0.703628\n",
      "Output tensor([0.4840], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([0.])\n",
      "tensor(0.6617, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 56 Loss: 0.661724\n",
      "Output tensor([0.4940], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([0.])\n",
      "tensor(0.6812, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 56 Loss: 0.681160\n",
      "Output tensor([0.4859], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([1.])\n",
      "tensor(0.7217, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 56 Loss: 0.721677\n",
      "Output tensor([0.4886], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([1.])\n",
      "tensor(0.7163, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 56 Loss: 0.716265\n",
      "Output tensor([0.4920], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([1.])\n",
      "tensor(0.7093, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 56 Loss: 0.709274\n",
      "Output tensor([0.4965], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([1.])\n",
      "tensor(0.7002, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 56 Loss: 0.700187\n",
      "Output tensor([0.5026], grad_fn=<SigmoidBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target tensor([1.])\n",
      "tensor(0.6880, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 56 Loss: 0.687957\n",
      "Output tensor([0.5072], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([1.])\n",
      "tensor(0.6788, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 56 Loss: 0.678847\n",
      "\n",
      "\n",
      "Train Epoch: 56 Loss: 0.724582\n",
      "Train Epoch: 56 Loss: 0.695487\n",
      "Train Epoch: 56 Loss: 0.724483\n",
      "Train Epoch: 56 Loss: 0.724427\n",
      "Train Epoch: 56 Loss: 0.688937\n",
      "Train Epoch: 56 Loss: 0.724475\n",
      "Train Epoch: 56 Loss: 0.662773\n",
      "Train Epoch: 56 Loss: 0.662627\n",
      "Train Epoch: 56 Loss: 0.662783\n",
      "Train Epoch: 56 Loss: 0.662756\n",
      "Train Epoch: 56 Loss: 0.662762\n",
      "Train Epoch: 56 Loss: 0.662819\n",
      "\n",
      "Test set: Average loss: 0.7508, Accuracy: 6/11 (55%)\n",
      "\n",
      "------------------\n",
      "Output tensor([0.5155], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([0.])\n",
      "tensor(0.7246, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 57 Loss: 0.724582\n",
      "Output tensor([0.5009], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([0.])\n",
      "tensor(0.6949, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 57 Loss: 0.694898\n",
      "Output tensor([0.5110], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([0.])\n",
      "tensor(0.7155, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 57 Loss: 0.715467\n",
      "Output tensor([0.5056], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([0.])\n",
      "tensor(0.7044, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 57 Loss: 0.704438\n",
      "Output tensor([0.4857], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([0.])\n",
      "tensor(0.6650, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 57 Loss: 0.665021\n",
      "Output tensor([0.4943], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([0.])\n",
      "tensor(0.6817, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 57 Loss: 0.681724\n",
      "Output tensor([0.4884], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([1.])\n",
      "tensor(0.7165, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 57 Loss: 0.716530\n",
      "Output tensor([0.4887], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([1.])\n",
      "tensor(0.7159, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 57 Loss: 0.715907\n",
      "Output tensor([0.4920], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([1.])\n",
      "tensor(0.7094, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 57 Loss: 0.709374\n",
      "Output tensor([0.4966], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([1.])\n",
      "tensor(0.7000, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 57 Loss: 0.699979\n",
      "Output tensor([0.5019], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([1.])\n",
      "tensor(0.6894, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 57 Loss: 0.689425\n",
      "Output tensor([0.5076], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([1.])\n",
      "tensor(0.6781, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 57 Loss: 0.678145\n",
      "\n",
      "\n",
      "Train Epoch: 57 Loss: 0.721266\n",
      "Train Epoch: 57 Loss: 0.692280\n",
      "Train Epoch: 57 Loss: 0.721196\n",
      "Train Epoch: 57 Loss: 0.721154\n",
      "Train Epoch: 57 Loss: 0.685824\n",
      "Train Epoch: 57 Loss: 0.721188\n",
      "Train Epoch: 57 Loss: 0.665873\n",
      "Train Epoch: 57 Loss: 0.665756\n",
      "Train Epoch: 57 Loss: 0.665883\n",
      "Train Epoch: 57 Loss: 0.665848\n",
      "Train Epoch: 57 Loss: 0.665865\n",
      "Train Epoch: 57 Loss: 0.665920\n",
      "\n",
      "Test set: Average loss: 0.7507, Accuracy: 6/11 (55%)\n",
      "\n",
      "------------------\n",
      "Output tensor([0.5139], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([0.])\n",
      "tensor(0.7213, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 58 Loss: 0.721266\n",
      "Output tensor([0.4994], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([0.])\n",
      "tensor(0.6919, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 58 Loss: 0.691901\n",
      "Output tensor([0.5099], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([0.])\n",
      "tensor(0.7132, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 58 Loss: 0.713197\n",
      "Output tensor([0.5049], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([0.])\n",
      "tensor(0.7030, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 58 Loss: 0.702958\n",
      "Output tensor([0.4854], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([0.])\n",
      "tensor(0.6644, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 58 Loss: 0.664364\n",
      "Output tensor([0.4943], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([0.])\n",
      "tensor(0.6817, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 58 Loss: 0.681716\n",
      "Output tensor([0.4887], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([1.])\n",
      "tensor(0.7160, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 58 Loss: 0.715966\n",
      "Output tensor([0.4890], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([1.])\n",
      "tensor(0.7153, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 58 Loss: 0.715336\n",
      "Output tensor([0.4921], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([1.])\n",
      "tensor(0.7091, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 58 Loss: 0.709068\n",
      "Output tensor([0.4966], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([1.])\n",
      "tensor(0.7000, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 58 Loss: 0.700027\n",
      "Output tensor([0.5016], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([1.])\n",
      "tensor(0.6899, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 58 Loss: 0.689872\n",
      "Output tensor([0.5071], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([1.])\n",
      "tensor(0.6790, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 58 Loss: 0.679017\n",
      "\n",
      "\n",
      "Train Epoch: 58 Loss: 0.719859\n",
      "Train Epoch: 58 Loss: 0.690830\n",
      "Train Epoch: 58 Loss: 0.719786\n",
      "Train Epoch: 58 Loss: 0.719717\n",
      "Train Epoch: 58 Loss: 0.684364\n",
      "Train Epoch: 58 Loss: 0.719778\n",
      "Train Epoch: 58 Loss: 0.667209\n",
      "Train Epoch: 58 Loss: 0.667078\n",
      "Train Epoch: 58 Loss: 0.667218\n",
      "Train Epoch: 58 Loss: 0.667176\n",
      "Train Epoch: 58 Loss: 0.667200\n",
      "Train Epoch: 58 Loss: 0.667257\n",
      "\n",
      "Test set: Average loss: 0.7507, Accuracy: 6/11 (55%)\n",
      "\n",
      "------------------\n",
      "Output tensor([0.5132], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([0.])\n",
      "tensor(0.7199, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 59 Loss: 0.719859\n",
      "Output tensor([0.4987], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([0.])\n",
      "tensor(0.6905, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 59 Loss: 0.690528\n",
      "Output tensor([0.5094], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([0.])\n",
      "tensor(0.7122, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 59 Loss: 0.712216\n",
      "Output tensor([0.5046], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([0.])\n",
      "tensor(0.7024, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 59 Loss: 0.702351\n",
      "Output tensor([0.4853], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([0.])\n",
      "tensor(0.6641, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 59 Loss: 0.664108\n",
      "Output tensor([0.4943], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([0.])\n",
      "tensor(0.6819, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 59 Loss: 0.681855\n",
      "Output tensor([0.4889], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([1.])\n",
      "tensor(0.7155, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 59 Loss: 0.715532\n",
      "Output tensor([0.4892], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([1.])\n",
      "tensor(0.7149, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 59 Loss: 0.714891\n",
      "Output tensor([0.4922], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([1.])\n",
      "tensor(0.7088, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 59 Loss: 0.708769\n",
      "Output tensor([0.4966], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([1.])\n",
      "tensor(0.6999, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 59 Loss: 0.699911\n",
      "Output tensor([0.5016], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([1.])\n",
      "tensor(0.6900, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 59 Loss: 0.689957\n",
      "Output tensor([0.5070], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([1.])\n",
      "tensor(0.6793, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 59 Loss: 0.679318\n",
      "\n",
      "\n",
      "Train Epoch: 59 Loss: 0.719306\n",
      "Train Epoch: 59 Loss: 0.690091\n",
      "Train Epoch: 59 Loss: 0.719231\n",
      "Train Epoch: 59 Loss: 0.719036\n",
      "Train Epoch: 59 Loss: 0.683688\n",
      "Train Epoch: 59 Loss: 0.719223\n",
      "Train Epoch: 59 Loss: 0.667736\n",
      "Train Epoch: 59 Loss: 0.667582\n",
      "Train Epoch: 59 Loss: 0.667744\n",
      "Train Epoch: 59 Loss: 0.667701\n",
      "Train Epoch: 59 Loss: 0.667726\n",
      "Train Epoch: 59 Loss: 0.667789\n",
      "\n",
      "Test set: Average loss: 0.7506, Accuracy: 6/11 (55%)\n",
      "\n",
      "------------------\n",
      "Output tensor([0.5129], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([0.])\n",
      "tensor(0.7193, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 60 Loss: 0.719306\n",
      "Output tensor([0.4983], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([0.])\n",
      "tensor(0.6898, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 60 Loss: 0.689829\n",
      "Output tensor([0.5093], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([0.])\n",
      "tensor(0.7119, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 60 Loss: 0.711874\n",
      "Output tensor([0.5043], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([0.])\n",
      "tensor(0.7018, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 60 Loss: 0.701848\n",
      "Output tensor([0.4852], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([0.])\n",
      "tensor(0.6640, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 60 Loss: 0.664018\n",
      "Output tensor([0.4944], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([0.])\n",
      "tensor(0.6821, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 60 Loss: 0.682052\n",
      "Output tensor([0.4891], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([1.])\n",
      "tensor(0.7152, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 60 Loss: 0.715172\n",
      "Output tensor([0.4894], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([1.])\n",
      "tensor(0.7145, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 60 Loss: 0.714499\n",
      "Output tensor([0.4924], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([1.])\n",
      "tensor(0.7085, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 60 Loss: 0.708455\n",
      "Output tensor([0.4962], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([1.])\n",
      "tensor(0.7007, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 60 Loss: 0.700733\n",
      "Output tensor([0.5017], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([1.])\n",
      "tensor(0.6898, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 60 Loss: 0.689843\n",
      "Output tensor([0.5070], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([1.])\n",
      "tensor(0.6793, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 60 Loss: 0.679327\n",
      "\n",
      "\n",
      "Train Epoch: 60 Loss: 0.719173\n",
      "Train Epoch: 60 Loss: 0.689929\n",
      "Train Epoch: 60 Loss: 0.719096\n",
      "Train Epoch: 60 Loss: 0.719000\n",
      "Train Epoch: 60 Loss: 0.683436\n",
      "Train Epoch: 60 Loss: 0.719087\n",
      "Train Epoch: 60 Loss: 0.667865\n",
      "Train Epoch: 60 Loss: 0.667663\n",
      "Train Epoch: 60 Loss: 0.667873\n",
      "Train Epoch: 60 Loss: 0.667817\n",
      "Train Epoch: 60 Loss: 0.667855\n",
      "Train Epoch: 60 Loss: 0.667912\n",
      "\n",
      "Test set: Average loss: 0.7506, Accuracy: 6/11 (55%)\n",
      "\n",
      "------------------\n",
      "Output tensor([0.5128], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([0.])\n",
      "tensor(0.7192, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 61 Loss: 0.719173\n",
      "Output tensor([0.4983], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([0.])\n",
      "tensor(0.6897, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 61 Loss: 0.689698\n",
      "Output tensor([0.5092], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([0.])\n",
      "tensor(0.7118, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 61 Loss: 0.711809\n",
      "Output tensor([0.5045], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([0.])\n",
      "tensor(0.7023, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 61 Loss: 0.702253\n",
      "Output tensor([0.4853], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([0.])\n",
      "tensor(0.6641, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 61 Loss: 0.664080\n",
      "Output tensor([0.4945], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([0.])\n",
      "tensor(0.6822, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 61 Loss: 0.682214\n",
      "Output tensor([0.4892], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([1.])\n",
      "tensor(0.7150, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 61 Loss: 0.714959\n",
      "Output tensor([0.4896], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([1.])\n",
      "tensor(0.7142, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 61 Loss: 0.714245\n",
      "Output tensor([0.4925], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([1.])\n",
      "tensor(0.7083, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 61 Loss: 0.708295\n",
      "Output tensor([0.4968], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([1.])\n",
      "tensor(0.6996, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 61 Loss: 0.699553\n",
      "Output tensor([0.5017], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([1.])\n",
      "tensor(0.6897, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 61 Loss: 0.689749\n",
      "Output tensor([0.5070], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([1.])\n",
      "tensor(0.6792, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 61 Loss: 0.679248\n",
      "\n",
      "\n",
      "Train Epoch: 61 Loss: 0.719215\n",
      "Train Epoch: 61 Loss: 0.689839\n",
      "Train Epoch: 61 Loss: 0.719134\n",
      "Train Epoch: 61 Loss: 0.719059\n",
      "Train Epoch: 61 Loss: 0.683339\n",
      "Train Epoch: 61 Loss: 0.719126\n",
      "Train Epoch: 61 Loss: 0.667828\n",
      "Train Epoch: 61 Loss: 0.667449\n",
      "Train Epoch: 61 Loss: 0.667836\n",
      "Train Epoch: 61 Loss: 0.667773\n",
      "Train Epoch: 61 Loss: 0.667818\n",
      "Train Epoch: 61 Loss: 0.667874\n",
      "\n",
      "Test set: Average loss: 0.7506, Accuracy: 6/11 (55%)\n",
      "\n",
      "------------------\n",
      "Output tensor([0.5129], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([0.])\n",
      "tensor(0.7192, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 62 Loss: 0.719215\n",
      "Output tensor([0.4982], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([0.])\n",
      "tensor(0.6896, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 62 Loss: 0.689594\n",
      "Output tensor([0.5093], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([0.])\n",
      "tensor(0.7119, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 62 Loss: 0.711905\n",
      "Output tensor([0.5046], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([0.])\n",
      "tensor(0.7023, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 62 Loss: 0.702343\n",
      "Output tensor([0.4853], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([0.])\n",
      "tensor(0.6641, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 62 Loss: 0.664103\n",
      "Output tensor([0.4946], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([0.])\n",
      "tensor(0.6824, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 62 Loss: 0.682414\n",
      "Output tensor([0.4893], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([1.])\n",
      "tensor(0.7147, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 62 Loss: 0.714730\n",
      "Output tensor([0.4898], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([1.])\n",
      "tensor(0.7138, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 62 Loss: 0.713825\n",
      "Output tensor([0.4926], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([1.])\n",
      "tensor(0.7081, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 62 Loss: 0.708085\n",
      "Output tensor([0.4969], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([1.])\n",
      "tensor(0.6994, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 62 Loss: 0.699358\n",
      "Output tensor([0.5018], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([1.])\n",
      "tensor(0.6896, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 62 Loss: 0.689560\n",
      "Output tensor([0.5071], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([1.])\n",
      "tensor(0.6791, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 62 Loss: 0.679073\n",
      "\n",
      "\n",
      "Train Epoch: 62 Loss: 0.719472\n",
      "Train Epoch: 62 Loss: 0.689648\n",
      "Train Epoch: 62 Loss: 0.719324\n",
      "Train Epoch: 62 Loss: 0.719076\n",
      "Train Epoch: 62 Loss: 0.683358\n",
      "Train Epoch: 62 Loss: 0.719321\n",
      "Train Epoch: 62 Loss: 0.667648\n",
      "Train Epoch: 62 Loss: 0.665627\n",
      "Train Epoch: 62 Loss: 0.667656\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 62 Loss: 0.667592\n",
      "Train Epoch: 62 Loss: 0.667638\n",
      "Train Epoch: 62 Loss: 0.667702\n",
      "\n",
      "Test set: Average loss: 0.7504, Accuracy: 6/11 (55%)\n",
      "\n",
      "------------------\n",
      "Output tensor([0.5130], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([0.])\n",
      "tensor(0.7195, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 63 Loss: 0.719472\n",
      "Output tensor([0.4981], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([0.])\n",
      "tensor(0.6894, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 63 Loss: 0.689421\n",
      "Output tensor([0.5094], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([0.])\n",
      "tensor(0.7121, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 63 Loss: 0.712105\n",
      "Output tensor([0.5042], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([0.])\n",
      "tensor(0.7017, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 63 Loss: 0.701672\n",
      "Output tensor([0.4853], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([0.])\n",
      "tensor(0.6641, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 63 Loss: 0.664082\n",
      "Output tensor([0.4942], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([0.])\n",
      "tensor(0.6817, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 63 Loss: 0.681681\n",
      "Output tensor([0.4878], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([1.])\n",
      "tensor(0.7178, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 63 Loss: 0.717757\n",
      "Output tensor([0.4904], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([1.])\n",
      "tensor(0.7126, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 63 Loss: 0.712594\n",
      "Output tensor([0.4927], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([1.])\n",
      "tensor(0.7079, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 63 Loss: 0.707891\n",
      "Output tensor([0.4968], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([1.])\n",
      "tensor(0.6995, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 63 Loss: 0.699529\n",
      "Output tensor([0.5019], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([1.])\n",
      "tensor(0.6893, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 63 Loss: 0.689308\n",
      "Output tensor([0.5073], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([1.])\n",
      "tensor(0.6786, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 63 Loss: 0.678637\n",
      "\n",
      "\n",
      "Train Epoch: 63 Loss: 0.725646\n",
      "Train Epoch: 63 Loss: 0.689385\n",
      "Train Epoch: 63 Loss: 0.719696\n",
      "Train Epoch: 63 Loss: 0.725148\n",
      "Train Epoch: 63 Loss: 0.687268\n",
      "Train Epoch: 63 Loss: 0.725051\n",
      "Train Epoch: 63 Loss: 0.664782\n",
      "Train Epoch: 63 Loss: 0.661436\n",
      "Train Epoch: 63 Loss: 0.666342\n",
      "Train Epoch: 63 Loss: 0.667145\n",
      "Train Epoch: 63 Loss: 0.667209\n",
      "Train Epoch: 63 Loss: 0.666904\n",
      "\n",
      "Test set: Average loss: 0.7515, Accuracy: 6/11 (55%)\n",
      "\n",
      "------------------\n",
      "Output tensor([0.5160], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([0.])\n",
      "tensor(0.7256, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 64 Loss: 0.725646\n",
      "Output tensor([0.4982], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([0.])\n",
      "tensor(0.6895, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 64 Loss: 0.689520\n",
      "Output tensor([0.5095], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([0.])\n",
      "tensor(0.7122, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 64 Loss: 0.712248\n",
      "Output tensor([0.5039], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([0.])\n",
      "tensor(0.7010, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 64 Loss: 0.700970\n",
      "Output tensor([0.4842], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([0.])\n",
      "tensor(0.6620, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 64 Loss: 0.661968\n",
      "Output tensor([0.4897], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([0.])\n",
      "tensor(0.6727, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 64 Loss: 0.672694\n",
      "Output tensor([0.4791], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([1.])\n",
      "tensor(0.7359, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 64 Loss: 0.735884\n",
      "Output tensor([0.4832], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([1.])\n",
      "tensor(0.7273, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 64 Loss: 0.727264\n",
      "Output tensor([0.4888], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([1.])\n",
      "tensor(0.7158, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 64 Loss: 0.715821\n",
      "Output tensor([0.4949], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([1.])\n",
      "tensor(0.7035, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 64 Loss: 0.703453\n",
      "Output tensor([0.5043], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([1.])\n",
      "tensor(0.6846, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 64 Loss: 0.684555\n",
      "Output tensor([0.5141], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([1.])\n",
      "tensor(0.6653, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 64 Loss: 0.665304\n",
      "\n",
      "\n",
      "Train Epoch: 64 Loss: 0.737984\n",
      "Train Epoch: 64 Loss: 0.695964\n",
      "Train Epoch: 64 Loss: 0.736433\n",
      "Train Epoch: 64 Loss: 0.745088\n",
      "Train Epoch: 64 Loss: 0.706024\n",
      "Train Epoch: 64 Loss: 0.744904\n",
      "Train Epoch: 64 Loss: 0.648889\n",
      "Train Epoch: 64 Loss: 0.650212\n",
      "Train Epoch: 64 Loss: 0.650303\n",
      "Train Epoch: 64 Loss: 0.643925\n",
      "Train Epoch: 64 Loss: 0.650394\n",
      "Train Epoch: 64 Loss: 0.643879\n",
      "\n",
      "Test set: Average loss: 0.7504, Accuracy: 6/11 (55%)\n",
      "\n",
      "------------------\n",
      "Output tensor([0.5219], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([0.])\n",
      "tensor(0.7380, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 65 Loss: 0.737984\n",
      "Output tensor([0.5021], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([0.])\n",
      "tensor(0.6974, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 65 Loss: 0.697362\n",
      "Output tensor([0.5085], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([0.])\n",
      "tensor(0.7103, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 65 Loss: 0.710329\n",
      "Output tensor([0.5145], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([0.])\n",
      "tensor(0.7226, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 65 Loss: 0.722634\n",
      "Output tensor([0.4841], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([0.])\n",
      "tensor(0.6618, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 65 Loss: 0.661775\n",
      "Output tensor([0.4925], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([0.])\n",
      "tensor(0.6783, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 65 Loss: 0.678350\n",
      "Output tensor([0.4872], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([1.])\n",
      "tensor(0.7191, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 65 Loss: 0.719094\n",
      "Output tensor([0.4879], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([1.])\n",
      "tensor(0.7177, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 65 Loss: 0.717651\n",
      "Output tensor([0.4916], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([1.])\n",
      "tensor(0.7101, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 65 Loss: 0.710149\n",
      "Output tensor([0.4937], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([1.])\n",
      "tensor(0.7058, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 65 Loss: 0.705835\n",
      "Output tensor([0.5021], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([1.])\n",
      "tensor(0.6890, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 65 Loss: 0.689022\n",
      "Output tensor([0.5079], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([1.])\n",
      "tensor(0.6775, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 65 Loss: 0.677494\n",
      "\n",
      "\n",
      "Train Epoch: 65 Loss: 0.722178\n",
      "Train Epoch: 65 Loss: 0.691568\n",
      "Train Epoch: 65 Loss: 0.722088\n",
      "Train Epoch: 65 Loss: 0.722081\n",
      "Train Epoch: 65 Loss: 0.685104\n",
      "Train Epoch: 65 Loss: 0.722080\n",
      "Train Epoch: 65 Loss: 0.665030\n",
      "Train Epoch: 65 Loss: 0.664890\n",
      "Train Epoch: 65 Loss: 0.665037\n",
      "Train Epoch: 65 Loss: 0.664947\n",
      "Train Epoch: 65 Loss: 0.665019\n",
      "Train Epoch: 65 Loss: 0.665071\n",
      "\n",
      "Test set: Average loss: 0.7505, Accuracy: 6/11 (55%)\n",
      "\n",
      "------------------\n",
      "Output tensor([0.5143], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([0.])\n",
      "tensor(0.7222, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 66 Loss: 0.722178\n",
      "Output tensor([0.4990], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([0.])\n",
      "tensor(0.6912, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 66 Loss: 0.691211\n",
      "Output tensor([0.5103], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([0.])\n",
      "tensor(0.7139, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 66 Loss: 0.713945\n",
      "Output tensor([0.5052], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([0.])\n",
      "tensor(0.7035, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 66 Loss: 0.703513\n",
      "Output tensor([0.4850], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([0.])\n",
      "tensor(0.6635, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 66 Loss: 0.663525\n",
      "Output tensor([0.4942], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([0.])\n",
      "tensor(0.6816, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 66 Loss: 0.681609\n",
      "Output tensor([0.4884], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([1.])\n",
      "tensor(0.7166, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 66 Loss: 0.716606\n",
      "Output tensor([0.4888], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([1.])\n",
      "tensor(0.7159, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 66 Loss: 0.715861\n",
      "Output tensor([0.4921], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([1.])\n",
      "tensor(0.7091, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 66 Loss: 0.709147\n",
      "Output tensor([0.4968], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([1.])\n",
      "tensor(0.6996, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 66 Loss: 0.699577\n",
      "Output tensor([0.5021], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([1.])\n",
      "tensor(0.6890, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 66 Loss: 0.689012\n",
      "Output tensor([0.5077], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([1.])\n",
      "tensor(0.6778, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 66 Loss: 0.677798\n",
      "\n",
      "\n",
      "Train Epoch: 66 Loss: 0.721451\n",
      "Train Epoch: 66 Loss: 0.690908\n",
      "Train Epoch: 66 Loss: 0.721365\n",
      "Train Epoch: 66 Loss: 0.721358\n",
      "Train Epoch: 66 Loss: 0.684416\n",
      "Train Epoch: 66 Loss: 0.721356\n",
      "Train Epoch: 66 Loss: 0.665715\n",
      "Train Epoch: 66 Loss: 0.665573\n",
      "Train Epoch: 66 Loss: 0.665721\n",
      "Train Epoch: 66 Loss: 0.665620\n",
      "Train Epoch: 66 Loss: 0.665703\n",
      "Train Epoch: 66 Loss: 0.665756\n",
      "\n",
      "Test set: Average loss: 0.7504, Accuracy: 6/11 (55%)\n",
      "\n",
      "------------------\n",
      "Output tensor([0.5140], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([0.])\n",
      "tensor(0.7215, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 67 Loss: 0.721451\n",
      "Output tensor([0.4987], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([0.])\n",
      "tensor(0.6906, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 67 Loss: 0.690632\n",
      "Output tensor([0.5101], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([0.])\n",
      "tensor(0.7137, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 67 Loss: 0.713651\n",
      "Output tensor([0.5052], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([0.])\n",
      "tensor(0.7036, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 67 Loss: 0.703606\n",
      "Output tensor([0.4852], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([0.])\n",
      "tensor(0.6639, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 67 Loss: 0.663883\n",
      "Output tensor([0.4946], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([0.])\n",
      "tensor(0.6825, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 67 Loss: 0.682453\n",
      "Output tensor([0.4890], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([1.])\n",
      "tensor(0.7153, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 67 Loss: 0.715319\n",
      "Output tensor([0.4894], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([1.])\n",
      "tensor(0.7146, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 67 Loss: 0.714630\n",
      "Output tensor([0.4925], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([1.])\n",
      "tensor(0.7082, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 67 Loss: 0.708196\n",
      "Output tensor([0.4971], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([1.])\n",
      "tensor(0.6990, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 67 Loss: 0.698957\n",
      "Output tensor([0.5022], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([1.])\n",
      "tensor(0.6887, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 67 Loss: 0.688713\n",
      "Output tensor([0.5077], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([1.])\n",
      "tensor(0.6778, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 67 Loss: 0.677785\n",
      "\n",
      "\n",
      "Train Epoch: 67 Loss: 0.721196\n",
      "Train Epoch: 67 Loss: 0.690618\n",
      "Train Epoch: 67 Loss: 0.721105\n",
      "Train Epoch: 67 Loss: 0.721098\n",
      "Train Epoch: 67 Loss: 0.684098\n",
      "Train Epoch: 67 Loss: 0.721095\n",
      "Train Epoch: 67 Loss: 0.665962\n",
      "Train Epoch: 67 Loss: 0.665805\n",
      "Train Epoch: 67 Loss: 0.665968\n",
      "Train Epoch: 67 Loss: 0.665857\n",
      "Train Epoch: 67 Loss: 0.665949\n",
      "Train Epoch: 67 Loss: 0.666003\n",
      "\n",
      "Test set: Average loss: 0.7504, Accuracy: 6/11 (55%)\n",
      "\n",
      "------------------\n",
      "Output tensor([0.5138], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([0.])\n",
      "tensor(0.7212, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 68 Loss: 0.721196\n",
      "Output tensor([0.4986], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([0.])\n",
      "tensor(0.6904, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 68 Loss: 0.690356\n",
      "Output tensor([0.5101], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([0.])\n",
      "tensor(0.7136, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 68 Loss: 0.713556\n",
      "Output tensor([0.5052], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([0.])\n",
      "tensor(0.7037, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 68 Loss: 0.703702\n",
      "Output tensor([0.4852], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([0.])\n",
      "tensor(0.6640, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 68 Loss: 0.664043\n",
      "Output tensor([0.4949], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([0.])\n",
      "tensor(0.6829, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 68 Loss: 0.682924\n",
      "Output tensor([0.4894], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([1.])\n",
      "tensor(0.7146, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 68 Loss: 0.714626\n",
      "Output tensor([0.4897], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([1.])\n",
      "tensor(0.7140, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 68 Loss: 0.713964\n",
      "Output tensor([0.4928], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([1.])\n",
      "tensor(0.7077, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 68 Loss: 0.707673\n",
      "Output tensor([0.4973], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([1.])\n",
      "tensor(0.6986, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 68 Loss: 0.698598\n",
      "Output tensor([0.5023], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([1.])\n",
      "tensor(0.6885, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 68 Loss: 0.688517\n",
      "Output tensor([0.5078], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([1.])\n",
      "tensor(0.6777, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 68 Loss: 0.677735\n",
      "\n",
      "\n",
      "Train Epoch: 68 Loss: 0.721114\n",
      "Train Epoch: 68 Loss: 0.690459\n",
      "Train Epoch: 68 Loss: 0.721017\n",
      "Train Epoch: 68 Loss: 0.721009\n",
      "Train Epoch: 68 Loss: 0.683922\n",
      "Train Epoch: 68 Loss: 0.721006\n",
      "Train Epoch: 68 Loss: 0.666045\n",
      "Train Epoch: 68 Loss: 0.665871\n",
      "Train Epoch: 68 Loss: 0.666052\n",
      "Train Epoch: 68 Loss: 0.665931\n",
      "Train Epoch: 68 Loss: 0.666032\n",
      "Train Epoch: 68 Loss: 0.666087\n",
      "\n",
      "Test set: Average loss: 0.7504, Accuracy: 6/11 (55%)\n",
      "\n",
      "------------------\n",
      "Output tensor([0.5138], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([0.])\n",
      "tensor(0.7211, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 69 Loss: 0.721114\n",
      "Output tensor([0.4985], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([0.])\n",
      "tensor(0.6902, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 69 Loss: 0.690205\n",
      "Output tensor([0.5101], grad_fn=<SigmoidBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target tensor([0.])\n",
      "tensor(0.7136, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 69 Loss: 0.713557\n",
      "Output tensor([0.5053], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([0.])\n",
      "tensor(0.7038, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 69 Loss: 0.703801\n",
      "Output tensor([0.4853], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([0.])\n",
      "tensor(0.6641, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 69 Loss: 0.664101\n",
      "Output tensor([0.4950], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([0.])\n",
      "tensor(0.6832, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 69 Loss: 0.683206\n",
      "Output tensor([0.4896], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([1.])\n",
      "tensor(0.7142, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 69 Loss: 0.714238\n",
      "Output tensor([0.4899], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([1.])\n",
      "tensor(0.7136, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 69 Loss: 0.713587\n",
      "Output tensor([0.4929], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([1.])\n",
      "tensor(0.7074, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 69 Loss: 0.707366\n",
      "Output tensor([0.4974], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([1.])\n",
      "tensor(0.6984, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 69 Loss: 0.698367\n",
      "Output tensor([0.5024], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([1.])\n",
      "tensor(0.6884, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 69 Loss: 0.688366\n",
      "Output tensor([0.5078], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([1.])\n",
      "tensor(0.6777, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 69 Loss: 0.677656\n",
      "\n",
      "\n",
      "Train Epoch: 69 Loss: 0.721136\n",
      "Train Epoch: 69 Loss: 0.690372\n",
      "Train Epoch: 69 Loss: 0.721034\n",
      "Train Epoch: 69 Loss: 0.721026\n",
      "Train Epoch: 69 Loss: 0.683824\n",
      "Train Epoch: 69 Loss: 0.721022\n",
      "Train Epoch: 69 Loss: 0.666030\n",
      "Train Epoch: 69 Loss: 0.665837\n",
      "Train Epoch: 69 Loss: 0.666037\n",
      "Train Epoch: 69 Loss: 0.665904\n",
      "Train Epoch: 69 Loss: 0.666015\n",
      "Train Epoch: 69 Loss: 0.666072\n",
      "\n",
      "Test set: Average loss: 0.7504, Accuracy: 6/11 (55%)\n",
      "\n",
      "------------------\n",
      "Output tensor([0.5138], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([0.])\n",
      "tensor(0.7211, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 70 Loss: 0.721136\n",
      "Output tensor([0.4985], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([0.])\n",
      "tensor(0.6901, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 70 Loss: 0.690124\n",
      "Output tensor([0.5101], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([0.])\n",
      "tensor(0.7136, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 70 Loss: 0.713618\n",
      "Output tensor([0.5054], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([0.])\n",
      "tensor(0.7039, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 70 Loss: 0.703908\n",
      "Output tensor([0.4853], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([0.])\n",
      "tensor(0.6641, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 70 Loss: 0.664106\n",
      "Output tensor([0.4951], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([0.])\n",
      "tensor(0.6834, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 70 Loss: 0.683391\n",
      "Output tensor([0.4897], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([1.])\n",
      "tensor(0.7140, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 70 Loss: 0.714007\n",
      "Output tensor([0.4900], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([1.])\n",
      "tensor(0.7134, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 70 Loss: 0.713361\n",
      "Output tensor([0.4930], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([1.])\n",
      "tensor(0.7072, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 70 Loss: 0.707171\n",
      "Output tensor([0.4975], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([1.])\n",
      "tensor(0.6982, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 70 Loss: 0.698202\n",
      "Output tensor([0.5025], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([1.])\n",
      "tensor(0.6882, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 70 Loss: 0.688236\n",
      "Output tensor([0.5079], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([1.])\n",
      "tensor(0.6776, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 70 Loss: 0.677555\n",
      "\n",
      "\n",
      "Train Epoch: 70 Loss: 0.721222\n",
      "Train Epoch: 70 Loss: 0.690326\n",
      "Train Epoch: 70 Loss: 0.721114\n",
      "Train Epoch: 70 Loss: 0.721105\n",
      "Train Epoch: 70 Loss: 0.683767\n",
      "Train Epoch: 70 Loss: 0.721102\n",
      "Train Epoch: 70 Loss: 0.665955\n",
      "Train Epoch: 70 Loss: 0.665742\n",
      "Train Epoch: 70 Loss: 0.665962\n",
      "Train Epoch: 70 Loss: 0.665817\n",
      "Train Epoch: 70 Loss: 0.665939\n",
      "Train Epoch: 70 Loss: 0.665998\n",
      "\n",
      "Test set: Average loss: 0.7504, Accuracy: 6/11 (55%)\n",
      "\n",
      "------------------\n",
      "Output tensor([0.5138], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([0.])\n",
      "tensor(0.7212, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 71 Loss: 0.721222\n",
      "Output tensor([0.4985], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([0.])\n",
      "tensor(0.6901, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 71 Loss: 0.690082\n",
      "Output tensor([0.5102], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([0.])\n",
      "tensor(0.7137, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 71 Loss: 0.713717\n",
      "Output tensor([0.5054], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([0.])\n",
      "tensor(0.7040, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 71 Loss: 0.704023\n",
      "Output tensor([0.4853], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([0.])\n",
      "tensor(0.6641, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 71 Loss: 0.664080\n",
      "Output tensor([0.4952], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([0.])\n",
      "tensor(0.6835, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 71 Loss: 0.683526\n",
      "Output tensor([0.4897], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([1.])\n",
      "tensor(0.7139, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 71 Loss: 0.713861\n",
      "Output tensor([0.4901], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([1.])\n",
      "tensor(0.7132, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 71 Loss: 0.713214\n",
      "Output tensor([0.4931], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([1.])\n",
      "tensor(0.7070, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 71 Loss: 0.707035\n",
      "Output tensor([0.4975], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([1.])\n",
      "tensor(0.6981, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 71 Loss: 0.698069\n",
      "Output tensor([0.5025], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([1.])\n",
      "tensor(0.6881, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 71 Loss: 0.688114\n",
      "Output tensor([0.5079], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([1.])\n",
      "tensor(0.6774, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 71 Loss: 0.677440\n",
      "\n",
      "\n",
      "Train Epoch: 71 Loss: 0.721347\n",
      "Train Epoch: 71 Loss: 0.690301\n",
      "Train Epoch: 71 Loss: 0.721233\n",
      "Train Epoch: 71 Loss: 0.721223\n",
      "Train Epoch: 71 Loss: 0.683725\n",
      "Train Epoch: 71 Loss: 0.721220\n",
      "Train Epoch: 71 Loss: 0.665843\n",
      "Train Epoch: 71 Loss: 0.665609\n",
      "Train Epoch: 71 Loss: 0.665850\n",
      "Train Epoch: 71 Loss: 0.665692\n",
      "Train Epoch: 71 Loss: 0.665825\n",
      "Train Epoch: 71 Loss: 0.665886\n",
      "\n",
      "Test set: Average loss: 0.7503, Accuracy: 6/11 (55%)\n",
      "\n",
      "------------------\n",
      "Output tensor([0.5139], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([0.])\n",
      "tensor(0.7213, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 72 Loss: 0.721347\n",
      "Output tensor([0.4985], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([0.])\n",
      "tensor(0.6901, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 72 Loss: 0.690059\n",
      "Output tensor([0.5102], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([0.])\n",
      "tensor(0.7138, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 72 Loss: 0.713839\n",
      "Output tensor([0.5055], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([0.])\n",
      "tensor(0.7041, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 72 Loss: 0.704144\n",
      "Output tensor([0.4852], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([0.])\n",
      "tensor(0.6640, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 72 Loss: 0.664029\n",
      "Output tensor([0.4952], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([0.])\n",
      "tensor(0.6836, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 72 Loss: 0.683633\n",
      "Output tensor([0.4898], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([1.])\n",
      "tensor(0.7138, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 72 Loss: 0.713761\n",
      "Output tensor([0.4901], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([1.])\n",
      "tensor(0.7131, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 72 Loss: 0.713111\n",
      "Output tensor([0.4932], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([1.])\n",
      "tensor(0.7069, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 72 Loss: 0.706932\n",
      "Output tensor([0.4976], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([1.])\n",
      "tensor(0.6980, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 72 Loss: 0.697955\n",
      "Output tensor([0.5026], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([1.])\n",
      "tensor(0.6880, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 72 Loss: 0.687995\n",
      "Output tensor([0.5080], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([1.])\n",
      "tensor(0.6773, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 72 Loss: 0.677315\n",
      "\n",
      "\n",
      "Train Epoch: 72 Loss: 0.721497\n",
      "Train Epoch: 72 Loss: 0.690288\n",
      "Train Epoch: 72 Loss: 0.721378\n",
      "Train Epoch: 72 Loss: 0.721367\n",
      "Train Epoch: 72 Loss: 0.683671\n",
      "Train Epoch: 72 Loss: 0.721364\n",
      "Train Epoch: 72 Loss: 0.665707\n",
      "Train Epoch: 72 Loss: 0.665449\n",
      "Train Epoch: 72 Loss: 0.665714\n",
      "Train Epoch: 72 Loss: 0.665542\n",
      "Train Epoch: 72 Loss: 0.665688\n",
      "Train Epoch: 72 Loss: 0.665751\n",
      "\n",
      "Test set: Average loss: 0.7503, Accuracy: 6/11 (55%)\n",
      "\n",
      "------------------\n",
      "Output tensor([0.5140], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([0.])\n",
      "tensor(0.7215, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 73 Loss: 0.721497\n",
      "Output tensor([0.4984], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([0.])\n",
      "tensor(0.6900, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 73 Loss: 0.690047\n",
      "Output tensor([0.5103], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([0.])\n",
      "tensor(0.7140, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 73 Loss: 0.713977\n",
      "Output tensor([0.5055], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([0.])\n",
      "tensor(0.7043, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 73 Loss: 0.704269\n",
      "Output tensor([0.4852], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([0.])\n",
      "tensor(0.6639, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 73 Loss: 0.663948\n",
      "Output tensor([0.4953], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([0.])\n",
      "tensor(0.6837, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 73 Loss: 0.683724\n",
      "Output tensor([0.4898], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([1.])\n",
      "tensor(0.7137, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 73 Loss: 0.713688\n",
      "Output tensor([0.4902], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([1.])\n",
      "tensor(0.7130, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 73 Loss: 0.713032\n",
      "Output tensor([0.4932], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([1.])\n",
      "tensor(0.7068, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 73 Loss: 0.706847\n",
      "Output tensor([0.4977], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([1.])\n",
      "tensor(0.6978, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 73 Loss: 0.697848\n",
      "Output tensor([0.5026], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([1.])\n",
      "tensor(0.6879, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 73 Loss: 0.687876\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output tensor([0.5080], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([1.])\n",
      "tensor(0.6772, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 73 Loss: 0.677179\n",
      "\n",
      "\n",
      "Train Epoch: 73 Loss: 0.721670\n",
      "Train Epoch: 73 Loss: 0.690286\n",
      "Train Epoch: 73 Loss: 0.721544\n",
      "Train Epoch: 73 Loss: 0.721531\n",
      "Train Epoch: 73 Loss: 0.683529\n",
      "Train Epoch: 73 Loss: 0.721528\n",
      "Train Epoch: 73 Loss: 0.665551\n",
      "Train Epoch: 73 Loss: 0.665266\n",
      "Train Epoch: 73 Loss: 0.665558\n",
      "Train Epoch: 73 Loss: 0.665369\n",
      "Train Epoch: 73 Loss: 0.665530\n",
      "Train Epoch: 73 Loss: 0.665596\n",
      "\n",
      "Test set: Average loss: 0.7503, Accuracy: 6/11 (55%)\n",
      "\n",
      "------------------\n",
      "Output tensor([0.5141], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([0.])\n",
      "tensor(0.7217, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 74 Loss: 0.721670\n",
      "Output tensor([0.4984], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([0.])\n",
      "tensor(0.6900, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 74 Loss: 0.690045\n",
      "Output tensor([0.5104], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([0.])\n",
      "tensor(0.7141, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 74 Loss: 0.714128\n",
      "Output tensor([0.5056], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([0.])\n",
      "tensor(0.7044, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 74 Loss: 0.704399\n",
      "Output tensor([0.4851], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([0.])\n",
      "tensor(0.6638, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 74 Loss: 0.663785\n",
      "Output tensor([0.4953], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([0.])\n",
      "tensor(0.6838, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 74 Loss: 0.683808\n",
      "Output tensor([0.4899], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([1.])\n",
      "tensor(0.7136, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 74 Loss: 0.713630\n",
      "Output tensor([0.4902], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([1.])\n",
      "tensor(0.7130, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 74 Loss: 0.712967\n",
      "Output tensor([0.4932], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([1.])\n",
      "tensor(0.7068, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 74 Loss: 0.706769\n",
      "Output tensor([0.4977], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([1.])\n",
      "tensor(0.6977, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 74 Loss: 0.697740\n",
      "Output tensor([0.5027], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([1.])\n",
      "tensor(0.6877, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 74 Loss: 0.687744\n",
      "Output tensor([0.5081], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([1.])\n",
      "tensor(0.6770, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 74 Loss: 0.677021\n",
      "\n",
      "\n",
      "Train Epoch: 74 Loss: 0.721881\n",
      "Train Epoch: 74 Loss: 0.690308\n",
      "Train Epoch: 74 Loss: 0.721747\n",
      "Train Epoch: 74 Loss: 0.721732\n",
      "Train Epoch: 74 Loss: 0.682599\n",
      "Train Epoch: 74 Loss: 0.721730\n",
      "Train Epoch: 74 Loss: 0.665360\n",
      "Train Epoch: 74 Loss: 0.665041\n",
      "Train Epoch: 74 Loss: 0.665367\n",
      "Train Epoch: 74 Loss: 0.665160\n",
      "Train Epoch: 74 Loss: 0.665337\n",
      "Train Epoch: 74 Loss: 0.665405\n",
      "\n",
      "Test set: Average loss: 0.7502, Accuracy: 6/11 (55%)\n",
      "\n",
      "------------------\n",
      "Output tensor([0.5142], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([0.])\n",
      "tensor(0.7219, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 75 Loss: 0.721881\n",
      "Output tensor([0.4985], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([0.])\n",
      "tensor(0.6901, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 75 Loss: 0.690066\n",
      "Output tensor([0.5105], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([0.])\n",
      "tensor(0.7143, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 75 Loss: 0.714305\n",
      "Output tensor([0.5057], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([0.])\n",
      "tensor(0.7045, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 75 Loss: 0.704543\n",
      "Output tensor([0.4847], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([0.])\n",
      "tensor(0.6630, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 75 Loss: 0.663007\n",
      "Output tensor([0.4954], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([0.])\n",
      "tensor(0.6839, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 75 Loss: 0.683905\n",
      "Output tensor([0.4899], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([1.])\n",
      "tensor(0.7136, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 75 Loss: 0.713579\n",
      "Output tensor([0.4902], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([1.])\n",
      "tensor(0.7129, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 75 Loss: 0.712891\n",
      "Output tensor([0.4933], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([1.])\n",
      "tensor(0.7067, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 75 Loss: 0.706657\n",
      "Output tensor([0.4978], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([1.])\n",
      "tensor(0.6976, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 75 Loss: 0.697554\n",
      "Output tensor([0.5028], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([1.])\n",
      "tensor(0.6875, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 75 Loss: 0.687484\n",
      "Output tensor([0.5083], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([1.])\n",
      "tensor(0.6767, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 75 Loss: 0.676681\n",
      "\n",
      "\n",
      "Train Epoch: 75 Loss: 0.722342\n",
      "Train Epoch: 75 Loss: 0.690549\n",
      "Train Epoch: 75 Loss: 0.722153\n",
      "Train Epoch: 75 Loss: 0.721983\n",
      "Train Epoch: 75 Loss: 0.677974\n",
      "Train Epoch: 75 Loss: 0.722184\n",
      "Train Epoch: 75 Loss: 0.664916\n",
      "Train Epoch: 75 Loss: 0.664525\n",
      "Train Epoch: 75 Loss: 0.664922\n",
      "Train Epoch: 75 Loss: 0.664693\n",
      "Train Epoch: 75 Loss: 0.664888\n",
      "Train Epoch: 75 Loss: 0.664964\n",
      "\n",
      "Test set: Average loss: 0.7496, Accuracy: 6/11 (55%)\n",
      "\n",
      "------------------\n",
      "Output tensor([0.5144], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([0.])\n",
      "tensor(0.7223, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 76 Loss: 0.722342\n",
      "Output tensor([0.4986], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([0.])\n",
      "tensor(0.6903, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 76 Loss: 0.690295\n",
      "Output tensor([0.5106], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([0.])\n",
      "tensor(0.7146, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 76 Loss: 0.714609\n",
      "Output tensor([0.5056], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([0.])\n",
      "tensor(0.7045, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 76 Loss: 0.704469\n",
      "Output tensor([0.4826], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([0.])\n",
      "tensor(0.6590, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 76 Loss: 0.659021\n",
      "Output tensor([0.4950], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([0.])\n",
      "tensor(0.6832, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 76 Loss: 0.683169\n",
      "Output tensor([0.4898], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([1.])\n",
      "tensor(0.7138, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 76 Loss: 0.713794\n",
      "Output tensor([0.4902], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([1.])\n",
      "tensor(0.7130, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 76 Loss: 0.713021\n",
      "Output tensor([0.4916], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([1.])\n",
      "tensor(0.7100, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 76 Loss: 0.710031\n",
      "Output tensor([0.4982], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([1.])\n",
      "tensor(0.6968, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 76 Loss: 0.696780\n",
      "Output tensor([0.5034], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([1.])\n",
      "tensor(0.6864, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 76 Loss: 0.686372\n",
      "Output tensor([0.5091], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([1.])\n",
      "tensor(0.6752, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 76 Loss: 0.675206\n",
      "\n",
      "\n",
      "Train Epoch: 76 Loss: 0.724471\n",
      "Train Epoch: 76 Loss: 0.692177\n",
      "Train Epoch: 76 Loss: 0.724249\n",
      "Train Epoch: 76 Loss: 0.724227\n",
      "Train Epoch: 76 Loss: 0.685063\n",
      "Train Epoch: 76 Loss: 0.724230\n",
      "Train Epoch: 76 Loss: 0.663004\n",
      "Train Epoch: 76 Loss: 0.662445\n",
      "Train Epoch: 76 Loss: 0.663010\n",
      "Train Epoch: 76 Loss: 0.662753\n",
      "Train Epoch: 76 Loss: 0.662962\n",
      "Train Epoch: 76 Loss: 0.663051\n",
      "\n",
      "Test set: Average loss: 0.7501, Accuracy: 6/11 (55%)\n",
      "\n",
      "------------------\n",
      "Output tensor([0.5154], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([0.])\n",
      "tensor(0.7245, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 77 Loss: 0.724471\n",
      "Output tensor([0.4993], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([0.])\n",
      "tensor(0.6918, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 77 Loss: 0.691833\n",
      "Output tensor([0.5114], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([0.])\n",
      "tensor(0.7163, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 77 Loss: 0.716250\n",
      "Output tensor([0.5064], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([0.])\n",
      "tensor(0.7060, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 77 Loss: 0.705966\n",
      "Output tensor([0.4852], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([0.])\n",
      "tensor(0.6640, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 77 Loss: 0.663973\n",
      "Output tensor([0.4956], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([0.])\n",
      "tensor(0.6845, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 77 Loss: 0.684481\n",
      "Output tensor([0.4900], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([1.])\n",
      "tensor(0.7133, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 77 Loss: 0.713282\n",
      "Output tensor([0.4903], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([1.])\n",
      "tensor(0.7126, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 77 Loss: 0.712643\n",
      "Output tensor([0.4935], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([1.])\n",
      "tensor(0.7063, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 77 Loss: 0.706304\n",
      "Output tensor([0.4981], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([1.])\n",
      "tensor(0.6971, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 77 Loss: 0.697053\n",
      "Output tensor([0.5032], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([1.])\n",
      "tensor(0.6868, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 77 Loss: 0.686833\n",
      "Output tensor([0.5087], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([1.])\n",
      "tensor(0.6758, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 77 Loss: 0.675823\n",
      "\n",
      "\n",
      "Train Epoch: 77 Loss: 0.723482\n",
      "Train Epoch: 77 Loss: 0.691273\n",
      "Train Epoch: 77 Loss: 0.723390\n",
      "Train Epoch: 77 Loss: 0.723364\n",
      "Train Epoch: 77 Loss: 0.684352\n",
      "Train Epoch: 77 Loss: 0.723369\n",
      "Train Epoch: 77 Loss: 0.663813\n",
      "Train Epoch: 77 Loss: 0.663536\n",
      "Train Epoch: 77 Loss: 0.663819\n",
      "Train Epoch: 77 Loss: 0.663540\n",
      "Train Epoch: 77 Loss: 0.663795\n",
      "Train Epoch: 77 Loss: 0.663861\n",
      "\n",
      "Test set: Average loss: 0.7501, Accuracy: 6/11 (55%)\n",
      "\n",
      "------------------\n",
      "Output tensor([0.5149], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([0.])\n",
      "tensor(0.7235, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 78 Loss: 0.723482\n",
      "Output tensor([0.4989], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([0.])\n",
      "tensor(0.6910, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 78 Loss: 0.690962\n",
      "Output tensor([0.5111], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([0.])\n",
      "tensor(0.7156, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 78 Loss: 0.715607\n",
      "Output tensor([0.5061], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([0.])\n",
      "tensor(0.7055, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 78 Loss: 0.705501\n",
      "Output tensor([0.4851], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([0.])\n",
      "tensor(0.6637, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 78 Loss: 0.663698\n",
      "Output tensor([0.4955], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([0.])\n",
      "tensor(0.6843, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 78 Loss: 0.684265\n",
      "Output tensor([0.4900], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([1.])\n",
      "tensor(0.7134, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 78 Loss: 0.713437\n",
      "Output tensor([0.4903], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([1.])\n",
      "tensor(0.7128, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 78 Loss: 0.712750\n",
      "Output tensor([0.4934], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([1.])\n",
      "tensor(0.7065, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 78 Loss: 0.706456\n",
      "Output tensor([0.4980], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([1.])\n",
      "tensor(0.6972, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 78 Loss: 0.697224\n",
      "Output tensor([0.5031], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([1.])\n",
      "tensor(0.6870, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 78 Loss: 0.687043\n",
      "Output tensor([0.5086], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([1.])\n",
      "tensor(0.6761, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 78 Loss: 0.676108\n",
      "\n",
      "\n",
      "Train Epoch: 78 Loss: 0.723169\n",
      "Train Epoch: 78 Loss: 0.690739\n",
      "Train Epoch: 78 Loss: 0.723002\n",
      "Train Epoch: 78 Loss: 0.722970\n",
      "Train Epoch: 78 Loss: 0.684014\n",
      "Train Epoch: 78 Loss: 0.722980\n",
      "Train Epoch: 78 Loss: 0.664180\n",
      "Train Epoch: 78 Loss: 0.663691\n",
      "Train Epoch: 78 Loss: 0.664186\n",
      "Train Epoch: 78 Loss: 0.663880\n",
      "Train Epoch: 78 Loss: 0.664146\n",
      "Train Epoch: 78 Loss: 0.664230\n",
      "\n",
      "Test set: Average loss: 0.7501, Accuracy: 6/11 (55%)\n",
      "\n",
      "------------------\n",
      "Output tensor([0.5148], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([0.])\n",
      "tensor(0.7232, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 79 Loss: 0.723169\n",
      "Output tensor([0.4987], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([0.])\n",
      "tensor(0.6905, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 79 Loss: 0.690464\n",
      "Output tensor([0.5110], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([0.])\n",
      "tensor(0.7153, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 79 Loss: 0.715323\n",
      "Output tensor([0.5060], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([0.])\n",
      "tensor(0.7053, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 79 Loss: 0.705302\n",
      "Output tensor([0.4850], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([0.])\n",
      "tensor(0.6635, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 79 Loss: 0.663539\n",
      "Output tensor([0.4955], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([0.])\n",
      "tensor(0.6842, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 79 Loss: 0.684181\n",
      "Output tensor([0.4899], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([1.])\n",
      "tensor(0.7135, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 79 Loss: 0.713498\n",
      "Output tensor([0.4903], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([1.])\n",
      "tensor(0.7128, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 79 Loss: 0.712801\n",
      "Output tensor([0.4934], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([1.])\n",
      "tensor(0.7065, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 79 Loss: 0.706510\n",
      "Output tensor([0.4979], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([1.])\n",
      "tensor(0.6973, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 79 Loss: 0.697277\n",
      "Output tensor([0.5030], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([1.])\n",
      "tensor(0.6871, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 79 Loss: 0.687120\n",
      "Output tensor([0.5085], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([1.])\n",
      "tensor(0.6762, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 79 Loss: 0.676215\n",
      "\n",
      "\n",
      "Train Epoch: 79 Loss: 0.723018\n",
      "Train Epoch: 79 Loss: 0.690441\n",
      "Train Epoch: 79 Loss: 0.722854\n",
      "Train Epoch: 79 Loss: 0.722816\n",
      "Train Epoch: 79 Loss: 0.683826\n",
      "Train Epoch: 79 Loss: 0.722830\n",
      "Train Epoch: 79 Loss: 0.664322\n",
      "Train Epoch: 79 Loss: 0.663809\n",
      "Train Epoch: 79 Loss: 0.664328\n",
      "Train Epoch: 79 Loss: 0.663986\n",
      "Train Epoch: 79 Loss: 0.664286\n",
      "Train Epoch: 79 Loss: 0.664373\n",
      "\n",
      "Test set: Average loss: 0.7501, Accuracy: 6/11 (55%)\n",
      "\n",
      "------------------\n",
      "Output tensor([0.5147], grad_fn=<SigmoidBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target tensor([0.])\n",
      "tensor(0.7230, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 80 Loss: 0.723018\n",
      "Output tensor([0.4985], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([0.])\n",
      "tensor(0.6902, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 80 Loss: 0.690181\n",
      "Output tensor([0.5109], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([0.])\n",
      "tensor(0.7152, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 80 Loss: 0.715226\n",
      "Output tensor([0.5060], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([0.])\n",
      "tensor(0.7052, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 80 Loss: 0.705240\n",
      "Output tensor([0.4849], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([0.])\n",
      "tensor(0.6634, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 80 Loss: 0.663433\n",
      "Output tensor([0.4955], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([0.])\n",
      "tensor(0.6842, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 80 Loss: 0.684161\n",
      "Output tensor([0.4899], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([1.])\n",
      "tensor(0.7135, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 80 Loss: 0.713516\n",
      "Output tensor([0.4903], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([1.])\n",
      "tensor(0.7128, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 80 Loss: 0.712802\n",
      "Output tensor([0.4934], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([1.])\n",
      "tensor(0.7065, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 80 Loss: 0.706517\n",
      "Output tensor([0.4979], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([1.])\n",
      "tensor(0.6973, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 80 Loss: 0.697271\n",
      "Output tensor([0.5030], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([1.])\n",
      "tensor(0.6871, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 80 Loss: 0.687116\n",
      "Output tensor([0.5085], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([1.])\n",
      "tensor(0.6762, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 80 Loss: 0.676225\n",
      "\n",
      "\n",
      "Train Epoch: 80 Loss: 0.723031\n",
      "Train Epoch: 80 Loss: 0.690255\n",
      "Train Epoch: 80 Loss: 0.722839\n",
      "Train Epoch: 80 Loss: 0.722793\n",
      "Train Epoch: 80 Loss: 0.683731\n",
      "Train Epoch: 80 Loss: 0.722814\n",
      "Train Epoch: 80 Loss: 0.664337\n",
      "Train Epoch: 80 Loss: 0.663722\n",
      "Train Epoch: 80 Loss: 0.664343\n",
      "Train Epoch: 80 Loss: 0.663964\n",
      "Train Epoch: 80 Loss: 0.664292\n",
      "Train Epoch: 80 Loss: 0.664390\n",
      "\n",
      "Test set: Average loss: 0.7500, Accuracy: 6/11 (55%)\n",
      "\n",
      "------------------\n",
      "Output tensor([0.5147], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([0.])\n",
      "tensor(0.7230, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 81 Loss: 0.723031\n",
      "Output tensor([0.4984], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([0.])\n",
      "tensor(0.6900, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 81 Loss: 0.690006\n",
      "Output tensor([0.5109], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([0.])\n",
      "tensor(0.7152, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 81 Loss: 0.715232\n",
      "Output tensor([0.5060], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([0.])\n",
      "tensor(0.7053, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 81 Loss: 0.705253\n",
      "Output tensor([0.4849], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([0.])\n",
      "tensor(0.6633, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 81 Loss: 0.663347\n",
      "Output tensor([0.4955], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([0.])\n",
      "tensor(0.6842, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 81 Loss: 0.684175\n",
      "Output tensor([0.4899], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([1.])\n",
      "tensor(0.7135, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 81 Loss: 0.713514\n",
      "Output tensor([0.4903], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([1.])\n",
      "tensor(0.7128, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 81 Loss: 0.712794\n",
      "Output tensor([0.4934], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([1.])\n",
      "tensor(0.7065, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 81 Loss: 0.706501\n",
      "Output tensor([0.4980], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([1.])\n",
      "tensor(0.6972, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 81 Loss: 0.697233\n",
      "Output tensor([0.5030], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([1.])\n",
      "tensor(0.6871, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 81 Loss: 0.687076\n",
      "Output tensor([0.5086], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([1.])\n",
      "tensor(0.6762, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 81 Loss: 0.676183\n",
      "\n",
      "\n",
      "Train Epoch: 81 Loss: 0.723083\n",
      "Train Epoch: 81 Loss: 0.690137\n",
      "Train Epoch: 81 Loss: 0.722898\n",
      "Train Epoch: 81 Loss: 0.722840\n",
      "Train Epoch: 81 Loss: 0.683670\n",
      "Train Epoch: 81 Loss: 0.722870\n",
      "Train Epoch: 81 Loss: 0.664284\n",
      "Train Epoch: 81 Loss: 0.663644\n",
      "Train Epoch: 81 Loss: 0.664290\n",
      "Train Epoch: 81 Loss: 0.663866\n",
      "Train Epoch: 81 Loss: 0.664238\n",
      "Train Epoch: 81 Loss: 0.664339\n",
      "\n",
      "Test set: Average loss: 0.7500, Accuracy: 6/11 (55%)\n",
      "\n",
      "------------------\n",
      "Output tensor([0.5147], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([0.])\n",
      "tensor(0.7231, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 82 Loss: 0.723083\n",
      "Output tensor([0.4984], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([0.])\n",
      "tensor(0.6899, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 82 Loss: 0.689892\n",
      "Output tensor([0.5110], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([0.])\n",
      "tensor(0.7153, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 82 Loss: 0.715294\n",
      "Output tensor([0.5060], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([0.])\n",
      "tensor(0.7053, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 82 Loss: 0.705304\n",
      "Output tensor([0.4848], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([0.])\n",
      "tensor(0.6633, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 82 Loss: 0.663268\n",
      "Output tensor([0.4955], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([0.])\n",
      "tensor(0.6842, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 82 Loss: 0.684203\n",
      "Output tensor([0.4899], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([1.])\n",
      "tensor(0.7135, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 82 Loss: 0.713505\n",
      "Output tensor([0.4903], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([1.])\n",
      "tensor(0.7128, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 82 Loss: 0.712767\n",
      "Output tensor([0.4934], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([1.])\n",
      "tensor(0.7065, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 82 Loss: 0.706476\n",
      "Output tensor([0.4980], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([1.])\n",
      "tensor(0.6972, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 82 Loss: 0.697181\n",
      "Output tensor([0.5031], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([1.])\n",
      "tensor(0.6870, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 82 Loss: 0.687008\n",
      "Output tensor([0.5086], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([1.])\n",
      "tensor(0.6761, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 82 Loss: 0.676111\n",
      "\n",
      "\n",
      "Train Epoch: 82 Loss: 0.723226\n",
      "Train Epoch: 82 Loss: 0.690047\n",
      "Train Epoch: 82 Loss: 0.723000\n",
      "Train Epoch: 82 Loss: 0.722926\n",
      "Train Epoch: 82 Loss: 0.683639\n",
      "Train Epoch: 82 Loss: 0.722969\n",
      "Train Epoch: 82 Loss: 0.664190\n",
      "Train Epoch: 82 Loss: 0.663399\n",
      "Train Epoch: 82 Loss: 0.664196\n",
      "Train Epoch: 82 Loss: 0.663724\n",
      "Train Epoch: 82 Loss: 0.664128\n",
      "Train Epoch: 82 Loss: 0.664248\n",
      "\n",
      "Test set: Average loss: 0.7500, Accuracy: 6/11 (55%)\n",
      "\n",
      "------------------\n",
      "Output tensor([0.5148], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([0.])\n",
      "tensor(0.7232, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 83 Loss: 0.723226\n",
      "Output tensor([0.4983], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([0.])\n",
      "tensor(0.6898, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 83 Loss: 0.689809\n",
      "Output tensor([0.5110], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([0.])\n",
      "tensor(0.7154, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 83 Loss: 0.715387\n",
      "Output tensor([0.5061], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([0.])\n",
      "tensor(0.7054, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 83 Loss: 0.705376\n",
      "Output tensor([0.4848], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([0.])\n",
      "tensor(0.6632, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 83 Loss: 0.663187\n",
      "Output tensor([0.4955], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([0.])\n",
      "tensor(0.6842, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 83 Loss: 0.684239\n",
      "Output tensor([0.4899], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([1.])\n",
      "tensor(0.7135, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 83 Loss: 0.713494\n",
      "Output tensor([0.4903], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([1.])\n",
      "tensor(0.7128, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 83 Loss: 0.712757\n",
      "Output tensor([0.4934], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([1.])\n",
      "tensor(0.7064, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 83 Loss: 0.706445\n",
      "Output tensor([0.4980], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([1.])\n",
      "tensor(0.6971, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 83 Loss: 0.697120\n",
      "Output tensor([0.5031], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([1.])\n",
      "tensor(0.6869, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 83 Loss: 0.686940\n",
      "Output tensor([0.5086], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([1.])\n",
      "tensor(0.6760, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 83 Loss: 0.676027\n",
      "\n",
      "\n",
      "Train Epoch: 83 Loss: 0.723312\n",
      "Train Epoch: 83 Loss: 0.689986\n",
      "Train Epoch: 83 Loss: 0.723122\n",
      "Train Epoch: 83 Loss: 0.723031\n",
      "Train Epoch: 83 Loss: 0.683601\n",
      "Train Epoch: 83 Loss: 0.723088\n",
      "Train Epoch: 83 Loss: 0.664078\n",
      "Train Epoch: 83 Loss: 0.663330\n",
      "Train Epoch: 83 Loss: 0.664083\n",
      "Train Epoch: 83 Loss: 0.663552\n",
      "Train Epoch: 83 Loss: 0.664023\n",
      "Train Epoch: 83 Loss: 0.664138\n",
      "\n",
      "Test set: Average loss: 0.7499, Accuracy: 6/11 (55%)\n",
      "\n",
      "------------------\n",
      "Output tensor([0.5149], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([0.])\n",
      "tensor(0.7233, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 84 Loss: 0.723312\n",
      "Output tensor([0.4983], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([0.])\n",
      "tensor(0.6897, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 84 Loss: 0.689745\n",
      "Output tensor([0.5111], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([0.])\n",
      "tensor(0.7155, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 84 Loss: 0.715497\n",
      "Output tensor([0.5061], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([0.])\n",
      "tensor(0.7055, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 84 Loss: 0.705458\n",
      "Output tensor([0.4848], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([0.])\n",
      "tensor(0.6631, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 84 Loss: 0.663104\n",
      "Output tensor([0.4955], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([0.])\n",
      "tensor(0.6843, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 84 Loss: 0.684275\n",
      "Output tensor([0.4899], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([1.])\n",
      "tensor(0.7135, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 84 Loss: 0.713486\n",
      "Output tensor([0.4903], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([1.])\n",
      "tensor(0.7127, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 84 Loss: 0.712711\n",
      "Output tensor([0.4934], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([1.])\n",
      "tensor(0.7064, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 84 Loss: 0.706419\n",
      "Output tensor([0.4980], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([1.])\n",
      "tensor(0.6971, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 84 Loss: 0.697059\n",
      "Output tensor([0.5032], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([1.])\n",
      "tensor(0.6868, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 84 Loss: 0.686843\n",
      "Output tensor([0.5087], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([1.])\n",
      "tensor(0.6759, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 84 Loss: 0.675933\n",
      "\n",
      "\n",
      "Train Epoch: 84 Loss: 0.723586\n",
      "Train Epoch: 84 Loss: 0.689915\n",
      "Train Epoch: 84 Loss: 0.723265\n",
      "Train Epoch: 84 Loss: 0.723147\n",
      "Train Epoch: 84 Loss: 0.683597\n",
      "Train Epoch: 84 Loss: 0.723228\n",
      "Train Epoch: 84 Loss: 0.663945\n",
      "Train Epoch: 84 Loss: 0.662819\n",
      "Train Epoch: 84 Loss: 0.663952\n",
      "Train Epoch: 84 Loss: 0.663360\n",
      "Train Epoch: 84 Loss: 0.663840\n",
      "Train Epoch: 84 Loss: 0.664010\n",
      "\n",
      "Test set: Average loss: 0.7499, Accuracy: 6/11 (55%)\n",
      "\n",
      "------------------\n",
      "Output tensor([0.5150], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([0.])\n",
      "tensor(0.7236, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 85 Loss: 0.723586\n",
      "Output tensor([0.4983], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([0.])\n",
      "tensor(0.6897, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 85 Loss: 0.689687\n",
      "Output tensor([0.5111], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([0.])\n",
      "tensor(0.7156, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 85 Loss: 0.715618\n",
      "Output tensor([0.5062], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([0.])\n",
      "tensor(0.7055, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 85 Loss: 0.705545\n",
      "Output tensor([0.4847], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([0.])\n",
      "tensor(0.6630, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 85 Loss: 0.663008\n",
      "Output tensor([0.4956], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([0.])\n",
      "tensor(0.6843, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 85 Loss: 0.684315\n",
      "Output tensor([0.4899], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([1.])\n",
      "tensor(0.7135, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 85 Loss: 0.713475\n",
      "Output tensor([0.4903], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([1.])\n",
      "tensor(0.7128, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 85 Loss: 0.712752\n",
      "Output tensor([0.4934], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([1.])\n",
      "tensor(0.7064, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 85 Loss: 0.706384\n",
      "Output tensor([0.4981], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([1.])\n",
      "tensor(0.6970, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 85 Loss: 0.696990\n",
      "Output tensor([0.5032], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([1.])\n",
      "tensor(0.6868, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 85 Loss: 0.686794\n",
      "Output tensor([0.5087], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([1.])\n",
      "tensor(0.6758, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 85 Loss: 0.675841\n",
      "\n",
      "\n",
      "Train Epoch: 85 Loss: 0.723502\n",
      "Train Epoch: 85 Loss: 0.689894\n",
      "Train Epoch: 85 Loss: 0.723400\n",
      "Train Epoch: 85 Loss: 0.723257\n",
      "Train Epoch: 85 Loss: 0.683520\n",
      "Train Epoch: 85 Loss: 0.723359\n",
      "Train Epoch: 85 Loss: 0.663822\n",
      "Train Epoch: 85 Loss: 0.663251\n",
      "Train Epoch: 85 Loss: 0.663827\n",
      "Train Epoch: 85 Loss: 0.663148\n",
      "Train Epoch: 85 Loss: 0.663787\n",
      "Train Epoch: 85 Loss: 0.663887\n",
      "\n",
      "Test set: Average loss: 0.7499, Accuracy: 6/11 (55%)\n",
      "\n",
      "------------------\n",
      "Output tensor([0.5149], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([0.])\n",
      "tensor(0.7235, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 86 Loss: 0.723502\n",
      "Output tensor([0.4982], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([0.])\n",
      "tensor(0.6896, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 86 Loss: 0.689641\n",
      "Output tensor([0.5112], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([0.])\n",
      "tensor(0.7157, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 86 Loss: 0.715743\n",
      "Output tensor([0.5062], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([0.])\n",
      "tensor(0.7056, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 86 Loss: 0.705631\n",
      "Output tensor([0.4847], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([0.])\n",
      "tensor(0.6629, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 86 Loss: 0.662921\n",
      "Output tensor([0.4956], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([0.])\n",
      "tensor(0.6843, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 86 Loss: 0.684346\n",
      "Output tensor([0.4899], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([1.])\n",
      "tensor(0.7135, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 86 Loss: 0.713477\n",
      "Output tensor([0.4904], grad_fn=<SigmoidBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target tensor([1.])\n",
      "tensor(0.7126, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 86 Loss: 0.712622\n",
      "Output tensor([0.4934], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([1.])\n",
      "tensor(0.7064, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 86 Loss: 0.706368\n",
      "Output tensor([0.4981], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([1.])\n",
      "tensor(0.6969, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 86 Loss: 0.696934\n",
      "Output tensor([0.5033], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([1.])\n",
      "tensor(0.6866, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 86 Loss: 0.686611\n",
      "Output tensor([0.5088], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([1.])\n",
      "tensor(0.6757, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 86 Loss: 0.675737\n",
      "\n",
      "\n",
      "Train Epoch: 86 Loss: 0.724470\n",
      "Train Epoch: 86 Loss: 0.689782\n",
      "Train Epoch: 86 Loss: 0.723584\n",
      "Train Epoch: 86 Loss: 0.723379\n",
      "Train Epoch: 86 Loss: 0.683641\n",
      "Train Epoch: 86 Loss: 0.723541\n",
      "Train Epoch: 86 Loss: 0.663654\n",
      "Train Epoch: 86 Loss: 0.661584\n",
      "Train Epoch: 86 Loss: 0.663670\n",
      "Train Epoch: 86 Loss: 0.662923\n",
      "Train Epoch: 86 Loss: 0.663218\n",
      "Train Epoch: 86 Loss: 0.663734\n",
      "\n",
      "Test set: Average loss: 0.7497, Accuracy: 6/11 (55%)\n",
      "\n",
      "------------------\n",
      "Output tensor([0.5154], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([0.])\n",
      "tensor(0.7245, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 87 Loss: 0.724470\n",
      "Output tensor([0.4982], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([0.])\n",
      "tensor(0.6896, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 87 Loss: 0.689606\n",
      "Output tensor([0.5112], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([0.])\n",
      "tensor(0.7159, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 87 Loss: 0.715869\n",
      "Output tensor([0.5062], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([0.])\n",
      "tensor(0.7057, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 87 Loss: 0.705720\n",
      "Output tensor([0.4846], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([0.])\n",
      "tensor(0.6628, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 87 Loss: 0.662773\n",
      "Output tensor([0.4956], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([0.])\n",
      "tensor(0.6844, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 87 Loss: 0.684394\n",
      "Output tensor([0.4899], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([1.])\n",
      "tensor(0.7135, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 87 Loss: 0.713453\n",
      "Output tensor([0.4902], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([1.])\n",
      "tensor(0.7129, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 87 Loss: 0.712862\n",
      "Output tensor([0.4935], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([1.])\n",
      "tensor(0.7063, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 87 Loss: 0.706316\n",
      "Output tensor([0.4982], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([1.])\n",
      "tensor(0.6968, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 87 Loss: 0.696845\n",
      "Output tensor([0.5033], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([1.])\n",
      "tensor(0.6867, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 87 Loss: 0.686654\n",
      "Output tensor([0.5088], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([1.])\n",
      "tensor(0.6757, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 87 Loss: 0.675656\n",
      "\n",
      "\n",
      "Train Epoch: 87 Loss: 0.723665\n",
      "Train Epoch: 87 Loss: 0.689911\n",
      "Train Epoch: 87 Loss: 0.723688\n",
      "Train Epoch: 87 Loss: 0.723467\n",
      "Train Epoch: 87 Loss: 0.683356\n",
      "Train Epoch: 87 Loss: 0.723635\n",
      "Train Epoch: 87 Loss: 0.663561\n",
      "Train Epoch: 87 Loss: 0.663541\n",
      "Train Epoch: 87 Loss: 0.663564\n",
      "Train Epoch: 87 Loss: 0.662646\n",
      "Train Epoch: 87 Loss: 0.663556\n",
      "Train Epoch: 87 Loss: 0.663631\n",
      "\n",
      "Test set: Average loss: 0.7498, Accuracy: 6/11 (55%)\n",
      "\n",
      "------------------\n",
      "Output tensor([0.5150], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([0.])\n",
      "tensor(0.7237, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 88 Loss: 0.723665\n",
      "Output tensor([0.4982], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([0.])\n",
      "tensor(0.6896, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 88 Loss: 0.689638\n",
      "Output tensor([0.5113], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([0.])\n",
      "tensor(0.7160, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 88 Loss: 0.715994\n",
      "Output tensor([0.5063], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([0.])\n",
      "tensor(0.7058, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 88 Loss: 0.705805\n",
      "Output tensor([0.4845], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([0.])\n",
      "tensor(0.6627, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 88 Loss: 0.662675\n",
      "Output tensor([0.4956], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([0.])\n",
      "tensor(0.6844, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 88 Loss: 0.684426\n",
      "Output tensor([0.4899], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([1.])\n",
      "tensor(0.7135, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 88 Loss: 0.713453\n",
      "Output tensor([0.4903], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([1.])\n",
      "tensor(0.7128, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 88 Loss: 0.712840\n",
      "Output tensor([0.4935], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([1.])\n",
      "tensor(0.7063, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 88 Loss: 0.706293\n",
      "Output tensor([0.4982], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([1.])\n",
      "tensor(0.6968, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 88 Loss: 0.696790\n",
      "Output tensor([0.5033], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([1.])\n",
      "tensor(0.6866, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 88 Loss: 0.686574\n",
      "Output tensor([0.5089], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([1.])\n",
      "tensor(0.6756, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 88 Loss: 0.675559\n",
      "\n",
      "\n",
      "Train Epoch: 88 Loss: 0.723821\n",
      "Train Epoch: 88 Loss: 0.689833\n",
      "Train Epoch: 88 Loss: 0.723841\n",
      "Train Epoch: 88 Loss: 0.723548\n",
      "Train Epoch: 88 Loss: 0.683326\n",
      "Train Epoch: 88 Loss: 0.723786\n",
      "Train Epoch: 88 Loss: 0.663417\n",
      "Train Epoch: 88 Loss: 0.663373\n",
      "Train Epoch: 88 Loss: 0.663421\n",
      "Train Epoch: 88 Loss: 0.662456\n",
      "Train Epoch: 88 Loss: 0.663413\n",
      "Train Epoch: 88 Loss: 0.663489\n",
      "\n",
      "Test set: Average loss: 0.7498, Accuracy: 6/11 (55%)\n",
      "\n",
      "------------------\n",
      "Output tensor([0.5151], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([0.])\n",
      "tensor(0.7238, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 89 Loss: 0.723821\n",
      "Output tensor([0.4982], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([0.])\n",
      "tensor(0.6896, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 89 Loss: 0.689561\n",
      "Output tensor([0.5114], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([0.])\n",
      "tensor(0.7161, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 89 Loss: 0.716123\n",
      "Output tensor([0.5063], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([0.])\n",
      "tensor(0.7059, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 89 Loss: 0.705880\n",
      "Output tensor([0.4845], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([0.])\n",
      "tensor(0.6626, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 89 Loss: 0.662571\n",
      "Output tensor([0.4956], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([0.])\n",
      "tensor(0.6845, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 89 Loss: 0.684455\n",
      "Output tensor([0.4899], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([1.])\n",
      "tensor(0.7135, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 89 Loss: 0.713461\n",
      "Output tensor([0.4903], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([1.])\n",
      "tensor(0.7128, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 89 Loss: 0.712815\n",
      "Output tensor([0.4935], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([1.])\n",
      "tensor(0.7063, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 89 Loss: 0.706275\n",
      "Output tensor([0.4982], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([1.])\n",
      "tensor(0.6967, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 89 Loss: 0.696719\n",
      "Output tensor([0.5033], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([1.])\n",
      "tensor(0.6865, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 89 Loss: 0.686492\n",
      "Output tensor([0.5089], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([1.])\n",
      "tensor(0.6755, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 89 Loss: 0.675464\n",
      "\n",
      "\n",
      "Train Epoch: 89 Loss: 0.723992\n",
      "Train Epoch: 89 Loss: 0.689760\n",
      "Train Epoch: 89 Loss: 0.724013\n",
      "Train Epoch: 89 Loss: 0.723657\n",
      "Train Epoch: 89 Loss: 0.683277\n",
      "Train Epoch: 89 Loss: 0.723945\n",
      "Train Epoch: 89 Loss: 0.663267\n",
      "Train Epoch: 89 Loss: 0.663167\n",
      "Train Epoch: 89 Loss: 0.663271\n",
      "Train Epoch: 89 Loss: 0.662103\n",
      "Train Epoch: 89 Loss: 0.663261\n",
      "Train Epoch: 89 Loss: 0.663341\n",
      "\n",
      "Test set: Average loss: 0.7497, Accuracy: 6/11 (55%)\n",
      "\n",
      "------------------\n",
      "Output tensor([0.5152], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([0.])\n",
      "tensor(0.7240, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 90 Loss: 0.723992\n",
      "Output tensor([0.4982], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([0.])\n",
      "tensor(0.6895, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 90 Loss: 0.689489\n",
      "Output tensor([0.5114], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([0.])\n",
      "tensor(0.7163, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 90 Loss: 0.716268\n",
      "Output tensor([0.5064], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([0.])\n",
      "tensor(0.7060, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 90 Loss: 0.705975\n",
      "Output tensor([0.4844], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([0.])\n",
      "tensor(0.6624, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 90 Loss: 0.662442\n",
      "Output tensor([0.4956], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([0.])\n",
      "tensor(0.6845, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 90 Loss: 0.684480\n",
      "Output tensor([0.4899], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([1.])\n",
      "tensor(0.7135, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 90 Loss: 0.713473\n",
      "Output tensor([0.4903], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([1.])\n",
      "tensor(0.7128, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 90 Loss: 0.712761\n",
      "Output tensor([0.4935], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([1.])\n",
      "tensor(0.7063, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 90 Loss: 0.706262\n",
      "Output tensor([0.4982], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([1.])\n",
      "tensor(0.6967, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 90 Loss: 0.696677\n",
      "Output tensor([0.5034], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([1.])\n",
      "tensor(0.6864, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 90 Loss: 0.686403\n",
      "Output tensor([0.5090], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([1.])\n",
      "tensor(0.6754, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 90 Loss: 0.675363\n",
      "\n",
      "\n",
      "Train Epoch: 90 Loss: 0.724195\n",
      "Train Epoch: 90 Loss: 0.689673\n",
      "Train Epoch: 90 Loss: 0.724175\n",
      "Train Epoch: 90 Loss: 0.723693\n",
      "Train Epoch: 90 Loss: 0.683153\n",
      "Train Epoch: 90 Loss: 0.724111\n",
      "Train Epoch: 90 Loss: 0.663110\n",
      "Train Epoch: 90 Loss: 0.662766\n",
      "Train Epoch: 90 Loss: 0.663113\n",
      "Train Epoch: 90 Loss: 0.661956\n",
      "Train Epoch: 90 Loss: 0.663092\n",
      "Train Epoch: 90 Loss: 0.663185\n",
      "\n",
      "Test set: Average loss: 0.7497, Accuracy: 6/11 (55%)\n",
      "\n",
      "------------------\n",
      "Output tensor([0.5153], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([0.])\n",
      "tensor(0.7242, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 91 Loss: 0.724195\n",
      "Output tensor([0.4981], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([0.])\n",
      "tensor(0.6894, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 91 Loss: 0.689407\n",
      "Output tensor([0.5115], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([0.])\n",
      "tensor(0.7164, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 91 Loss: 0.716402\n",
      "Output tensor([0.5064], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([0.])\n",
      "tensor(0.7060, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 91 Loss: 0.706040\n",
      "Output tensor([0.4843], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([0.])\n",
      "tensor(0.6622, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 91 Loss: 0.662238\n",
      "Output tensor([0.4957], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([0.])\n",
      "tensor(0.6845, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 91 Loss: 0.684502\n",
      "Output tensor([0.4899], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([1.])\n",
      "tensor(0.7135, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 91 Loss: 0.713491\n",
      "Output tensor([0.4904], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([1.])\n",
      "tensor(0.7126, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 91 Loss: 0.712596\n",
      "Output tensor([0.4935], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([1.])\n",
      "tensor(0.7063, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 91 Loss: 0.706254\n",
      "Output tensor([0.4983], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([1.])\n",
      "tensor(0.6966, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 91 Loss: 0.696591\n",
      "Output tensor([0.5035], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([1.])\n",
      "tensor(0.6862, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 91 Loss: 0.686181\n",
      "Output tensor([0.5090], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([1.])\n",
      "tensor(0.6753, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 91 Loss: 0.675259\n",
      "\n",
      "\n",
      "Train Epoch: 91 Loss: 0.725289\n",
      "Train Epoch: 91 Loss: 0.689554\n",
      "Train Epoch: 91 Loss: 0.724436\n",
      "Train Epoch: 91 Loss: 0.723837\n",
      "Train Epoch: 91 Loss: 0.682558\n",
      "Train Epoch: 91 Loss: 0.724346\n",
      "Train Epoch: 91 Loss: 0.662893\n",
      "Train Epoch: 91 Loss: 0.660674\n",
      "Train Epoch: 91 Loss: 0.662910\n",
      "Train Epoch: 91 Loss: 0.661434\n",
      "Train Epoch: 91 Loss: 0.662374\n",
      "Train Epoch: 91 Loss: 0.662995\n",
      "\n",
      "Test set: Average loss: 0.7494, Accuracy: 6/11 (55%)\n",
      "\n",
      "------------------\n",
      "Output tensor([0.5158], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([0.])\n",
      "tensor(0.7253, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 92 Loss: 0.725289\n",
      "Output tensor([0.4981], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([0.])\n",
      "tensor(0.6894, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 92 Loss: 0.689372\n",
      "Output tensor([0.5116], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([0.])\n",
      "tensor(0.7166, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 92 Loss: 0.716586\n",
      "Output tensor([0.5065], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([0.])\n",
      "tensor(0.7062, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 92 Loss: 0.706168\n",
      "Output tensor([0.4839], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([0.])\n",
      "tensor(0.6615, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 92 Loss: 0.661496\n",
      "Output tensor([0.4957], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([0.])\n",
      "tensor(0.6846, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 92 Loss: 0.684562\n",
      "Output tensor([0.4899], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([1.])\n",
      "tensor(0.7135, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 92 Loss: 0.713472\n",
      "Output tensor([0.4902], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([1.])\n",
      "tensor(0.7128, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 92 Loss: 0.712841\n",
      "Output tensor([0.4935], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([1.])\n",
      "tensor(0.7062, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 92 Loss: 0.706171\n",
      "Output tensor([0.4983], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([1.])\n",
      "tensor(0.6965, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 92 Loss: 0.696479\n",
      "Output tensor([0.5035], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([1.])\n",
      "tensor(0.6861, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 92 Loss: 0.686120\n",
      "Output tensor([0.5081], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([1.])\n",
      "tensor(0.6772, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 92 Loss: 0.677160\n",
      "\n",
      "\n",
      "Train Epoch: 92 Loss: 0.724621\n",
      "Train Epoch: 92 Loss: 0.689762\n",
      "Train Epoch: 92 Loss: 0.724646\n",
      "Train Epoch: 92 Loss: 0.723837\n",
      "Train Epoch: 92 Loss: 0.682606\n",
      "Train Epoch: 92 Loss: 0.724576\n",
      "Train Epoch: 92 Loss: 0.662670\n",
      "Train Epoch: 92 Loss: 0.662629\n",
      "Train Epoch: 92 Loss: 0.662673\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 92 Loss: 0.661313\n",
      "Train Epoch: 92 Loss: 0.662665\n",
      "Train Epoch: 92 Loss: 0.662748\n",
      "\n",
      "Test set: Average loss: 0.7495, Accuracy: 6/11 (55%)\n",
      "\n",
      "------------------\n",
      "Output tensor([0.5155], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([0.])\n",
      "tensor(0.7246, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 93 Loss: 0.724621\n",
      "Output tensor([0.4982], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([0.])\n",
      "tensor(0.6895, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 93 Loss: 0.689486\n",
      "Output tensor([0.5117], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([0.])\n",
      "tensor(0.7167, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 93 Loss: 0.716737\n",
      "Output tensor([0.5065], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([0.])\n",
      "tensor(0.7062, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 93 Loss: 0.706220\n",
      "Output tensor([0.4843], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([0.])\n",
      "tensor(0.6622, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 93 Loss: 0.662244\n",
      "Output tensor([0.4957], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([0.])\n",
      "tensor(0.6846, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 93 Loss: 0.684602\n",
      "Output tensor([0.4900], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([1.])\n",
      "tensor(0.7134, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 93 Loss: 0.713433\n",
      "Output tensor([0.4903], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([1.])\n",
      "tensor(0.7128, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 93 Loss: 0.712788\n",
      "Output tensor([0.4935], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([1.])\n",
      "tensor(0.7062, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 93 Loss: 0.706154\n",
      "Output tensor([0.4984], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([1.])\n",
      "tensor(0.6964, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 93 Loss: 0.696398\n",
      "Output tensor([0.5035], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([1.])\n",
      "tensor(0.6862, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 93 Loss: 0.686155\n",
      "Output tensor([0.5091], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([1.])\n",
      "tensor(0.6751, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 93 Loss: 0.675106\n",
      "\n",
      "\n",
      "Train Epoch: 93 Loss: 0.724668\n",
      "Train Epoch: 93 Loss: 0.689659\n",
      "Train Epoch: 93 Loss: 0.724758\n",
      "Train Epoch: 93 Loss: 0.723949\n",
      "Train Epoch: 93 Loss: 0.683281\n",
      "Train Epoch: 93 Loss: 0.724581\n",
      "Train Epoch: 93 Loss: 0.662668\n",
      "Train Epoch: 93 Loss: 0.662576\n",
      "Train Epoch: 93 Loss: 0.662671\n",
      "Train Epoch: 93 Loss: 0.660455\n",
      "Train Epoch: 93 Loss: 0.662662\n",
      "Train Epoch: 93 Loss: 0.662732\n",
      "\n",
      "Test set: Average loss: 0.7495, Accuracy: 6/11 (55%)\n",
      "\n",
      "------------------\n",
      "Output tensor([0.5155], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([0.])\n",
      "tensor(0.7247, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 94 Loss: 0.724668\n",
      "Output tensor([0.4981], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([0.])\n",
      "tensor(0.6894, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 94 Loss: 0.689377\n",
      "Output tensor([0.5117], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([0.])\n",
      "tensor(0.7169, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 94 Loss: 0.716874\n",
      "Output tensor([0.5066], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([0.])\n",
      "tensor(0.7064, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 94 Loss: 0.706358\n",
      "Output tensor([0.4843], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([0.])\n",
      "tensor(0.6621, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 94 Loss: 0.662138\n",
      "Output tensor([0.4957], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([0.])\n",
      "tensor(0.6846, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 94 Loss: 0.684630\n",
      "Output tensor([0.4900], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([1.])\n",
      "tensor(0.7134, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 94 Loss: 0.713442\n",
      "Output tensor([0.4903], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([1.])\n",
      "tensor(0.7127, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 94 Loss: 0.712727\n",
      "Output tensor([0.4935], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([1.])\n",
      "tensor(0.7062, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 94 Loss: 0.706152\n",
      "Output tensor([0.4983], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([1.])\n",
      "tensor(0.6965, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 94 Loss: 0.696464\n",
      "Output tensor([0.5035], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([1.])\n",
      "tensor(0.6861, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 94 Loss: 0.686089\n",
      "Output tensor([0.5092], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([1.])\n",
      "tensor(0.6749, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 94 Loss: 0.674911\n",
      "\n",
      "\n",
      "Train Epoch: 94 Loss: 0.724769\n",
      "Train Epoch: 94 Loss: 0.689507\n",
      "Train Epoch: 94 Loss: 0.724745\n",
      "Train Epoch: 94 Loss: 0.723254\n",
      "Train Epoch: 94 Loss: 0.683210\n",
      "Train Epoch: 94 Loss: 0.724690\n",
      "Train Epoch: 94 Loss: 0.662557\n",
      "Train Epoch: 94 Loss: 0.662207\n",
      "Train Epoch: 94 Loss: 0.662560\n",
      "Train Epoch: 94 Loss: 0.661351\n",
      "Train Epoch: 94 Loss: 0.662541\n",
      "Train Epoch: 94 Loss: 0.662627\n",
      "\n",
      "Test set: Average loss: 0.7495, Accuracy: 6/11 (55%)\n",
      "\n",
      "------------------\n",
      "Output tensor([0.5156], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([0.])\n",
      "tensor(0.7248, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 95 Loss: 0.724769\n",
      "Output tensor([0.4980], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([0.])\n",
      "tensor(0.6892, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 95 Loss: 0.689235\n",
      "Output tensor([0.5117], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([0.])\n",
      "tensor(0.7169, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 95 Loss: 0.716884\n",
      "Output tensor([0.5064], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([0.])\n",
      "tensor(0.7060, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 95 Loss: 0.705973\n",
      "Output tensor([0.4842], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([0.])\n",
      "tensor(0.6620, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 95 Loss: 0.661964\n",
      "Output tensor([0.4952], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([0.])\n",
      "tensor(0.6836, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 95 Loss: 0.683594\n",
      "Output tensor([0.4783], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([1.])\n",
      "tensor(0.7376, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 95 Loss: 0.737584\n",
      "Output tensor([0.4905], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([1.])\n",
      "tensor(0.7123, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 95 Loss: 0.712318\n",
      "Output tensor([0.4935], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([1.])\n",
      "tensor(0.7063, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 95 Loss: 0.706252\n",
      "Output tensor([0.4981], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([1.])\n",
      "tensor(0.6970, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 95 Loss: 0.696953\n",
      "Output tensor([0.5028], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([1.])\n",
      "tensor(0.6876, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 95 Loss: 0.687569\n",
      "Output tensor([0.5079], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([1.])\n",
      "tensor(0.6775, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 95 Loss: 0.677491\n",
      "\n",
      "\n",
      "Train Epoch: 95 Loss: 0.722138\n",
      "Train Epoch: 95 Loss: 0.686576\n",
      "Train Epoch: 95 Loss: 0.720882\n",
      "Train Epoch: 95 Loss: 0.721287\n",
      "Train Epoch: 95 Loss: 0.720931\n",
      "Train Epoch: 95 Loss: 0.720578\n",
      "Train Epoch: 95 Loss: 0.666476\n",
      "Train Epoch: 95 Loss: 0.663652\n",
      "Train Epoch: 95 Loss: 0.666518\n",
      "Train Epoch: 95 Loss: 0.663827\n",
      "Train Epoch: 95 Loss: 0.665326\n",
      "Train Epoch: 95 Loss: 0.666512\n",
      "\n",
      "Test set: Average loss: 0.7532, Accuracy: 6/11 (55%)\n",
      "\n",
      "------------------\n",
      "Output tensor([0.5143], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([0.])\n",
      "tensor(0.7221, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 96 Loss: 0.722138\n",
      "Output tensor([0.4967], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([0.])\n",
      "tensor(0.6866, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 96 Loss: 0.686609\n",
      "Output tensor([0.5103], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([0.])\n",
      "tensor(0.7139, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 96 Loss: 0.713924\n",
      "Output tensor([0.5058], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([0.])\n",
      "tensor(0.7047, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 96 Loss: 0.704722\n",
      "Output tensor([0.5006], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([0.])\n",
      "tensor(0.6944, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 96 Loss: 0.694395\n",
      "Output tensor([0.4955], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([0.])\n",
      "tensor(0.6842, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 96 Loss: 0.684237\n",
      "Output tensor([0.4905], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([1.])\n",
      "tensor(0.7124, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 96 Loss: 0.712366\n",
      "Output tensor([0.4906], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([1.])\n",
      "tensor(0.7120, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 96 Loss: 0.712042\n",
      "Output tensor([0.4933], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([1.])\n",
      "tensor(0.7067, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 96 Loss: 0.706704\n",
      "Output tensor([0.4971], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([1.])\n",
      "tensor(0.6989, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 96 Loss: 0.698922\n",
      "Output tensor([0.5012], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([1.])\n",
      "tensor(0.6908, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 96 Loss: 0.690769\n",
      "Output tensor([0.5056], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([1.])\n",
      "tensor(0.6819, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 96 Loss: 0.681916\n",
      "\n",
      "\n",
      "Train Epoch: 96 Loss: 0.714143\n",
      "Train Epoch: 96 Loss: 0.682678\n",
      "Train Epoch: 96 Loss: 0.714146\n",
      "Train Epoch: 96 Loss: 0.714162\n",
      "Train Epoch: 96 Loss: 0.714133\n",
      "Train Epoch: 96 Loss: 0.714128\n",
      "Train Epoch: 96 Loss: 0.672599\n",
      "Train Epoch: 96 Loss: 0.672584\n",
      "Train Epoch: 96 Loss: 0.672605\n",
      "Train Epoch: 96 Loss: 0.671727\n",
      "Train Epoch: 96 Loss: 0.672597\n",
      "Train Epoch: 96 Loss: 0.672599\n",
      "\n",
      "Test set: Average loss: 0.7535, Accuracy: 6/11 (55%)\n",
      "\n",
      "------------------\n",
      "Output tensor([0.5104], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([0.])\n",
      "tensor(0.7141, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 97 Loss: 0.714143\n",
      "Output tensor([0.4948], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([0.])\n",
      "tensor(0.6828, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 97 Loss: 0.682834\n",
      "Output tensor([0.5078], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([0.])\n",
      "tensor(0.7089, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 97 Loss: 0.708891\n",
      "Output tensor([0.5040], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([0.])\n",
      "tensor(0.7012, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 97 Loss: 0.701204\n",
      "Output tensor([0.4996], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([0.])\n",
      "tensor(0.6924, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 97 Loss: 0.692372\n",
      "Output tensor([0.4951], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([0.])\n",
      "tensor(0.6833, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 97 Loss: 0.683348\n",
      "Output tensor([0.4905], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([1.])\n",
      "tensor(0.7123, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 97 Loss: 0.712339\n",
      "Output tensor([0.4907], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([1.])\n",
      "tensor(0.7120, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 97 Loss: 0.711952\n",
      "Output tensor([0.4931], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([1.])\n",
      "tensor(0.7070, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 97 Loss: 0.706991\n",
      "Output tensor([0.4969], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([1.])\n",
      "tensor(0.6994, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 97 Loss: 0.699399\n",
      "Output tensor([0.5004], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([1.])\n",
      "tensor(0.6922, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 97 Loss: 0.692248\n",
      "Output tensor([0.5045], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([1.])\n",
      "tensor(0.6841, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 97 Loss: 0.684136\n",
      "\n",
      "\n",
      "Train Epoch: 97 Loss: 0.711092\n",
      "Train Epoch: 97 Loss: 0.680957\n",
      "Train Epoch: 97 Loss: 0.711320\n",
      "Train Epoch: 97 Loss: 0.711875\n",
      "Train Epoch: 97 Loss: 0.711099\n",
      "Train Epoch: 97 Loss: 0.710939\n",
      "Train Epoch: 97 Loss: 0.675684\n",
      "Train Epoch: 97 Loss: 0.675646\n",
      "Train Epoch: 97 Loss: 0.675691\n",
      "Train Epoch: 97 Loss: 0.672769\n",
      "Train Epoch: 97 Loss: 0.675682\n",
      "Train Epoch: 97 Loss: 0.675684\n",
      "\n",
      "Test set: Average loss: 0.7535, Accuracy: 6/11 (55%)\n",
      "\n",
      "------------------\n",
      "Output tensor([0.5089], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([0.])\n",
      "tensor(0.7111, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 98 Loss: 0.711092\n",
      "Output tensor([0.4940], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([0.])\n",
      "tensor(0.6812, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 98 Loss: 0.681206\n",
      "Output tensor([0.5067], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([0.])\n",
      "tensor(0.7066, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 98 Loss: 0.706570\n",
      "Output tensor([0.5032], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([0.])\n",
      "tensor(0.6996, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 98 Loss: 0.699615\n",
      "Output tensor([0.4991], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([0.])\n",
      "tensor(0.6914, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 98 Loss: 0.691408\n",
      "Output tensor([0.4949], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([0.])\n",
      "tensor(0.6830, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 98 Loss: 0.683002\n",
      "Output tensor([0.4906], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([1.])\n",
      "tensor(0.7122, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 98 Loss: 0.712169\n",
      "Output tensor([0.4908], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([1.])\n",
      "tensor(0.7117, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 98 Loss: 0.711737\n",
      "Output tensor([0.4931], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([1.])\n",
      "tensor(0.7070, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 98 Loss: 0.707008\n",
      "Output tensor([0.4966], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([1.])\n",
      "tensor(0.7000, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 98 Loss: 0.700000\n",
      "Output tensor([0.5001], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([1.])\n",
      "tensor(0.6929, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 98 Loss: 0.692930\n",
      "Output tensor([0.5040], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([1.])\n",
      "tensor(0.6852, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 98 Loss: 0.685248\n",
      "\n",
      "\n",
      "Train Epoch: 98 Loss: 0.709268\n",
      "Train Epoch: 98 Loss: 0.680197\n",
      "Train Epoch: 98 Loss: 0.709273\n",
      "Train Epoch: 98 Loss: 0.709302\n",
      "Train Epoch: 98 Loss: 0.709256\n",
      "Train Epoch: 98 Loss: 0.709244\n",
      "Train Epoch: 98 Loss: 0.677307\n",
      "Train Epoch: 98 Loss: 0.677214\n",
      "Train Epoch: 98 Loss: 0.677313\n",
      "Train Epoch: 98 Loss: 0.675994\n",
      "Train Epoch: 98 Loss: 0.677303\n",
      "Train Epoch: 98 Loss: 0.677307\n",
      "\n",
      "Test set: Average loss: 0.7535, Accuracy: 6/11 (55%)\n",
      "\n",
      "------------------\n",
      "Output tensor([0.5080], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([0.])\n",
      "tensor(0.7093, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 99 Loss: 0.709268\n",
      "Output tensor([0.4937], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([0.])\n",
      "tensor(0.6805, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 99 Loss: 0.680532\n",
      "Output tensor([0.5060], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([0.])\n",
      "tensor(0.7052, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 99 Loss: 0.705174\n",
      "Output tensor([0.5028], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([0.])\n",
      "tensor(0.6987, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 99 Loss: 0.698709\n",
      "Output tensor([0.4989], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([0.])\n",
      "tensor(0.6910, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 99 Loss: 0.691016\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output tensor([0.4949], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([0.])\n",
      "tensor(0.6830, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 99 Loss: 0.682967\n",
      "Output tensor([0.4907], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([1.])\n",
      "tensor(0.7119, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 99 Loss: 0.711894\n",
      "Output tensor([0.4910], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([1.])\n",
      "tensor(0.7114, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 99 Loss: 0.711402\n",
      "Output tensor([0.4932], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([1.])\n",
      "tensor(0.7069, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 99 Loss: 0.706860\n",
      "Output tensor([0.4968], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([1.])\n",
      "tensor(0.6996, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 99 Loss: 0.699611\n",
      "Output tensor([0.5000], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([1.])\n",
      "tensor(0.6932, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 99 Loss: 0.693212\n",
      "Output tensor([0.5037], grad_fn=<SigmoidBackward>)\n",
      "Target tensor([1.])\n",
      "tensor(0.6858, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train Epoch: 99 Loss: 0.685779\n",
      "\n",
      "\n",
      "Train Epoch: 99 Loss: 0.708739\n",
      "Train Epoch: 99 Loss: 0.680126\n",
      "Train Epoch: 99 Loss: 0.709118\n",
      "Train Epoch: 99 Loss: 0.709950\n",
      "Train Epoch: 99 Loss: 0.708746\n",
      "Train Epoch: 99 Loss: 0.708448\n",
      "Train Epoch: 99 Loss: 0.678112\n",
      "Train Epoch: 99 Loss: 0.677844\n",
      "Train Epoch: 99 Loss: 0.678119\n",
      "Train Epoch: 99 Loss: 0.674728\n",
      "Train Epoch: 99 Loss: 0.678104\n",
      "Train Epoch: 99 Loss: 0.678111\n",
      "\n",
      "Test set: Average loss: 0.7536, Accuracy: 6/11 (55%)\n",
      "\n",
      "------------------\n",
      "[0.54545455 0.54545455 0.54545455 0.54545455 0.54545455 0.54545455\n",
      " 0.54545455 0.54545455 0.54545455 0.54545455 0.54545455 0.54545455\n",
      " 0.54545455 0.54545455 0.54545455 0.54545455 0.54545455 0.54545455\n",
      " 0.54545455 0.54545455 0.54545455 0.54545455 0.54545455 0.54545455\n",
      " 0.54545455 0.54545455 0.54545455 0.54545455 0.54545455 0.54545455\n",
      " 0.54545455 0.54545455 0.54545455 0.54545455 0.54545455 0.54545455\n",
      " 0.54545455 0.54545455 0.54545455 0.54545455 0.54545455 0.54545455\n",
      " 0.54545455 0.54545455 0.54545455 0.54545455 0.54545455 0.54545455\n",
      " 0.54545455 0.54545455 0.54545455 0.54545455 0.54545455 0.54545455\n",
      " 0.54545455 0.54545455 0.54545455 0.54545455 0.54545455 0.54545455\n",
      " 0.54545455 0.54545455 0.54545455 0.54545455 0.54545455 0.54545455\n",
      " 0.54545455 0.54545455 0.54545455 0.54545455 0.54545455 0.54545455\n",
      " 0.54545455 0.54545455 0.54545455 0.54545455 0.54545455 0.54545455\n",
      " 0.54545455 0.54545455 0.54545455 0.54545455 0.54545455 0.54545455\n",
      " 0.54545455 0.54545455 0.54545455 0.54545455 0.54545455 0.54545455\n",
      " 0.54545455 0.54545455 0.54545455 0.54545455 0.54545455 0.54545455\n",
      " 0.54545455 0.54545455 0.54545455]\n",
      "99\n"
     ]
    }
   ],
   "source": [
    "# model from http://aqibsaeed.github.io/2016-09-03-urban-sound-classification-part-1/\n",
    "\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.l1 = nn.Linear(193, 280)\n",
    "        self.l2 = nn.Linear(280, 300)\n",
    "        self.l3 = nn.Linear(300, 1)\n",
    "        self.tan =nn.Tanh()\n",
    "        self.sigmoid=nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        #x = x.view(-1, 784)  # Flatten the data (n, 1, 28, 28)-> (n, 784)\n",
    "        x = self.tan(self.l1(x))\n",
    "        x = self.tan(self.l2(x))\n",
    "        x = self.sigmoid(self.l3(x))\n",
    "        return x\n",
    "\n",
    "\n",
    "model = Net()\n",
    "\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.5)\n",
    "\n",
    "tr_features = Variable ( torch.FloatTensor(tr_features) )\n",
    "tr_labels1 = Variable ( torch.FloatTensor(tr_labels) )\n",
    "\n",
    "tst_features = Variable ( torch.FloatTensor(tst_features) )\n",
    "tst_labels1 = Variable ( torch.FloatTensor(tst_labels) )\n",
    "\n",
    "\n",
    "\n",
    "def train(epoch):\n",
    "    \n",
    "    for i,data in enumerate(tr_features):\n",
    "        \n",
    "        targetTnsr=tr_labels1[i]\n",
    "        targetTnsr=torch.tensor([targetTnsr])\n",
    "           \n",
    "    \n",
    "        #target = torch.from_numpy(target)\n",
    "        \n",
    "        data, targetTnsr = Variable(data), Variable(targetTnsr)\n",
    "    \n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        print(\"Output \"+str(output))\n",
    "        print(\"Target \"+str(targetTnsr))\n",
    "        loss = criterion(output, targetTnsr)\n",
    "        print(loss)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        print('Train Epoch: {} Loss: {:.6f}'.format(\n",
    "                epoch, loss.item()))\n",
    "\n",
    "def test():\n",
    "\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    for i,data in enumerate(tst_features):\n",
    "        targetTnsr=tst_labels1[i]\n",
    "        targetTnsr=torch.tensor([targetTnsr])\n",
    "       \n",
    "        data, targetTnsr = Variable(data), Variable(targetTnsr)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        \n",
    "        # sum up batch loss\n",
    "        loss = criterion(output, targetTnsr)\n",
    "        test_loss += loss\n",
    "        pred = output.data\n",
    "\n",
    "        if int(pred.data.numpy()[0])==round(tr_labels[i]):\n",
    "            correct += 1\n",
    "        \n",
    "        \n",
    "       \n",
    "\n",
    "        print('Train Epoch: {} Loss: {:.6f}'.format(\n",
    "                epoch, loss.item()))\n",
    "        \n",
    "        \n",
    "        \n",
    "    \n",
    "\n",
    "    test_loss /= 11\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        test_loss, correct, 11,\n",
    "        100. * correct / 11))\n",
    "    return (correct / 11)\n",
    "\n",
    "\n",
    "\n",
    "acc=[]\n",
    "npacc=np.empty(0)\n",
    "totalEpoch = 100\n",
    "for epoch in range(1, totalEpoch):\n",
    "    train(epoch)\n",
    "    print('\\n')\n",
    "    ac = test()\n",
    "    acc.append(ac)\n",
    "    npacc = np.asarray(acc)\n",
    "    print(\"------------------\")\n",
    "print(npacc)\n",
    "print(len(acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "epoc=range(1, totalEpoch)\n",
    "plt.plot(epoc, acc, 'ro')\n",
    "plt.axis([0, totalEpoch, 0, 1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
